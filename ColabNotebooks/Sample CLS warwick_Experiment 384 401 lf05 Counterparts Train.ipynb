{"cells":[{"cell_type":"markdown","metadata":{"id":"-1_HUut4YYm5"},"source":["## This is the official counterparts evaluation script of MIL tasks\n","* Use google colab pro+ (high RAM+GPU)\n","* we use the P100 GPU for the Experiments\n","\n","## The code and Training process along with all record are private\n","* Our github page: https://github.com/sagizty/pancreatic-cancer-diagnosis-tansformer\n","* The ROSE dataset is not publicly aviliable.\n","* However the MICCAI 2015 chanllenge dataset is avaliable for illustration.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1651152773355,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"},"user_tz":-480},"id":"ZnbrNSoSXFm5","outputId":"c5c23fd3-eb7e-4674-b68f-93a1ef7f797c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr 28 13:32:52 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# check GPU\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1651152773355,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"},"user_tz":-480},"id":"n9GPOn5gcykA","outputId":"c64ff487-8510-4632-bf7d-dd1b2cceff1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr 28 21:32:52 UTC 2022\n"]}],"source":["!date --date='+8 hour'  # CST time zone"]},{"cell_type":"markdown","metadata":{"id":"fbnpeHYUgsJz"},"source":["## Mount Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3obRNrIaffjK","executionInfo":{"status":"ok","timestamp":1651152808292,"user_tz":-480,"elapsed":34941,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"8e30fb6b-376f-424e-f30a-26872301d27f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"BYevYeMFYmlx"},"source":["## create file-system enviroment\n","* mount your google drive first\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ePtQFcQCEPlu","outputId":"9eab362d-ebe5-4aba-af66-7c480e934c74","executionInfo":{"status":"ok","timestamp":1651152857719,"user_tz":-480,"elapsed":49436,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Folder Tree Creation completed!\n","Cloning into '/home/MIL_Experiment/code'...\n","remote: Enumerating objects: 639, done.\u001b[K\n","remote: Counting objects: 100% (16/16), done.\u001b[K\n","remote: Compressing objects: 100% (16/16), done.\u001b[K\n","remote: Total 639 (delta 6), reused 0 (delta 0), pack-reused 623\u001b[K\n","Receiving objects: 100% (639/639), 500.96 MiB | 11.96 MiB/s, done.\n","Resolving deltas: 100% (115/115), done.\n","Checking out files: 100% (453/453), done.\n","code transfer from github completed!\n","data transfer completed!\n"]}],"source":["# create file-system enviroment\n","# mount the google drive first\n","# https://drive.google.com/drive/u/1/my-drive\n","\n","# clear colab path\n","!rm -rf /data\n","!rm -rf /home/MIL_Experiment\n","\n","# create path\n","!mkdir /home/MIL_Experiment\n","!mkdir /home/MIL_Experiment/runs\n","!mkdir /home/MIL_Experiment/code\n","!mkdir /home/MIL_Experiment/saved_models\n","!mkdir /home/MIL_Experiment/imaging_results\n","\n","!mkdir /data\n","!mkdir /data/MIL_Experiment\n","!mkdir /data/MIL_Experiment/dataset\n","\n","print('Folder Tree Creation completed!')\n","\n","# get the latest code from Github MIL-SI official page\n","!git clone https://www.github.com/sagizty/pancreatic-cancer-diagnosis-tansformer.git /home/MIL_Experiment/code\n","print('code transfer from github completed!')\n","\n","# copy runs if u want to compare\n","# !cp -r /content/drive/MyDrive/MIL_Experiment/runs/* /home/MIL_Experiment/runs\n","# print('tensorboard log transfer completed!')\n","\n","# copy saved_models if u want to compare\n","# !cp -r /content/drive/MyDrive/MIL_Experiment/saved_models/* /home/MIL_Experiment/saved_models\n","# print('saved_models transfer completed!')\n","\n","# get the MIL and CLS dataset from github\n","# by its zip\n","# !cp /home/MIL_Experiment/code/sample_datasets/warwick_MIL.zip /data/MIL_Experiment/dataset/\n","!cp /home/MIL_Experiment/code/sample_datasets/warwick_CLS.zip /data/MIL_Experiment/dataset/\n","# unzip\n","# !unzip -q /data/MIL_Experiment/dataset/warwick_MIL.zip -d /data/MIL_Experiment/dataset/\n","!unzip -q /data/MIL_Experiment/dataset/warwick_CLS.zip -d /data/MIL_Experiment/dataset/\n","\n","# alter the path\n","# !rm -rf /data/MIL_Experiment/dataset/warwick_MIL.zip\n","!rm -rf /data/MIL_Experiment/dataset/warwick_CLS.zip\n","print('data transfer completed!')"]},{"cell_type":"markdown","metadata":{"id":"xLxxHGq_wwwL"},"source":["## Arrange the working enviorment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1Yb2b6TGF4r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651152870537,"user_tz":-480,"elapsed":12832,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"a626f2cb-481a-4f92-bb25-0ad13086750d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/home/MIL_Experiment/code\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5\n","Collecting timm\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[K     |████████████████████████████████| 431 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n","Installing collected packages: timm\n","Successfully installed timm-0.5.4\n","Collecting notifyemail\n","  Downloading notifyemail-1.0.2-py3-none-any.whl (31 kB)\n","Installing collected packages: notifyemail\n","Successfully installed notifyemail-1.0.2\n","Collecting ttach\n","  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n","Installing collected packages: ttach\n","Successfully installed ttach-0.0.3\n"]}],"source":["# change working dir\n","import os\n","os.chdir(\"/home/MIL_Experiment/code\")\n","!pwd\n","\n","# get packages\n","!pip install tensorboardX\n","!pip install timm\n","!pip install notifyemail\n","!pip install ttach\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87Owjg_pN2yD","executionInfo":{"status":"ok","timestamp":1651152870538,"user_tz":-480,"elapsed":17,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"7e2f2da6-4b25-4ac9-c422-7e38a22a3e78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.13\n"]}],"source":["!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpEVUWwqK79D","executionInfo":{"status":"ok","timestamp":1651152871189,"user_tz":-480,"elapsed":656,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"ce0f8a81-f723-472c-ab8d-29266ebeb7ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Package                       Version\n","----------------------------- ---------------------\n","absl-py                       1.0.0\n","alabaster                     0.7.12\n","albumentations                0.1.12\n","altair                        4.2.0\n","appdirs                       1.4.4\n","argon2-cffi                   21.3.0\n","argon2-cffi-bindings          21.2.0\n","arviz                         0.12.0\n","astor                         0.8.1\n","astropy                       4.3.1\n","astunparse                    1.6.3\n","atari-py                      0.2.9\n","atomicwrites                  1.4.0\n","attrs                         21.4.0\n","audioread                     2.1.9\n","autograd                      1.4\n","Babel                         2.9.1\n","backcall                      0.2.0\n","beautifulsoup4                4.6.3\n","bleach                        5.0.0\n","blis                          0.4.1\n","bokeh                         2.3.3\n","Bottleneck                    1.3.4\n","branca                        0.5.0\n","bs4                           0.0.1\n","CacheControl                  0.12.10\n","cached-property               1.5.2\n","cachetools                    4.2.4\n","catalogue                     1.0.0\n","certifi                       2021.10.8\n","cffi                          1.15.0\n","cftime                        1.6.0\n","chardet                       3.0.4\n","charset-normalizer            2.0.12\n","click                         7.1.2\n","cloudpickle                   1.3.0\n","cmake                         3.12.0\n","cmdstanpy                     0.9.5\n","colorcet                      3.0.0\n","colorlover                    0.3.0\n","community                     1.0.0b1\n","contextlib2                   0.5.5\n","convertdate                   2.4.0\n","coverage                      3.7.1\n","coveralls                     0.5\n","crcmod                        1.7\n","cufflinks                     0.17.3\n","cupy-cuda111                  9.4.0\n","cvxopt                        1.2.7\n","cvxpy                         1.0.31\n","cycler                        0.11.0\n","cymem                         2.0.6\n","Cython                        0.29.28\n","daft                          0.0.4\n","dask                          2.12.0\n","datascience                   0.10.6\n","debugpy                       1.0.0\n","decorator                     4.4.2\n","defusedxml                    0.7.1\n","descartes                     1.1.0\n","dill                          0.3.4\n","distributed                   1.25.3\n","dlib                          19.18.0\n","dm-tree                       0.1.7\n","docopt                        0.6.2\n","docutils                      0.17.1\n","dopamine-rl                   1.0.5\n","earthengine-api               0.1.306\n","easydict                      1.9\n","ecos                          2.0.10\n","editdistance                  0.5.3\n","en-core-web-sm                2.2.5\n","entrypoints                   0.4\n","ephem                         4.1.3\n","et-xmlfile                    1.1.0\n","fa2                           0.3.5\n","fastai                        1.0.61\n","fastdtw                       0.3.4\n","fastjsonschema                2.15.3\n","fastprogress                  1.0.2\n","fastrlock                     0.8\n","fbprophet                     0.7.1\n","feather-format                0.4.1\n","filelock                      3.6.0\n","firebase-admin                4.4.0\n","fix-yahoo-finance             0.0.22\n","Flask                         1.1.4\n","flatbuffers                   2.0\n","folium                        0.8.3\n","future                        0.16.0\n","gast                          0.5.3\n","GDAL                          2.2.2\n","gdown                         4.4.0\n","gensim                        3.6.0\n","geographiclib                 1.52\n","geopy                         1.17.0\n","gin-config                    0.5.0\n","glob2                         0.7\n","google                        2.0.3\n","google-api-core               1.31.5\n","google-api-python-client      1.12.11\n","google-auth                   1.35.0\n","google-auth-httplib2          0.0.4\n","google-auth-oauthlib          0.4.6\n","google-cloud-bigquery         1.21.0\n","google-cloud-bigquery-storage 1.1.1\n","google-cloud-core             1.0.3\n","google-cloud-datastore        1.8.0\n","google-cloud-firestore        1.7.0\n","google-cloud-language         1.2.0\n","google-cloud-storage          1.18.1\n","google-cloud-translate        1.5.0\n","google-colab                  1.0.0\n","google-pasta                  0.2.0\n","google-resumable-media        0.4.1\n","googleapis-common-protos      1.56.0\n","googledrivedownloader         0.4\n","graphviz                      0.10.1\n","greenlet                      1.1.2\n","grpcio                        1.44.0\n","gspread                       3.4.2\n","gspread-dataframe             3.0.8\n","gym                           0.17.3\n","h5py                          3.1.0\n","HeapDict                      1.0.1\n","hijri-converter               2.2.3\n","holidays                      0.10.5.2\n","holoviews                     1.14.8\n","html5lib                      1.0.1\n","httpimport                    0.5.18\n","httplib2                      0.17.4\n","httplib2shim                  0.0.3\n","humanize                      0.5.1\n","hyperopt                      0.1.2\n","ideep4py                      2.0.0.post3\n","idna                          2.10\n","imageio                       2.4.1\n","imagesize                     1.3.0\n","imbalanced-learn              0.8.1\n","imblearn                      0.0\n","imgaug                        0.2.9\n","importlib-metadata            4.11.3\n","importlib-resources           5.7.1\n","imutils                       0.5.4\n","inflect                       2.1.0\n","iniconfig                     1.1.1\n","intel-openmp                  2022.0.2\n","intervaltree                  2.1.0\n","ipykernel                     4.10.1\n","ipython                       5.5.0\n","ipython-genutils              0.2.0\n","ipython-sql                   0.3.9\n","ipywidgets                    7.7.0\n","itsdangerous                  1.1.0\n","jax                           0.3.4\n","jaxlib                        0.3.2+cuda11.cudnn805\n","jedi                          0.18.1\n","jieba                         0.42.1\n","Jinja2                        2.11.3\n","joblib                        1.1.0\n","jpeg4py                       0.1.4\n","jsonschema                    4.3.3\n","jupyter                       1.0.0\n","jupyter-client                5.3.5\n","jupyter-console               5.2.0\n","jupyter-core                  4.10.0\n","jupyterlab-pygments           0.2.2\n","jupyterlab-widgets            1.1.0\n","kaggle                        1.5.12\n","kapre                         0.3.7\n","keras                         2.8.0\n","Keras-Preprocessing           1.1.2\n","keras-vis                     0.4.1\n","kiwisolver                    1.4.2\n","korean-lunar-calendar         0.2.1\n","libclang                      13.0.0\n","librosa                       0.8.1\n","lightgbm                      2.2.3\n","llvmlite                      0.34.0\n","lmdb                          0.99\n","LunarCalendar                 0.0.9\n","lxml                          4.2.6\n","Markdown                      3.3.6\n","MarkupSafe                    2.0.1\n","matplotlib                    3.2.2\n","matplotlib-inline             0.1.3\n","matplotlib-venn               0.11.7\n","missingno                     0.5.1\n","mistune                       0.8.4\n","mizani                        0.6.0\n","mkl                           2019.0\n","mlxtend                       0.14.0\n","more-itertools                8.12.0\n","moviepy                       0.2.3.5\n","mpmath                        1.2.1\n","msgpack                       1.0.3\n","multiprocess                  0.70.12.2\n","multitasking                  0.0.10\n","murmurhash                    1.0.6\n","music21                       5.5.0\n","natsort                       5.5.0\n","nbclient                      0.6.0\n","nbconvert                     5.6.1\n","nbformat                      5.3.0\n","nest-asyncio                  1.5.5\n","netCDF4                       1.5.8\n","networkx                      2.6.3\n","nibabel                       3.0.2\n","nltk                          3.2.5\n","notebook                      5.3.1\n","notifyemail                   1.0.2\n","numba                         0.51.2\n","numexpr                       2.8.1\n","numpy                         1.21.6\n","nvidia-ml-py3                 7.352.0\n","oauth2client                  4.1.3\n","oauthlib                      3.2.0\n","okgrade                       0.4.3\n","opencv-contrib-python         4.1.2.30\n","opencv-python                 4.1.2.30\n","openpyxl                      3.0.9\n","opt-einsum                    3.3.0\n","osqp                          0.6.2.post0\n","packaging                     21.3\n","palettable                    3.3.0\n","pandas                        1.3.5\n","pandas-datareader             0.9.0\n","pandas-gbq                    0.13.3\n","pandas-profiling              1.4.1\n","pandocfilters                 1.5.0\n","panel                         0.12.1\n","param                         1.12.1\n","parso                         0.8.3\n","pathlib                       1.0.1\n","patsy                         0.5.2\n","pep517                        0.12.0\n","pexpect                       4.8.0\n","pickleshare                   0.7.5\n","Pillow                        7.1.2\n","pip                           21.1.3\n","pip-tools                     6.2.0\n","plac                          1.1.3\n","plotly                        5.5.0\n","plotnine                      0.6.0\n","pluggy                        0.7.1\n","pooch                         1.6.0\n","portpicker                    1.3.9\n","prefetch-generator            1.0.1\n","preshed                       3.0.6\n","prettytable                   3.2.0\n","progressbar2                  3.38.0\n","prometheus-client             0.14.1\n","promise                       2.3\n","prompt-toolkit                1.0.18\n","protobuf                      3.17.3\n","psutil                        5.4.8\n","psycopg2                      2.7.6.1\n","ptyprocess                    0.7.0\n","py                            1.11.0\n","pyarrow                       6.0.1\n","pyasn1                        0.4.8\n","pyasn1-modules                0.2.8\n","pycocotools                   2.0.4\n","pycparser                     2.21\n","pyct                          0.4.8\n","pydata-google-auth            1.4.0\n","pydot                         1.3.0\n","pydot-ng                      2.0.0\n","pydotplus                     2.0.2\n","PyDrive                       1.3.1\n","pyemd                         0.5.1\n","pyerfa                        2.0.0.1\n","pyglet                        1.5.0\n","Pygments                      2.6.1\n","pygobject                     3.26.1\n","pymc3                         3.11.4\n","PyMeeus                       0.5.11\n","pymongo                       4.1.1\n","pymystem3                     0.2.0\n","PyOpenGL                      3.1.6\n","pyparsing                     3.0.8\n","pyrsistent                    0.18.1\n","pysndfile                     1.3.8\n","PySocks                       1.7.1\n","pystan                        2.19.1.1\n","pytest                        3.6.4\n","python-apt                    0.0.0\n","python-chess                  0.23.11\n","python-dateutil               2.8.2\n","python-louvain                0.16\n","python-slugify                6.1.1\n","python-utils                  3.1.0\n","pytz                          2022.1\n","pyviz-comms                   2.2.0\n","PyWavelets                    1.3.0\n","PyYAML                        3.13\n","pyzmq                         22.3.0\n","qdldl                         0.1.5.post2\n","qtconsole                     5.3.0\n","QtPy                          2.0.1\n","regex                         2019.12.20\n","requests                      2.23.0\n","requests-oauthlib             1.3.1\n","resampy                       0.2.2\n","rpy2                          3.4.5\n","rsa                           4.8\n","scikit-image                  0.18.3\n","scikit-learn                  1.0.2\n","scipy                         1.4.1\n","screen-resolution-extra       0.0.0\n","scs                           3.2.0\n","seaborn                       0.11.2\n","semver                        2.13.0\n","Send2Trash                    1.8.0\n","setuptools                    57.4.0\n","setuptools-git                1.2\n","Shapely                       1.8.1.post1\n","simplegeneric                 0.8.1\n","six                           1.15.0\n","sklearn                       0.0\n","sklearn-pandas                1.8.0\n","smart-open                    5.2.1\n","snowballstemmer               2.2.0\n","sortedcontainers              2.4.0\n","SoundFile                     0.10.3.post1\n","soupsieve                     2.3.2.post1\n","spacy                         2.2.4\n","Sphinx                        1.8.6\n","sphinxcontrib-serializinghtml 1.1.5\n","sphinxcontrib-websupport      1.2.4\n","SQLAlchemy                    1.4.35\n","sqlparse                      0.4.2\n","srsly                         1.0.5\n","statsmodels                   0.10.2\n","sympy                         1.7.1\n","tables                        3.7.0\n","tabulate                      0.8.9\n","tblib                         1.7.0\n","tenacity                      8.0.1\n","tensorboard                   2.8.0\n","tensorboard-data-server       0.6.1\n","tensorboard-plugin-wit        1.8.1\n","tensorboardX                  2.5\n","tensorflow                    2.8.0\n","tensorflow-datasets           4.0.1\n","tensorflow-estimator          2.8.0\n","tensorflow-gcs-config         2.8.0\n","tensorflow-hub                0.12.0\n","tensorflow-io-gcs-filesystem  0.24.0\n","tensorflow-metadata           1.7.0\n","tensorflow-probability        0.16.0\n","termcolor                     1.1.0\n","terminado                     0.13.3\n","testpath                      0.6.0\n","text-unidecode                1.3\n","textblob                      0.15.3\n","Theano-PyMC                   1.1.2\n","thinc                         7.4.0\n","threadpoolctl                 3.1.0\n","tifffile                      2021.11.2\n","timm                          0.5.4\n","tinycss2                      1.1.1\n","tomli                         2.0.1\n","toolz                         0.11.2\n","torch                         1.11.0+cu113\n","torchaudio                    0.11.0+cu113\n","torchsummary                  1.5.1\n","torchtext                     0.12.0\n","torchvision                   0.12.0+cu113\n","tornado                       5.1.1\n","tqdm                          4.64.0\n","traitlets                     5.1.1\n","ttach                         0.0.3\n","tweepy                        3.10.0\n","typeguard                     2.7.1\n","typing-extensions             4.2.0\n","tzlocal                       1.5.1\n","uritemplate                   3.0.1\n","urllib3                       1.24.3\n","vega-datasets                 0.9.0\n","wasabi                        0.9.1\n","wcwidth                       0.2.5\n","webencodings                  0.5.1\n","Werkzeug                      1.0.1\n","wheel                         0.37.1\n","widgetsnbextension            3.6.0\n","wordcloud                     1.5.0\n","wrapt                         1.14.0\n","xarray                        0.18.2\n","xgboost                       0.90\n","xkit                          0.0.0\n","xlrd                          1.1.0\n","xlwt                          1.3.0\n","yellowbrick                   1.4\n","zict                          2.1.0\n","zipp                          3.8.0\n"]}],"source":["!pip list"]},{"cell_type":"markdown","metadata":{"id":"h31KAx1ZZEl9"},"source":["## Start Training\n","* by command line\n","* use argparse to set down hyper-parameter\n","\n","* 5-fold experiment is used here"]},{"cell_type":"markdown","metadata":{"id":"--aldMsHOZkP"},"source":["# CLS counterparts"]},{"cell_type":"markdown","metadata":{"id":"QqeBMVh6OjTu"},"source":["* Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxJcLMKMYeDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651153411309,"user_tz":-480,"elapsed":540122,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"f77b89ef-4009-40dd-f813-a09a51ce2512"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ViT_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 2.3224, -1.6767]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 24, 24]         590,592\n","          Identity-2             [-1, 576, 768]               0\n","        PatchEmbed-3             [-1, 576, 768]               0\n","           Dropout-4             [-1, 577, 768]               0\n","         LayerNorm-5             [-1, 577, 768]           1,536\n","            Linear-6            [-1, 577, 2304]       1,771,776\n","           Dropout-7         [-1, 12, 577, 577]               0\n","            Linear-8             [-1, 577, 768]         590,592\n","           Dropout-9             [-1, 577, 768]               0\n","        Attention-10             [-1, 577, 768]               0\n","         Identity-11             [-1, 577, 768]               0\n","        LayerNorm-12             [-1, 577, 768]           1,536\n","           Linear-13            [-1, 577, 3072]       2,362,368\n","             GELU-14            [-1, 577, 3072]               0\n","          Dropout-15            [-1, 577, 3072]               0\n","           Linear-16             [-1, 577, 768]       2,360,064\n","          Dropout-17             [-1, 577, 768]               0\n","              Mlp-18             [-1, 577, 768]               0\n","         Identity-19             [-1, 577, 768]               0\n","            Block-20             [-1, 577, 768]               0\n","        LayerNorm-21             [-1, 577, 768]           1,536\n","           Linear-22            [-1, 577, 2304]       1,771,776\n","          Dropout-23         [-1, 12, 577, 577]               0\n","           Linear-24             [-1, 577, 768]         590,592\n","          Dropout-25             [-1, 577, 768]               0\n","        Attention-26             [-1, 577, 768]               0\n","         Identity-27             [-1, 577, 768]               0\n","        LayerNorm-28             [-1, 577, 768]           1,536\n","           Linear-29            [-1, 577, 3072]       2,362,368\n","             GELU-30            [-1, 577, 3072]               0\n","          Dropout-31            [-1, 577, 3072]               0\n","           Linear-32             [-1, 577, 768]       2,360,064\n","          Dropout-33             [-1, 577, 768]               0\n","              Mlp-34             [-1, 577, 768]               0\n","         Identity-35             [-1, 577, 768]               0\n","            Block-36             [-1, 577, 768]               0\n","        LayerNorm-37             [-1, 577, 768]           1,536\n","           Linear-38            [-1, 577, 2304]       1,771,776\n","          Dropout-39         [-1, 12, 577, 577]               0\n","           Linear-40             [-1, 577, 768]         590,592\n","          Dropout-41             [-1, 577, 768]               0\n","        Attention-42             [-1, 577, 768]               0\n","         Identity-43             [-1, 577, 768]               0\n","        LayerNorm-44             [-1, 577, 768]           1,536\n","           Linear-45            [-1, 577, 3072]       2,362,368\n","             GELU-46            [-1, 577, 3072]               0\n","          Dropout-47            [-1, 577, 3072]               0\n","           Linear-48             [-1, 577, 768]       2,360,064\n","          Dropout-49             [-1, 577, 768]               0\n","              Mlp-50             [-1, 577, 768]               0\n","         Identity-51             [-1, 577, 768]               0\n","            Block-52             [-1, 577, 768]               0\n","        LayerNorm-53             [-1, 577, 768]           1,536\n","           Linear-54            [-1, 577, 2304]       1,771,776\n","          Dropout-55         [-1, 12, 577, 577]               0\n","           Linear-56             [-1, 577, 768]         590,592\n","          Dropout-57             [-1, 577, 768]               0\n","        Attention-58             [-1, 577, 768]               0\n","         Identity-59             [-1, 577, 768]               0\n","        LayerNorm-60             [-1, 577, 768]           1,536\n","           Linear-61            [-1, 577, 3072]       2,362,368\n","             GELU-62            [-1, 577, 3072]               0\n","          Dropout-63            [-1, 577, 3072]               0\n","           Linear-64             [-1, 577, 768]       2,360,064\n","          Dropout-65             [-1, 577, 768]               0\n","              Mlp-66             [-1, 577, 768]               0\n","         Identity-67             [-1, 577, 768]               0\n","            Block-68             [-1, 577, 768]               0\n","        LayerNorm-69             [-1, 577, 768]           1,536\n","           Linear-70            [-1, 577, 2304]       1,771,776\n","          Dropout-71         [-1, 12, 577, 577]               0\n","           Linear-72             [-1, 577, 768]         590,592\n","          Dropout-73             [-1, 577, 768]               0\n","        Attention-74             [-1, 577, 768]               0\n","         Identity-75             [-1, 577, 768]               0\n","        LayerNorm-76             [-1, 577, 768]           1,536\n","           Linear-77            [-1, 577, 3072]       2,362,368\n","             GELU-78            [-1, 577, 3072]               0\n","          Dropout-79            [-1, 577, 3072]               0\n","           Linear-80             [-1, 577, 768]       2,360,064\n","          Dropout-81             [-1, 577, 768]               0\n","              Mlp-82             [-1, 577, 768]               0\n","         Identity-83             [-1, 577, 768]               0\n","            Block-84             [-1, 577, 768]               0\n","        LayerNorm-85             [-1, 577, 768]           1,536\n","           Linear-86            [-1, 577, 2304]       1,771,776\n","          Dropout-87         [-1, 12, 577, 577]               0\n","           Linear-88             [-1, 577, 768]         590,592\n","          Dropout-89             [-1, 577, 768]               0\n","        Attention-90             [-1, 577, 768]               0\n","         Identity-91             [-1, 577, 768]               0\n","        LayerNorm-92             [-1, 577, 768]           1,536\n","           Linear-93            [-1, 577, 3072]       2,362,368\n","             GELU-94            [-1, 577, 3072]               0\n","          Dropout-95            [-1, 577, 3072]               0\n","           Linear-96             [-1, 577, 768]       2,360,064\n","          Dropout-97             [-1, 577, 768]               0\n","              Mlp-98             [-1, 577, 768]               0\n","         Identity-99             [-1, 577, 768]               0\n","           Block-100             [-1, 577, 768]               0\n","       LayerNorm-101             [-1, 577, 768]           1,536\n","          Linear-102            [-1, 577, 2304]       1,771,776\n","         Dropout-103         [-1, 12, 577, 577]               0\n","          Linear-104             [-1, 577, 768]         590,592\n","         Dropout-105             [-1, 577, 768]               0\n","       Attention-106             [-1, 577, 768]               0\n","        Identity-107             [-1, 577, 768]               0\n","       LayerNorm-108             [-1, 577, 768]           1,536\n","          Linear-109            [-1, 577, 3072]       2,362,368\n","            GELU-110            [-1, 577, 3072]               0\n","         Dropout-111            [-1, 577, 3072]               0\n","          Linear-112             [-1, 577, 768]       2,360,064\n","         Dropout-113             [-1, 577, 768]               0\n","             Mlp-114             [-1, 577, 768]               0\n","        Identity-115             [-1, 577, 768]               0\n","           Block-116             [-1, 577, 768]               0\n","       LayerNorm-117             [-1, 577, 768]           1,536\n","          Linear-118            [-1, 577, 2304]       1,771,776\n","         Dropout-119         [-1, 12, 577, 577]               0\n","          Linear-120             [-1, 577, 768]         590,592\n","         Dropout-121             [-1, 577, 768]               0\n","       Attention-122             [-1, 577, 768]               0\n","        Identity-123             [-1, 577, 768]               0\n","       LayerNorm-124             [-1, 577, 768]           1,536\n","          Linear-125            [-1, 577, 3072]       2,362,368\n","            GELU-126            [-1, 577, 3072]               0\n","         Dropout-127            [-1, 577, 3072]               0\n","          Linear-128             [-1, 577, 768]       2,360,064\n","         Dropout-129             [-1, 577, 768]               0\n","             Mlp-130             [-1, 577, 768]               0\n","        Identity-131             [-1, 577, 768]               0\n","           Block-132             [-1, 577, 768]               0\n","       LayerNorm-133             [-1, 577, 768]           1,536\n","          Linear-134            [-1, 577, 2304]       1,771,776\n","         Dropout-135         [-1, 12, 577, 577]               0\n","          Linear-136             [-1, 577, 768]         590,592\n","         Dropout-137             [-1, 577, 768]               0\n","       Attention-138             [-1, 577, 768]               0\n","        Identity-139             [-1, 577, 768]               0\n","       LayerNorm-140             [-1, 577, 768]           1,536\n","          Linear-141            [-1, 577, 3072]       2,362,368\n","            GELU-142            [-1, 577, 3072]               0\n","         Dropout-143            [-1, 577, 3072]               0\n","          Linear-144             [-1, 577, 768]       2,360,064\n","         Dropout-145             [-1, 577, 768]               0\n","             Mlp-146             [-1, 577, 768]               0\n","        Identity-147             [-1, 577, 768]               0\n","           Block-148             [-1, 577, 768]               0\n","       LayerNorm-149             [-1, 577, 768]           1,536\n","          Linear-150            [-1, 577, 2304]       1,771,776\n","         Dropout-151         [-1, 12, 577, 577]               0\n","          Linear-152             [-1, 577, 768]         590,592\n","         Dropout-153             [-1, 577, 768]               0\n","       Attention-154             [-1, 577, 768]               0\n","        Identity-155             [-1, 577, 768]               0\n","       LayerNorm-156             [-1, 577, 768]           1,536\n","          Linear-157            [-1, 577, 3072]       2,362,368\n","            GELU-158            [-1, 577, 3072]               0\n","         Dropout-159            [-1, 577, 3072]               0\n","          Linear-160             [-1, 577, 768]       2,360,064\n","         Dropout-161             [-1, 577, 768]               0\n","             Mlp-162             [-1, 577, 768]               0\n","        Identity-163             [-1, 577, 768]               0\n","           Block-164             [-1, 577, 768]               0\n","       LayerNorm-165             [-1, 577, 768]           1,536\n","          Linear-166            [-1, 577, 2304]       1,771,776\n","         Dropout-167         [-1, 12, 577, 577]               0\n","          Linear-168             [-1, 577, 768]         590,592\n","         Dropout-169             [-1, 577, 768]               0\n","       Attention-170             [-1, 577, 768]               0\n","        Identity-171             [-1, 577, 768]               0\n","       LayerNorm-172             [-1, 577, 768]           1,536\n","          Linear-173            [-1, 577, 3072]       2,362,368\n","            GELU-174            [-1, 577, 3072]               0\n","         Dropout-175            [-1, 577, 3072]               0\n","          Linear-176             [-1, 577, 768]       2,360,064\n","         Dropout-177             [-1, 577, 768]               0\n","             Mlp-178             [-1, 577, 768]               0\n","        Identity-179             [-1, 577, 768]               0\n","           Block-180             [-1, 577, 768]               0\n","       LayerNorm-181             [-1, 577, 768]           1,536\n","          Linear-182            [-1, 577, 2304]       1,771,776\n","         Dropout-183         [-1, 12, 577, 577]               0\n","          Linear-184             [-1, 577, 768]         590,592\n","         Dropout-185             [-1, 577, 768]               0\n","       Attention-186             [-1, 577, 768]               0\n","        Identity-187             [-1, 577, 768]               0\n","       LayerNorm-188             [-1, 577, 768]           1,536\n","          Linear-189            [-1, 577, 3072]       2,362,368\n","            GELU-190            [-1, 577, 3072]               0\n","         Dropout-191            [-1, 577, 3072]               0\n","          Linear-192             [-1, 577, 768]       2,360,064\n","         Dropout-193             [-1, 577, 768]               0\n","             Mlp-194             [-1, 577, 768]               0\n","        Identity-195             [-1, 577, 768]               0\n","           Block-196             [-1, 577, 768]               0\n","       LayerNorm-197             [-1, 577, 768]           1,536\n","        Identity-198                  [-1, 768]               0\n","          Linear-199                    [-1, 2]           1,538\n","================================================================\n","Total params: 85,648,130\n","Trainable params: 85,648,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1437.49\n","Params size (MB): 326.72\n","Estimated Total Size (MB): 1765.90\n","----------------------------------------------------------------\n","model : ViT_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 2.5605740547180176\n","minibatch AVG loss: 0.6450582131743431\n","Epoch: 1     train index of 5 minibatch: 2      time used: 2.276779890060425\n","minibatch AVG loss: 1.090887939184904\n","Epoch: 1     train index of 5 minibatch: 3      time used: 2.289381265640259\n","minibatch AVG loss: 0.5039468072354794\n","\n","Epoch: 1  train \n","Loss: 0.6510  Acc: 72.4638\n","benign precision: 70.0000  recall: 70.0000\n","benign sensitivity: 70.0000  specificity: 76.3158\n","benign FPR: 23.6842  NPV: 76.3158\n","benign TP: 21.0\n","benign TN: 29.0\n","benign FP: 9.0\n","benign FN: 9.0\n","malignant precision: 76.3158  recall: 76.3158\n","malignant sensitivity: 76.3158  specificity: 70.0000\n","malignant FPR: 30.0000  NPV: 70.0000\n","malignant TP: 29.0\n","malignant TN: 21.0\n","malignant FP: 9.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.3703  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 2.6348886489868164\n","minibatch AVG loss: 0.05361242825165391\n","Epoch: 2     train index of 5 minibatch: 2      time used: 2.349233388900757\n","minibatch AVG loss: 0.10996638610959053\n","Epoch: 2     train index of 5 minibatch: 3      time used: 2.3524222373962402\n","minibatch AVG loss: 0.03211902566254139\n","\n","Epoch: 2  train \n","Loss: 0.0580  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.2278  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 2.6516664028167725\n","minibatch AVG loss: 0.01742330938577652\n","Epoch: 3     train index of 5 minibatch: 2      time used: 2.3682100772857666\n","minibatch AVG loss: 0.06185529241338372\n","Epoch: 3     train index of 5 minibatch: 3      time used: 2.4018778800964355\n","minibatch AVG loss: 0.03849927154369652\n","\n","Epoch: 3  train \n","Loss: 0.0354  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.0983  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 2.7281880378723145\n","minibatch AVG loss: 0.008697702616336756\n","Epoch: 4     train index of 5 minibatch: 2      time used: 2.4325175285339355\n","minibatch AVG loss: 0.014091835508588701\n","Epoch: 4     train index of 5 minibatch: 3      time used: 2.432744264602661\n","minibatch AVG loss: 0.002075046289246529\n","\n","Epoch: 4  train \n","Loss: 0.0072  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.0856  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 2.761301279067993\n","minibatch AVG loss: 0.0012428450572770089\n","Epoch: 5     train index of 5 minibatch: 2      time used: 2.496051549911499\n","minibatch AVG loss: 0.006519802496768534\n","Epoch: 5     train index of 5 minibatch: 3      time used: 2.5160956382751465\n","minibatch AVG loss: 0.0019088134169578553\n","\n","Epoch: 5  train \n","Loss: 0.0042  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.0715  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 2.7495453357696533\n","minibatch AVG loss: 0.002340848575113341\n","Epoch: 6     train index of 5 minibatch: 2      time used: 2.5689780712127686\n","minibatch AVG loss: 0.003127199453592766\n","Epoch: 6     train index of 5 minibatch: 3      time used: 2.60066819190979\n","minibatch AVG loss: 0.0028945417958311736\n","\n","Epoch: 6  train \n","Loss: 0.0029  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.0920  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 2.826124429702759\n","minibatch AVG loss: 0.005721207641909132\n","Epoch: 7     train index of 5 minibatch: 2      time used: 2.660252332687378\n","minibatch AVG loss: 0.0027993739262456075\n","Epoch: 7     train index of 5 minibatch: 3      time used: 2.6844704151153564\n","minibatch AVG loss: 0.00042427463340573014\n","\n","Epoch: 7  train \n","Loss: 0.0027  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.0578  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 2.9095749855041504\n","minibatch AVG loss: 0.00028290653717704116\n","Epoch: 8     train index of 5 minibatch: 2      time used: 2.758202314376831\n","minibatch AVG loss: 0.000683697771455627\n","Epoch: 8     train index of 5 minibatch: 3      time used: 2.769547700881958\n","minibatch AVG loss: 0.0009985824406612664\n","\n","Epoch: 8  train \n","Loss: 0.0007  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.0574  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 2.9587457180023193\n","minibatch AVG loss: 0.0015468472680367994\n","Epoch: 9     train index of 5 minibatch: 2      time used: 2.7612781524658203\n","minibatch AVG loss: 0.00021550527890212834\n","Epoch: 9     train index of 5 minibatch: 3      time used: 2.7284960746765137\n","minibatch AVG loss: 0.010777531890198588\n","\n","Epoch: 9  train \n","Loss: 0.0037  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.0767  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 2.8947393894195557\n","minibatch AVG loss: 0.0016600332739471924\n","Epoch: 10     train index of 5 minibatch: 2      time used: 2.688905715942383\n","minibatch AVG loss: 0.016420174278027843\n","Epoch: 10     train index of 5 minibatch: 3      time used: 2.653947114944458\n","minibatch AVG loss: 0.0012989990831556498\n","\n","Epoch: 10  train \n","Loss: 0.0056  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.0421  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 2.8291492462158203\n","minibatch AVG loss: 0.0032241093769698635\n","Epoch: 11     train index of 5 minibatch: 2      time used: 2.6182384490966797\n","minibatch AVG loss: 0.0006830998216173611\n","Epoch: 11     train index of 5 minibatch: 3      time used: 2.590357780456543\n","minibatch AVG loss: 0.0004558044638542924\n","\n","Epoch: 11  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.0393  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 2.7962095737457275\n","minibatch AVG loss: 0.0004654858901631087\n","Epoch: 12     train index of 5 minibatch: 2      time used: 2.594712734222412\n","minibatch AVG loss: 0.0004307047667680308\n","Epoch: 12     train index of 5 minibatch: 3      time used: 2.5744354724884033\n","minibatch AVG loss: 0.00043447152274893596\n","\n","Epoch: 12  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.0457  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 2.7794837951660156\n","minibatch AVG loss: 0.0002970199886476621\n","Epoch: 13     train index of 5 minibatch: 2      time used: 2.5854194164276123\n","minibatch AVG loss: 0.0008568080374971032\n","Epoch: 13     train index of 5 minibatch: 3      time used: 2.5727899074554443\n","minibatch AVG loss: 0.00023620122392458142\n","\n","Epoch: 13  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.0481  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 2.7816622257232666\n","minibatch AVG loss: 0.0005391326325479895\n","Epoch: 14     train index of 5 minibatch: 2      time used: 2.5921871662139893\n","minibatch AVG loss: 0.00022989737881289328\n","Epoch: 14     train index of 5 minibatch: 3      time used: 2.5967109203338623\n","minibatch AVG loss: 0.0010054133090307005\n","\n","Epoch: 14  train \n","Loss: 0.0005  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.0472  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 2.8038759231567383\n","minibatch AVG loss: 0.0005098045490740333\n","Epoch: 15     train index of 5 minibatch: 2      time used: 2.6226680278778076\n","minibatch AVG loss: 0.0043950631283223626\n","Epoch: 15     train index of 5 minibatch: 3      time used: 2.6318907737731934\n","minibatch AVG loss: 0.002876964544702787\n","\n","Epoch: 15  train \n","Loss: 0.0024  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.0675  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 2.836594820022583\n","minibatch AVG loss: 0.0002016882921452634\n","Epoch: 16     train index of 5 minibatch: 2      time used: 2.661774158477783\n","minibatch AVG loss: 0.00026138616667594763\n","Epoch: 16     train index of 5 minibatch: 3      time used: 2.643134593963623\n","minibatch AVG loss: 0.00022029116662451996\n","\n","Epoch: 16  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.0794  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 2.856675148010254\n","minibatch AVG loss: 0.0005433788443042431\n","Epoch: 17     train index of 5 minibatch: 2      time used: 2.668253183364868\n","minibatch AVG loss: 0.0016040246220654808\n","Epoch: 17     train index of 5 minibatch: 3      time used: 2.6566414833068848\n","minibatch AVG loss: 0.00016248523752437904\n","\n","Epoch: 17  train \n","Loss: 0.0007  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.0603  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 2.859196901321411\n","minibatch AVG loss: 0.00015715670415374916\n","Epoch: 18     train index of 5 minibatch: 2      time used: 2.6818184852600098\n","minibatch AVG loss: 0.0004559568929835223\n","Epoch: 18     train index of 5 minibatch: 3      time used: 2.6497185230255127\n","minibatch AVG loss: 0.0006113325973274187\n","\n","Epoch: 18  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.0544  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 2.8465986251831055\n","minibatch AVG loss: 0.00020334990695118903\n","Epoch: 19     train index of 5 minibatch: 2      time used: 2.6596806049346924\n","minibatch AVG loss: 0.00012371077609714122\n","Epoch: 19     train index of 5 minibatch: 3      time used: 2.650125503540039\n","minibatch AVG loss: 0.00022297139512374997\n","\n","Epoch: 19  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.0572  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 2.8312394618988037\n","minibatch AVG loss: 0.0002569920799942338\n","Epoch: 20     train index of 5 minibatch: 2      time used: 2.6406474113464355\n","minibatch AVG loss: 0.0007034265225229319\n","Epoch: 20     train index of 5 minibatch: 3      time used: 2.630467653274536\n","minibatch AVG loss: 0.00025208077204297297\n","\n","Epoch: 20  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.0604  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 2.825120687484741\n","minibatch AVG loss: 0.00015861731590121052\n","Epoch: 21     train index of 5 minibatch: 2      time used: 2.6296911239624023\n","minibatch AVG loss: 0.00040496574583812615\n","Epoch: 21     train index of 5 minibatch: 3      time used: 2.6198792457580566\n","minibatch AVG loss: 0.00023170389686129057\n","\n","Epoch: 21  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.0603  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 2.811635971069336\n","minibatch AVG loss: 0.00021791485269204714\n","Epoch: 22     train index of 5 minibatch: 2      time used: 2.616386651992798\n","minibatch AVG loss: 0.00028190629091113807\n","Epoch: 22     train index of 5 minibatch: 3      time used: 2.60891056060791\n","minibatch AVG loss: 0.00017486961733084171\n","\n","Epoch: 22  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.0621  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 2.8117034435272217\n","minibatch AVG loss: 0.0003548446497006807\n","Epoch: 23     train index of 5 minibatch: 2      time used: 2.6271185874938965\n","minibatch AVG loss: 0.0005485604619025253\n","Epoch: 23     train index of 5 minibatch: 3      time used: 2.6104941368103027\n","minibatch AVG loss: 7.667100253456738e-05\n","\n","Epoch: 23  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.0656  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 2.8403024673461914\n","minibatch AVG loss: 0.0001520739300758578\n","Epoch: 24     train index of 5 minibatch: 2      time used: 2.631040334701538\n","minibatch AVG loss: 0.0006186398597492371\n","Epoch: 24     train index of 5 minibatch: 3      time used: 2.6275742053985596\n","minibatch AVG loss: 0.001270689426746685\n","\n","Epoch: 24  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.0659  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 2.8162405490875244\n","minibatch AVG loss: 0.00030221191846067084\n","Epoch: 25     train index of 5 minibatch: 2      time used: 2.6449337005615234\n","minibatch AVG loss: 0.00015498120792472035\n","Epoch: 25     train index of 5 minibatch: 3      time used: 2.6461293697357178\n","minibatch AVG loss: 0.0004605497286320315\n","\n","Epoch: 25  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.0698  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 2.838970422744751\n","minibatch AVG loss: 8.378819838981145e-05\n","Epoch: 26     train index of 5 minibatch: 2      time used: 2.6494054794311523\n","minibatch AVG loss: 0.0003682016511447728\n","Epoch: 26     train index of 5 minibatch: 3      time used: 2.6473469734191895\n","minibatch AVG loss: 0.00015561246964352905\n","\n","Epoch: 26  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.0687  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 2.8440520763397217\n","minibatch AVG loss: 0.00019954476738348603\n","Epoch: 27     train index of 5 minibatch: 2      time used: 2.6528446674346924\n","minibatch AVG loss: 0.0003081402595853433\n","Epoch: 27     train index of 5 minibatch: 3      time used: 2.65059232711792\n","minibatch AVG loss: 0.0002788695303024724\n","\n","Epoch: 27  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.0627  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 2.834453821182251\n","minibatch AVG loss: 0.0003153260455292184\n","Epoch: 28     train index of 5 minibatch: 2      time used: 2.6516947746276855\n","minibatch AVG loss: 8.911381612506375e-05\n","Epoch: 28     train index of 5 minibatch: 3      time used: 2.6383543014526367\n","minibatch AVG loss: 0.0008345619135070592\n","\n","Epoch: 28  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.0632  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 2.830700159072876\n","minibatch AVG loss: 0.00011628013962763361\n","Epoch: 29     train index of 5 minibatch: 2      time used: 2.6568424701690674\n","minibatch AVG loss: 0.0002263577829580754\n","Epoch: 29     train index of 5 minibatch: 3      time used: 2.6333353519439697\n","minibatch AVG loss: 0.0001283924124436453\n","\n","Epoch: 29  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.0657  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 2.834462881088257\n","minibatch AVG loss: 0.00018388778844382613\n","Epoch: 30     train index of 5 minibatch: 2      time used: 2.6385319232940674\n","minibatch AVG loss: 0.0001946895179571584\n","Epoch: 30     train index of 5 minibatch: 3      time used: 2.628221273422241\n","minibatch AVG loss: 0.0002552038604335394\n","\n","Epoch: 30  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.0676  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 2.832584857940674\n","minibatch AVG loss: 0.00026817421385203486\n","Epoch: 31     train index of 5 minibatch: 2      time used: 2.637481689453125\n","minibatch AVG loss: 0.0002695360053621698\n","Epoch: 31     train index of 5 minibatch: 3      time used: 2.626325845718384\n","minibatch AVG loss: 0.00018378782842773945\n","\n","Epoch: 31  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.0677  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 2.8330821990966797\n","minibatch AVG loss: 0.00017943906059372238\n","Epoch: 32     train index of 5 minibatch: 2      time used: 2.6375832557678223\n","minibatch AVG loss: 0.00023508605427196015\n","Epoch: 32     train index of 5 minibatch: 3      time used: 2.6242194175720215\n","minibatch AVG loss: 0.0002878715666156495\n","\n","Epoch: 32  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.0669  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 2.8182125091552734\n","minibatch AVG loss: 0.00018195396260125562\n","Epoch: 33     train index of 5 minibatch: 2      time used: 2.6272103786468506\n","minibatch AVG loss: 0.00010969209943141322\n","Epoch: 33     train index of 5 minibatch: 3      time used: 2.6303892135620117\n","minibatch AVG loss: 0.000477418349328218\n","\n","Epoch: 33  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.0679  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 2.8345212936401367\n","minibatch AVG loss: 0.0001822182457544841\n","Epoch: 34     train index of 5 minibatch: 2      time used: 2.6321208477020264\n","minibatch AVG loss: 0.0016814171001897193\n","Epoch: 34     train index of 5 minibatch: 3      time used: 2.619166135787964\n","minibatch AVG loss: 0.00013267351823742501\n","\n","Epoch: 34  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.0835  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 2.81964111328125\n","minibatch AVG loss: 0.0001881657102785539\n","Epoch: 35     train index of 5 minibatch: 2      time used: 2.630023956298828\n","minibatch AVG loss: 0.0016795016097603365\n","Epoch: 35     train index of 5 minibatch: 3      time used: 2.6213035583496094\n","minibatch AVG loss: 0.00036256187231629153\n","\n","Epoch: 35  train \n","Loss: 0.0007  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.0765  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 2.8160650730133057\n","minibatch AVG loss: 0.0005362812415114604\n","Epoch: 36     train index of 5 minibatch: 2      time used: 2.63423752784729\n","minibatch AVG loss: 0.00015184493240667508\n","Epoch: 36     train index of 5 minibatch: 3      time used: 2.622082233428955\n","minibatch AVG loss: 0.00033806018727773336\n","\n","Epoch: 36  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.0758  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 2.818718671798706\n","minibatch AVG loss: 0.00018138884997824788\n","Epoch: 37     train index of 5 minibatch: 2      time used: 2.6395812034606934\n","minibatch AVG loss: 7.160295499488711e-05\n","Epoch: 37     train index of 5 minibatch: 3      time used: 2.6316394805908203\n","minibatch AVG loss: 0.00021524421317735686\n","\n","Epoch: 37  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.0756  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 2.83266282081604\n","minibatch AVG loss: 0.0001953549410245614\n","Epoch: 38     train index of 5 minibatch: 2      time used: 2.639180898666382\n","minibatch AVG loss: 0.00010083982197102159\n","Epoch: 38     train index of 5 minibatch: 3      time used: 2.6265556812286377\n","minibatch AVG loss: 0.0003774303957470693\n","\n","Epoch: 38  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.0756  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 2.82920503616333\n","minibatch AVG loss: 6.700251979054883e-05\n","Epoch: 39     train index of 5 minibatch: 2      time used: 2.639525890350342\n","minibatch AVG loss: 0.00038813176506664605\n","Epoch: 39     train index of 5 minibatch: 3      time used: 2.6331288814544678\n","minibatch AVG loss: 0.00019122427111142314\n","\n","Epoch: 39  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.0758  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 2.825035810470581\n","minibatch AVG loss: 0.00010950617288472131\n","Epoch: 40     train index of 5 minibatch: 2      time used: 2.6508121490478516\n","minibatch AVG loss: 0.00017466537101427094\n","Epoch: 40     train index of 5 minibatch: 3      time used: 2.6235506534576416\n","minibatch AVG loss: 0.0003690706278575817\n","\n","Epoch: 40  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.0765  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 2.835124969482422\n","minibatch AVG loss: 0.0002268553085741587\n","Epoch: 41     train index of 5 minibatch: 2      time used: 2.6495187282562256\n","minibatch AVG loss: 0.0003918944268662017\n","Epoch: 41     train index of 5 minibatch: 3      time used: 2.635270833969116\n","minibatch AVG loss: 0.00018228511289635206\n","\n","Epoch: 41  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.0765  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 2.844403028488159\n","minibatch AVG loss: 0.0002765504365015659\n","Epoch: 42     train index of 5 minibatch: 2      time used: 2.653104305267334\n","minibatch AVG loss: 0.00018705041220528073\n","Epoch: 42     train index of 5 minibatch: 3      time used: 2.6323845386505127\n","minibatch AVG loss: 0.00012069651820638683\n","\n","Epoch: 42  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.0755  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 2.8355424404144287\n","minibatch AVG loss: 0.00019182585056114475\n","Epoch: 43     train index of 5 minibatch: 2      time used: 2.6445422172546387\n","minibatch AVG loss: 0.0007502379361540079\n","Epoch: 43     train index of 5 minibatch: 3      time used: 2.648451089859009\n","minibatch AVG loss: 0.00012611981146619654\n","\n","Epoch: 43  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.0755  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 2.844021797180176\n","minibatch AVG loss: 0.00025263981588068417\n","Epoch: 44     train index of 5 minibatch: 2      time used: 2.6574153900146484\n","minibatch AVG loss: 0.0004406447031215066\n","Epoch: 44     train index of 5 minibatch: 3      time used: 2.6451754570007324\n","minibatch AVG loss: 0.00016489476365677547\n","\n","Epoch: 44  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.0751  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 2.849889039993286\n","minibatch AVG loss: 0.0002242192826088285\n","Epoch: 45     train index of 5 minibatch: 2      time used: 2.657562255859375\n","minibatch AVG loss: 8.829663274809719e-05\n","Epoch: 45     train index of 5 minibatch: 3      time used: 2.643651247024536\n","minibatch AVG loss: 0.00017990624182857574\n","\n","Epoch: 45  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.0747  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 2.841282606124878\n","minibatch AVG loss: 0.00018987645162269474\n","Epoch: 46     train index of 5 minibatch: 2      time used: 2.6574959754943848\n","minibatch AVG loss: 0.00010459111072123051\n","Epoch: 46     train index of 5 minibatch: 3      time used: 2.6427111625671387\n","minibatch AVG loss: 0.00033651040430413557\n","\n","Epoch: 46  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.0739  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 2.8441684246063232\n","minibatch AVG loss: 0.00015438806294696406\n","Epoch: 47     train index of 5 minibatch: 2      time used: 2.6546080112457275\n","minibatch AVG loss: 0.0003203658350685146\n","Epoch: 47     train index of 5 minibatch: 3      time used: 2.633035898208618\n","minibatch AVG loss: 9.15845583222108e-05\n","\n","Epoch: 47  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.0737  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 2.829697370529175\n","minibatch AVG loss: 0.00013018442696193232\n","Epoch: 48     train index of 5 minibatch: 2      time used: 2.636655330657959\n","minibatch AVG loss: 0.0001732499076751992\n","Epoch: 48     train index of 5 minibatch: 3      time used: 2.6381654739379883\n","minibatch AVG loss: 0.00017558799008838833\n","\n","Epoch: 48  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.0731  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 2.8278326988220215\n","minibatch AVG loss: 0.00020468577567953617\n","Epoch: 49     train index of 5 minibatch: 2      time used: 2.650557279586792\n","minibatch AVG loss: 0.0004051943076774478\n","Epoch: 49     train index of 5 minibatch: 3      time used: 2.6322453022003174\n","minibatch AVG loss: 0.000371313044524868\n","\n","Epoch: 49  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.0732  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 2.8351192474365234\n","minibatch AVG loss: 0.00036608351219911126\n","Epoch: 50     train index of 5 minibatch: 2      time used: 2.644253730773926\n","minibatch AVG loss: 0.0002491932620614534\n","Epoch: 50     train index of 5 minibatch: 3      time used: 2.6329519748687744\n","minibatch AVG loss: 0.00029082954497425815\n","\n","Epoch: 50  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.0729  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 8m 21s\n","Best epoch idx:  14\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ViT_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx ViT_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXuVfzuAZ7IB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651153927007,"user_tz":-480,"elapsed":515711,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"57428e24-e80e-4b15-b474-00d8e9b9869c"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name='Cutout', backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 2.3224, -1.6767]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 24, 24]         590,592\n","          Identity-2             [-1, 576, 768]               0\n","        PatchEmbed-3             [-1, 576, 768]               0\n","           Dropout-4             [-1, 577, 768]               0\n","         LayerNorm-5             [-1, 577, 768]           1,536\n","            Linear-6            [-1, 577, 2304]       1,771,776\n","           Dropout-7         [-1, 12, 577, 577]               0\n","            Linear-8             [-1, 577, 768]         590,592\n","           Dropout-9             [-1, 577, 768]               0\n","        Attention-10             [-1, 577, 768]               0\n","         Identity-11             [-1, 577, 768]               0\n","        LayerNorm-12             [-1, 577, 768]           1,536\n","           Linear-13            [-1, 577, 3072]       2,362,368\n","             GELU-14            [-1, 577, 3072]               0\n","          Dropout-15            [-1, 577, 3072]               0\n","           Linear-16             [-1, 577, 768]       2,360,064\n","          Dropout-17             [-1, 577, 768]               0\n","              Mlp-18             [-1, 577, 768]               0\n","         Identity-19             [-1, 577, 768]               0\n","            Block-20             [-1, 577, 768]               0\n","        LayerNorm-21             [-1, 577, 768]           1,536\n","           Linear-22            [-1, 577, 2304]       1,771,776\n","          Dropout-23         [-1, 12, 577, 577]               0\n","           Linear-24             [-1, 577, 768]         590,592\n","          Dropout-25             [-1, 577, 768]               0\n","        Attention-26             [-1, 577, 768]               0\n","         Identity-27             [-1, 577, 768]               0\n","        LayerNorm-28             [-1, 577, 768]           1,536\n","           Linear-29            [-1, 577, 3072]       2,362,368\n","             GELU-30            [-1, 577, 3072]               0\n","          Dropout-31            [-1, 577, 3072]               0\n","           Linear-32             [-1, 577, 768]       2,360,064\n","          Dropout-33             [-1, 577, 768]               0\n","              Mlp-34             [-1, 577, 768]               0\n","         Identity-35             [-1, 577, 768]               0\n","            Block-36             [-1, 577, 768]               0\n","        LayerNorm-37             [-1, 577, 768]           1,536\n","           Linear-38            [-1, 577, 2304]       1,771,776\n","          Dropout-39         [-1, 12, 577, 577]               0\n","           Linear-40             [-1, 577, 768]         590,592\n","          Dropout-41             [-1, 577, 768]               0\n","        Attention-42             [-1, 577, 768]               0\n","         Identity-43             [-1, 577, 768]               0\n","        LayerNorm-44             [-1, 577, 768]           1,536\n","           Linear-45            [-1, 577, 3072]       2,362,368\n","             GELU-46            [-1, 577, 3072]               0\n","          Dropout-47            [-1, 577, 3072]               0\n","           Linear-48             [-1, 577, 768]       2,360,064\n","          Dropout-49             [-1, 577, 768]               0\n","              Mlp-50             [-1, 577, 768]               0\n","         Identity-51             [-1, 577, 768]               0\n","            Block-52             [-1, 577, 768]               0\n","        LayerNorm-53             [-1, 577, 768]           1,536\n","           Linear-54            [-1, 577, 2304]       1,771,776\n","          Dropout-55         [-1, 12, 577, 577]               0\n","           Linear-56             [-1, 577, 768]         590,592\n","          Dropout-57             [-1, 577, 768]               0\n","        Attention-58             [-1, 577, 768]               0\n","         Identity-59             [-1, 577, 768]               0\n","        LayerNorm-60             [-1, 577, 768]           1,536\n","           Linear-61            [-1, 577, 3072]       2,362,368\n","             GELU-62            [-1, 577, 3072]               0\n","          Dropout-63            [-1, 577, 3072]               0\n","           Linear-64             [-1, 577, 768]       2,360,064\n","          Dropout-65             [-1, 577, 768]               0\n","              Mlp-66             [-1, 577, 768]               0\n","         Identity-67             [-1, 577, 768]               0\n","            Block-68             [-1, 577, 768]               0\n","        LayerNorm-69             [-1, 577, 768]           1,536\n","           Linear-70            [-1, 577, 2304]       1,771,776\n","          Dropout-71         [-1, 12, 577, 577]               0\n","           Linear-72             [-1, 577, 768]         590,592\n","          Dropout-73             [-1, 577, 768]               0\n","        Attention-74             [-1, 577, 768]               0\n","         Identity-75             [-1, 577, 768]               0\n","        LayerNorm-76             [-1, 577, 768]           1,536\n","           Linear-77            [-1, 577, 3072]       2,362,368\n","             GELU-78            [-1, 577, 3072]               0\n","          Dropout-79            [-1, 577, 3072]               0\n","           Linear-80             [-1, 577, 768]       2,360,064\n","          Dropout-81             [-1, 577, 768]               0\n","              Mlp-82             [-1, 577, 768]               0\n","         Identity-83             [-1, 577, 768]               0\n","            Block-84             [-1, 577, 768]               0\n","        LayerNorm-85             [-1, 577, 768]           1,536\n","           Linear-86            [-1, 577, 2304]       1,771,776\n","          Dropout-87         [-1, 12, 577, 577]               0\n","           Linear-88             [-1, 577, 768]         590,592\n","          Dropout-89             [-1, 577, 768]               0\n","        Attention-90             [-1, 577, 768]               0\n","         Identity-91             [-1, 577, 768]               0\n","        LayerNorm-92             [-1, 577, 768]           1,536\n","           Linear-93            [-1, 577, 3072]       2,362,368\n","             GELU-94            [-1, 577, 3072]               0\n","          Dropout-95            [-1, 577, 3072]               0\n","           Linear-96             [-1, 577, 768]       2,360,064\n","          Dropout-97             [-1, 577, 768]               0\n","              Mlp-98             [-1, 577, 768]               0\n","         Identity-99             [-1, 577, 768]               0\n","           Block-100             [-1, 577, 768]               0\n","       LayerNorm-101             [-1, 577, 768]           1,536\n","          Linear-102            [-1, 577, 2304]       1,771,776\n","         Dropout-103         [-1, 12, 577, 577]               0\n","          Linear-104             [-1, 577, 768]         590,592\n","         Dropout-105             [-1, 577, 768]               0\n","       Attention-106             [-1, 577, 768]               0\n","        Identity-107             [-1, 577, 768]               0\n","       LayerNorm-108             [-1, 577, 768]           1,536\n","          Linear-109            [-1, 577, 3072]       2,362,368\n","            GELU-110            [-1, 577, 3072]               0\n","         Dropout-111            [-1, 577, 3072]               0\n","          Linear-112             [-1, 577, 768]       2,360,064\n","         Dropout-113             [-1, 577, 768]               0\n","             Mlp-114             [-1, 577, 768]               0\n","        Identity-115             [-1, 577, 768]               0\n","           Block-116             [-1, 577, 768]               0\n","       LayerNorm-117             [-1, 577, 768]           1,536\n","          Linear-118            [-1, 577, 2304]       1,771,776\n","         Dropout-119         [-1, 12, 577, 577]               0\n","          Linear-120             [-1, 577, 768]         590,592\n","         Dropout-121             [-1, 577, 768]               0\n","       Attention-122             [-1, 577, 768]               0\n","        Identity-123             [-1, 577, 768]               0\n","       LayerNorm-124             [-1, 577, 768]           1,536\n","          Linear-125            [-1, 577, 3072]       2,362,368\n","            GELU-126            [-1, 577, 3072]               0\n","         Dropout-127            [-1, 577, 3072]               0\n","          Linear-128             [-1, 577, 768]       2,360,064\n","         Dropout-129             [-1, 577, 768]               0\n","             Mlp-130             [-1, 577, 768]               0\n","        Identity-131             [-1, 577, 768]               0\n","           Block-132             [-1, 577, 768]               0\n","       LayerNorm-133             [-1, 577, 768]           1,536\n","          Linear-134            [-1, 577, 2304]       1,771,776\n","         Dropout-135         [-1, 12, 577, 577]               0\n","          Linear-136             [-1, 577, 768]         590,592\n","         Dropout-137             [-1, 577, 768]               0\n","       Attention-138             [-1, 577, 768]               0\n","        Identity-139             [-1, 577, 768]               0\n","       LayerNorm-140             [-1, 577, 768]           1,536\n","          Linear-141            [-1, 577, 3072]       2,362,368\n","            GELU-142            [-1, 577, 3072]               0\n","         Dropout-143            [-1, 577, 3072]               0\n","          Linear-144             [-1, 577, 768]       2,360,064\n","         Dropout-145             [-1, 577, 768]               0\n","             Mlp-146             [-1, 577, 768]               0\n","        Identity-147             [-1, 577, 768]               0\n","           Block-148             [-1, 577, 768]               0\n","       LayerNorm-149             [-1, 577, 768]           1,536\n","          Linear-150            [-1, 577, 2304]       1,771,776\n","         Dropout-151         [-1, 12, 577, 577]               0\n","          Linear-152             [-1, 577, 768]         590,592\n","         Dropout-153             [-1, 577, 768]               0\n","       Attention-154             [-1, 577, 768]               0\n","        Identity-155             [-1, 577, 768]               0\n","       LayerNorm-156             [-1, 577, 768]           1,536\n","          Linear-157            [-1, 577, 3072]       2,362,368\n","            GELU-158            [-1, 577, 3072]               0\n","         Dropout-159            [-1, 577, 3072]               0\n","          Linear-160             [-1, 577, 768]       2,360,064\n","         Dropout-161             [-1, 577, 768]               0\n","             Mlp-162             [-1, 577, 768]               0\n","        Identity-163             [-1, 577, 768]               0\n","           Block-164             [-1, 577, 768]               0\n","       LayerNorm-165             [-1, 577, 768]           1,536\n","          Linear-166            [-1, 577, 2304]       1,771,776\n","         Dropout-167         [-1, 12, 577, 577]               0\n","          Linear-168             [-1, 577, 768]         590,592\n","         Dropout-169             [-1, 577, 768]               0\n","       Attention-170             [-1, 577, 768]               0\n","        Identity-171             [-1, 577, 768]               0\n","       LayerNorm-172             [-1, 577, 768]           1,536\n","          Linear-173            [-1, 577, 3072]       2,362,368\n","            GELU-174            [-1, 577, 3072]               0\n","         Dropout-175            [-1, 577, 3072]               0\n","          Linear-176             [-1, 577, 768]       2,360,064\n","         Dropout-177             [-1, 577, 768]               0\n","             Mlp-178             [-1, 577, 768]               0\n","        Identity-179             [-1, 577, 768]               0\n","           Block-180             [-1, 577, 768]               0\n","       LayerNorm-181             [-1, 577, 768]           1,536\n","          Linear-182            [-1, 577, 2304]       1,771,776\n","         Dropout-183         [-1, 12, 577, 577]               0\n","          Linear-184             [-1, 577, 768]         590,592\n","         Dropout-185             [-1, 577, 768]               0\n","       Attention-186             [-1, 577, 768]               0\n","        Identity-187             [-1, 577, 768]               0\n","       LayerNorm-188             [-1, 577, 768]           1,536\n","          Linear-189            [-1, 577, 3072]       2,362,368\n","            GELU-190            [-1, 577, 3072]               0\n","         Dropout-191            [-1, 577, 3072]               0\n","          Linear-192             [-1, 577, 768]       2,360,064\n","         Dropout-193             [-1, 577, 768]               0\n","             Mlp-194             [-1, 577, 768]               0\n","        Identity-195             [-1, 577, 768]               0\n","           Block-196             [-1, 577, 768]               0\n","       LayerNorm-197             [-1, 577, 768]           1,536\n","        Identity-198                  [-1, 768]               0\n","          Linear-199                    [-1, 2]           1,538\n","================================================================\n","Total params: 85,648,130\n","Trainable params: 85,648,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1437.49\n","Params size (MB): 326.72\n","Estimated Total Size (MB): 1765.90\n","----------------------------------------------------------------\n","model : ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 2.77125883102417\n","minibatch AVG loss: 0.5541215024888515\n","Epoch: 1     train index of 5 minibatch: 2      time used: 2.572490930557251\n","minibatch AVG loss: 1.5625541523098945\n","Epoch: 1     train index of 5 minibatch: 3      time used: 2.5958876609802246\n","minibatch AVG loss: 0.43790491968393325\n","\n","Epoch: 1  train \n","Loss: 0.7454  Acc: 75.3623\n","benign precision: 73.3333  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 78.9474\n","benign FPR: 21.0526  NPV: 78.9474\n","benign TP: 22.0\n","benign TN: 30.0\n","benign FP: 8.0\n","benign FN: 8.0\n","malignant precision: 78.9474  recall: 78.9474\n","malignant sensitivity: 78.9474  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 73.3333\n","malignant TP: 30.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 8.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.7400  Acc: 75.0000\n","benign precision: 100.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 69.2308\n","benign TP: 3.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 69.2308  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 2.8299100399017334\n","minibatch AVG loss: 0.3058598801493645\n","Epoch: 2     train index of 5 minibatch: 2      time used: 2.6502909660339355\n","minibatch AVG loss: 0.1489327987190336\n","Epoch: 2     train index of 5 minibatch: 3      time used: 2.6707255840301514\n","minibatch AVG loss: 0.12913268222473562\n","\n","Epoch: 2  train \n","Loss: 0.1700  Acc: 92.7536\n","benign precision: 93.1034  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 94.8718\n","benign TP: 27.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 93.1034\n","malignant TP: 37.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.3130  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 3.0018858909606934\n","minibatch AVG loss: 0.25402083839289846\n","Epoch: 3     train index of 5 minibatch: 2      time used: 2.7301089763641357\n","minibatch AVG loss: 0.07464597090147436\n","Epoch: 3     train index of 5 minibatch: 3      time used: 2.734804391860962\n","minibatch AVG loss: 0.17438859939575196\n","\n","Epoch: 3  train \n","Loss: 0.1483  Acc: 91.3043\n","benign precision: 90.0000  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 94.7368\n","benign TP: 27.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 90.0000\n","malignant TP: 36.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.1645  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 2.9859094619750977\n","minibatch AVG loss: 0.040486601273005365\n","Epoch: 4     train index of 5 minibatch: 2      time used: 2.70798921585083\n","minibatch AVG loss: 0.020550167144392617\n","Epoch: 4     train index of 5 minibatch: 3      time used: 2.6827986240386963\n","minibatch AVG loss: 0.017945469540427438\n","\n","Epoch: 4  train \n","Loss: 0.0229  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.0896  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 2.9524877071380615\n","minibatch AVG loss: 0.0013073714246274904\n","Epoch: 5     train index of 5 minibatch: 2      time used: 2.6724658012390137\n","minibatch AVG loss: 0.009140925892279484\n","Epoch: 5     train index of 5 minibatch: 3      time used: 2.6564157009124756\n","minibatch AVG loss: 0.009372862102463842\n","\n","Epoch: 5  train \n","Loss: 0.0117  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.0801  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 2.825063467025757\n","minibatch AVG loss: 0.009636792162200435\n","Epoch: 6     train index of 5 minibatch: 2      time used: 2.6447720527648926\n","minibatch AVG loss: 0.0036900902290653904\n","Epoch: 6     train index of 5 minibatch: 3      time used: 2.6261708736419678\n","minibatch AVG loss: 0.0055212379058502846\n","\n","Epoch: 6  train \n","Loss: 0.0107  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.1593  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 2.8117315769195557\n","minibatch AVG loss: 0.01679836317161971\n","Epoch: 7     train index of 5 minibatch: 2      time used: 2.6168253421783447\n","minibatch AVG loss: 0.0025368116730533075\n","Epoch: 7     train index of 5 minibatch: 3      time used: 2.6106560230255127\n","minibatch AVG loss: 0.014066628167347517\n","\n","Epoch: 7  train \n","Loss: 0.0105  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.0648  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 2.8132922649383545\n","minibatch AVG loss: 0.00024186290575016755\n","Epoch: 8     train index of 5 minibatch: 2      time used: 2.6203906536102295\n","minibatch AVG loss: 0.003389257918752264\n","Epoch: 8     train index of 5 minibatch: 3      time used: 2.605909824371338\n","minibatch AVG loss: 0.12098816499055828\n","\n","Epoch: 8  train \n","Loss: 0.0362  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.1081  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 2.8029706478118896\n","minibatch AVG loss: 0.008077357215006487\n","Epoch: 9     train index of 5 minibatch: 2      time used: 2.6250805854797363\n","minibatch AVG loss: 0.18594993615406566\n","Epoch: 9     train index of 5 minibatch: 3      time used: 2.627134323120117\n","minibatch AVG loss: 0.003803141525713727\n","\n","Epoch: 9  train \n","Loss: 0.0578  Acc: 95.6522\n","benign precision: 93.5484  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.5484\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.3954  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 2.8282265663146973\n","minibatch AVG loss: 0.1582011852719006\n","Epoch: 10     train index of 5 minibatch: 2      time used: 2.6340482234954834\n","minibatch AVG loss: 0.024654008969082496\n","Epoch: 10     train index of 5 minibatch: 3      time used: 2.631795883178711\n","minibatch AVG loss: 0.13262701076455413\n","\n","Epoch: 10  train \n","Loss: 0.0915  Acc: 92.7536\n","benign precision: 93.1034  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 94.8718\n","benign TP: 27.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 93.1034\n","malignant TP: 37.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.0228  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 2.822779417037964\n","minibatch AVG loss: 0.0009369827632326633\n","Epoch: 11     train index of 5 minibatch: 2      time used: 2.6564042568206787\n","minibatch AVG loss: 0.00011822335800388828\n","Epoch: 11     train index of 5 minibatch: 3      time used: 2.6369411945343018\n","minibatch AVG loss: 0.003079234197502956\n","\n","Epoch: 11  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.1558  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 2.855257749557495\n","minibatch AVG loss: 0.09461522712372243\n","Epoch: 12     train index of 5 minibatch: 2      time used: 2.661426305770874\n","minibatch AVG loss: 0.003996072878362611\n","Epoch: 12     train index of 5 minibatch: 3      time used: 2.6502437591552734\n","minibatch AVG loss: 0.005066109070321545\n","\n","Epoch: 12  train \n","Loss: 0.0525  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.0275  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 2.842618465423584\n","minibatch AVG loss: 0.0034144605888286605\n","Epoch: 13     train index of 5 minibatch: 2      time used: 2.645603895187378\n","minibatch AVG loss: 0.008102359391341452\n","Epoch: 13     train index of 5 minibatch: 3      time used: 2.6379103660583496\n","minibatch AVG loss: 0.006842147922725417\n","\n","Epoch: 13  train \n","Loss: 0.0056  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.2825  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 2.8570001125335693\n","minibatch AVG loss: 0.016271002541179767\n","Epoch: 14     train index of 5 minibatch: 2      time used: 2.658827543258667\n","minibatch AVG loss: 0.0002411214083622326\n","Epoch: 14     train index of 5 minibatch: 3      time used: 2.6403896808624268\n","minibatch AVG loss: 0.0030413304801186316\n","\n","Epoch: 14  train \n","Loss: 0.0057  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.0619  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 2.8448832035064697\n","minibatch AVG loss: 0.00017211833146575374\n","Epoch: 15     train index of 5 minibatch: 2      time used: 2.650007724761963\n","minibatch AVG loss: 0.016728161513310625\n","Epoch: 15     train index of 5 minibatch: 3      time used: 2.637921094894409\n","minibatch AVG loss: 6.73926442686934e-05\n","\n","Epoch: 15  train \n","Loss: 0.0049  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.2158  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 2.8354575634002686\n","minibatch AVG loss: 0.001975908887652622\n","Epoch: 16     train index of 5 minibatch: 2      time used: 2.649596691131592\n","minibatch AVG loss: 0.0006254168751183897\n","Epoch: 16     train index of 5 minibatch: 3      time used: 2.6348743438720703\n","minibatch AVG loss: 0.0006809497185258806\n","\n","Epoch: 16  train \n","Loss: 0.0011  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.2578  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 2.8350186347961426\n","minibatch AVG loss: 0.0005284138216666179\n","Epoch: 17     train index of 5 minibatch: 2      time used: 2.647754669189453\n","minibatch AVG loss: 0.003395759495515449\n","Epoch: 17     train index of 5 minibatch: 3      time used: 2.6275525093078613\n","minibatch AVG loss: 0.00019139044288749574\n","\n","Epoch: 17  train \n","Loss: 0.0100  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.1904  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 2.836113929748535\n","minibatch AVG loss: 5.9043824694526845e-05\n","Epoch: 18     train index of 5 minibatch: 2      time used: 2.6396234035491943\n","minibatch AVG loss: 0.03452455466840547\n","Epoch: 18     train index of 5 minibatch: 3      time used: 2.6320128440856934\n","minibatch AVG loss: 0.0021094551844726083\n","\n","Epoch: 18  train \n","Loss: 0.0107  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.1614  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 2.8370449542999268\n","minibatch AVG loss: 7.893841539043933e-05\n","Epoch: 19     train index of 5 minibatch: 2      time used: 2.6502277851104736\n","minibatch AVG loss: 0.0014683753538065503\n","Epoch: 19     train index of 5 minibatch: 3      time used: 2.6301772594451904\n","minibatch AVG loss: 0.09434408647757664\n","\n","Epoch: 19  train \n","Loss: 0.0278  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.0113  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 2.8322510719299316\n","minibatch AVG loss: 0.0037275943786880816\n","Epoch: 20     train index of 5 minibatch: 2      time used: 2.6398940086364746\n","minibatch AVG loss: 0.07230579210590804\n","Epoch: 20     train index of 5 minibatch: 3      time used: 2.626394033432007\n","minibatch AVG loss: 0.0013526531000934484\n","\n","Epoch: 20  train \n","Loss: 0.0225  Acc: 97.1014\n","benign precision: 96.6667  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.6667\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.0145  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 2.829176187515259\n","minibatch AVG loss: 0.00032600661270407725\n","Epoch: 21     train index of 5 minibatch: 2      time used: 2.6450939178466797\n","minibatch AVG loss: 0.00023593801381593948\n","Epoch: 21     train index of 5 minibatch: 3      time used: 2.622218132019043\n","minibatch AVG loss: 0.0005755198768383707\n","\n","Epoch: 21  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.0740  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 2.837305784225464\n","minibatch AVG loss: 0.00017509838471596595\n","Epoch: 22     train index of 5 minibatch: 2      time used: 2.635514736175537\n","minibatch AVG loss: 0.00014997483663137245\n","Epoch: 22     train index of 5 minibatch: 3      time used: 2.6284070014953613\n","minibatch AVG loss: 0.004102214740601085\n","\n","Epoch: 22  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.0577  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 2.8393142223358154\n","minibatch AVG loss: 0.0010243274806271073\n","Epoch: 23     train index of 5 minibatch: 2      time used: 2.6367852687835693\n","minibatch AVG loss: 5.390342985265306e-05\n","Epoch: 23     train index of 5 minibatch: 3      time used: 2.630683183670044\n","minibatch AVG loss: 0.0023095927657323045\n","\n","Epoch: 23  train \n","Loss: 0.0010  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.0407  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 2.8502275943756104\n","minibatch AVG loss: 1.889963386929594e-05\n","Epoch: 24     train index of 5 minibatch: 2      time used: 2.6396706104278564\n","minibatch AVG loss: 0.0004225565786782681\n","Epoch: 24     train index of 5 minibatch: 3      time used: 2.630577564239502\n","minibatch AVG loss: 0.00011590680551307742\n","\n","Epoch: 24  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.0451  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 2.8420512676239014\n","minibatch AVG loss: 0.0006777310634788591\n","Epoch: 25     train index of 5 minibatch: 2      time used: 2.635936737060547\n","minibatch AVG loss: 5.125777561261202e-05\n","Epoch: 25     train index of 5 minibatch: 3      time used: 2.635755777359009\n","minibatch AVG loss: 0.0006600511300348444\n","\n","Epoch: 25  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.0390  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 2.8268022537231445\n","minibatch AVG loss: 9.130116798132804e-05\n","Epoch: 26     train index of 5 minibatch: 2      time used: 2.6457645893096924\n","minibatch AVG loss: 0.00136433252828283\n","Epoch: 26     train index of 5 minibatch: 3      time used: 2.631375789642334\n","minibatch AVG loss: 0.00038576564534764657\n","\n","Epoch: 26  train \n","Loss: 0.0005  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.0288  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 2.8324153423309326\n","minibatch AVG loss: 0.002102079597352713\n","Epoch: 27     train index of 5 minibatch: 2      time used: 2.6420705318450928\n","minibatch AVG loss: 0.00033819324871728893\n","Epoch: 27     train index of 5 minibatch: 3      time used: 2.6322641372680664\n","minibatch AVG loss: 3.3822365003288725e-05\n","\n","Epoch: 27  train \n","Loss: 0.0007  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.0323  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 2.855461835861206\n","minibatch AVG loss: 0.0021538356919336365\n","Epoch: 28     train index of 5 minibatch: 2      time used: 2.6491222381591797\n","minibatch AVG loss: 4.24152083496665e-05\n","Epoch: 28     train index of 5 minibatch: 3      time used: 2.6414597034454346\n","minibatch AVG loss: 0.00024021018285793616\n","\n","Epoch: 28  train \n","Loss: 0.0008  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.0251  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 2.8423914909362793\n","minibatch AVG loss: 0.001292783560757016\n","Epoch: 29     train index of 5 minibatch: 2      time used: 2.650278329849243\n","minibatch AVG loss: 0.00013973471295685157\n","Epoch: 29     train index of 5 minibatch: 3      time used: 2.637197971343994\n","minibatch AVG loss: 0.013519714977883268\n","\n","Epoch: 29  train \n","Loss: 0.0043  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.0116  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 2.841837167739868\n","minibatch AVG loss: 2.8984463097003754e-05\n","Epoch: 30     train index of 5 minibatch: 2      time used: 2.655897855758667\n","minibatch AVG loss: 0.00038884157047505143\n","Epoch: 30     train index of 5 minibatch: 3      time used: 2.6488940715789795\n","minibatch AVG loss: 0.00014976987476984506\n","\n","Epoch: 30  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.0062  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 2.850255250930786\n","minibatch AVG loss: 0.00018676027723358858\n","Epoch: 31     train index of 5 minibatch: 2      time used: 2.6540091037750244\n","minibatch AVG loss: 0.011805430471213185\n","Epoch: 31     train index of 5 minibatch: 3      time used: 2.6363604068756104\n","minibatch AVG loss: 0.0002959848085026806\n","\n","Epoch: 31  train \n","Loss: 0.0036  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.0135  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 2.8373594284057617\n","minibatch AVG loss: 0.0014794143153267214\n","Epoch: 32     train index of 5 minibatch: 2      time used: 2.658407211303711\n","minibatch AVG loss: 2.927985350424933e-05\n","Epoch: 32     train index of 5 minibatch: 3      time used: 2.6331746578216553\n","minibatch AVG loss: 3.534646575644729e-05\n","\n","Epoch: 32  train \n","Loss: 0.0005  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.0165  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 2.857517719268799\n","minibatch AVG loss: 0.0027022045677767893\n","Epoch: 33     train index of 5 minibatch: 2      time used: 2.652501344680786\n","minibatch AVG loss: 6.461994312303431e-05\n","Epoch: 33     train index of 5 minibatch: 3      time used: 2.628298759460449\n","minibatch AVG loss: 0.0005054622122770524\n","\n","Epoch: 33  train \n","Loss: 0.0010  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.0122  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 2.845611095428467\n","minibatch AVG loss: 0.0001055398166499799\n","Epoch: 34     train index of 5 minibatch: 2      time used: 2.654463529586792\n","minibatch AVG loss: 0.0008840668111588457\n","Epoch: 34     train index of 5 minibatch: 3      time used: 2.6356303691864014\n","minibatch AVG loss: 0.002049840990730445\n","\n","Epoch: 34  train \n","Loss: 0.0009  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.0104  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 2.83669376373291\n","minibatch AVG loss: 1.5091409477463457e-05\n","Epoch: 35     train index of 5 minibatch: 2      time used: 2.6540939807891846\n","minibatch AVG loss: 5.648827163895476e-05\n","Epoch: 35     train index of 5 minibatch: 3      time used: 2.62471342086792\n","minibatch AVG loss: 0.00025537321343733766\n","\n","Epoch: 35  train \n","Loss: 0.0001  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.0097  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 2.832003593444824\n","minibatch AVG loss: 4.089545109309256e-05\n","Epoch: 36     train index of 5 minibatch: 2      time used: 2.644695520401001\n","minibatch AVG loss: 2.673487865649804e-05\n","Epoch: 36     train index of 5 minibatch: 3      time used: 2.64383864402771\n","minibatch AVG loss: 3.4974212758243085e-05\n","\n","Epoch: 36  train \n","Loss: 0.0000  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.0096  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 2.8343141078948975\n","minibatch AVG loss: 0.01174267476087607\n","Epoch: 37     train index of 5 minibatch: 2      time used: 2.643033742904663\n","minibatch AVG loss: 1.1652504667836183e-05\n","Epoch: 37     train index of 5 minibatch: 3      time used: 2.629709243774414\n","minibatch AVG loss: 0.0011987866719664452\n","\n","Epoch: 37  train \n","Loss: 0.0038  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.0268  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 2.8150205612182617\n","minibatch AVG loss: 3.636300757534627e-05\n","Epoch: 38     train index of 5 minibatch: 2      time used: 2.6464695930480957\n","minibatch AVG loss: 1.550856950416346e-05\n","Epoch: 38     train index of 5 minibatch: 3      time used: 2.633671522140503\n","minibatch AVG loss: 4.686693077928794e-05\n","\n","Epoch: 38  train \n","Loss: 0.0000  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.0327  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 2.8379809856414795\n","minibatch AVG loss: 3.176024547428824e-05\n","Epoch: 39     train index of 5 minibatch: 2      time used: 2.640394926071167\n","minibatch AVG loss: 0.0010199348095284223\n","Epoch: 39     train index of 5 minibatch: 3      time used: 2.6385445594787598\n","minibatch AVG loss: 0.0007198145212896634\n","\n","Epoch: 39  train \n","Loss: 0.0007  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.0310  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 2.839048147201538\n","minibatch AVG loss: 4.487649512157077e-05\n","Epoch: 40     train index of 5 minibatch: 2      time used: 2.6512250900268555\n","minibatch AVG loss: 0.00019517218079272424\n","Epoch: 40     train index of 5 minibatch: 3      time used: 2.640615224838257\n","minibatch AVG loss: 0.0018821616381273997\n","\n","Epoch: 40  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.0301  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 2.849623680114746\n","minibatch AVG loss: 9.297065421378647e-05\n","Epoch: 41     train index of 5 minibatch: 2      time used: 2.6511266231536865\n","minibatch AVG loss: 7.845797128993581e-05\n","Epoch: 41     train index of 5 minibatch: 3      time used: 2.629751443862915\n","minibatch AVG loss: 0.012255101597565954\n","\n","Epoch: 41  train \n","Loss: 0.0036  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.0227  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 2.8395299911499023\n","minibatch AVG loss: 1.4107671421470514e-05\n","Epoch: 42     train index of 5 minibatch: 2      time used: 2.6577494144439697\n","minibatch AVG loss: 3.675120442494517e-05\n","Epoch: 42     train index of 5 minibatch: 3      time used: 2.643829345703125\n","minibatch AVG loss: 1.828569838835392e-05\n","\n","Epoch: 42  train \n","Loss: 0.0000  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.0173  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 2.841639995574951\n","minibatch AVG loss: 0.009686212267388328\n","Epoch: 43     train index of 5 minibatch: 2      time used: 2.6609201431274414\n","minibatch AVG loss: 0.006757776922859194\n","Epoch: 43     train index of 5 minibatch: 3      time used: 2.6263656616210938\n","minibatch AVG loss: 0.00033741584711606266\n","\n","Epoch: 43  train \n","Loss: 0.0049  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.0179  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 2.8401546478271484\n","minibatch AVG loss: 4.6804690464341546e-05\n","Epoch: 44     train index of 5 minibatch: 2      time used: 2.653637170791626\n","minibatch AVG loss: 4.7241377251339146e-05\n","Epoch: 44     train index of 5 minibatch: 3      time used: 2.633976936340332\n","minibatch AVG loss: 2.4763406895544903e-05\n","\n","Epoch: 44  train \n","Loss: 0.0000  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.0188  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 2.84161114692688\n","minibatch AVG loss: 0.0005511974822184129\n","Epoch: 45     train index of 5 minibatch: 2      time used: 2.6591432094573975\n","minibatch AVG loss: 3.5898071109841113e-05\n","Epoch: 45     train index of 5 minibatch: 3      time used: 2.63244366645813\n","minibatch AVG loss: 3.1462932565773374e-05\n","\n","Epoch: 45  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.0185  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 2.845322847366333\n","minibatch AVG loss: 3.36192628765275e-05\n","Epoch: 46     train index of 5 minibatch: 2      time used: 2.6471457481384277\n","minibatch AVG loss: 2.4965485090433502e-05\n","Epoch: 46     train index of 5 minibatch: 3      time used: 2.6501998901367188\n","minibatch AVG loss: 0.00985102443227106\n","\n","Epoch: 46  train \n","Loss: 0.0029  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.0220  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 2.848792314529419\n","minibatch AVG loss: 9.571622249495703e-05\n","Epoch: 47     train index of 5 minibatch: 2      time used: 2.653204917907715\n","minibatch AVG loss: 4.112648628051829e-06\n","Epoch: 47     train index of 5 minibatch: 3      time used: 2.629941701889038\n","minibatch AVG loss: 2.015724430748378e-05\n","\n","Epoch: 47  train \n","Loss: 0.0001  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.0254  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 2.854572057723999\n","minibatch AVG loss: 2.559226084031252e-05\n","Epoch: 48     train index of 5 minibatch: 2      time used: 2.650543212890625\n","minibatch AVG loss: 8.961303319665603e-05\n","Epoch: 48     train index of 5 minibatch: 3      time used: 2.634463310241699\n","minibatch AVG loss: 4.923338301523472e-05\n","\n","Epoch: 48  train \n","Loss: 0.0001  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.0258  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 2.833681106567383\n","minibatch AVG loss: 0.0002404544186902058\n","Epoch: 49     train index of 5 minibatch: 2      time used: 2.6456525325775146\n","minibatch AVG loss: 4.8545248864684255e-05\n","Epoch: 49     train index of 5 minibatch: 3      time used: 2.6384811401367188\n","minibatch AVG loss: 0.0013161232155198376\n","\n","Epoch: 49  train \n","Loss: 0.0005  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.0254  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 2.83720326423645\n","minibatch AVG loss: 1.2706524296390853e-05\n","Epoch: 50     train index of 5 minibatch: 2      time used: 2.6404495239257812\n","minibatch AVG loss: 0.0005585395646392044\n","Epoch: 50     train index of 5 minibatch: 3      time used: 2.6305930614471436\n","minibatch AVG loss: 0.0001351112405245658\n","\n","Epoch: 50  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.0242  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 8m 27s\n","Best epoch idx:  50\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS.pth\n"]}],"source":["!python Train.py --model_idx ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS --augmentation_name Cutout --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQ3MJZGaaIRc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651154443324,"user_tz":-480,"elapsed":516325,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"5776ebb2-030e-49bd-d735-4ad4d389ecce"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name='CutMix', backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 2.3224, -1.6767]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 24, 24]         590,592\n","          Identity-2             [-1, 576, 768]               0\n","        PatchEmbed-3             [-1, 576, 768]               0\n","           Dropout-4             [-1, 577, 768]               0\n","         LayerNorm-5             [-1, 577, 768]           1,536\n","            Linear-6            [-1, 577, 2304]       1,771,776\n","           Dropout-7         [-1, 12, 577, 577]               0\n","            Linear-8             [-1, 577, 768]         590,592\n","           Dropout-9             [-1, 577, 768]               0\n","        Attention-10             [-1, 577, 768]               0\n","         Identity-11             [-1, 577, 768]               0\n","        LayerNorm-12             [-1, 577, 768]           1,536\n","           Linear-13            [-1, 577, 3072]       2,362,368\n","             GELU-14            [-1, 577, 3072]               0\n","          Dropout-15            [-1, 577, 3072]               0\n","           Linear-16             [-1, 577, 768]       2,360,064\n","          Dropout-17             [-1, 577, 768]               0\n","              Mlp-18             [-1, 577, 768]               0\n","         Identity-19             [-1, 577, 768]               0\n","            Block-20             [-1, 577, 768]               0\n","        LayerNorm-21             [-1, 577, 768]           1,536\n","           Linear-22            [-1, 577, 2304]       1,771,776\n","          Dropout-23         [-1, 12, 577, 577]               0\n","           Linear-24             [-1, 577, 768]         590,592\n","          Dropout-25             [-1, 577, 768]               0\n","        Attention-26             [-1, 577, 768]               0\n","         Identity-27             [-1, 577, 768]               0\n","        LayerNorm-28             [-1, 577, 768]           1,536\n","           Linear-29            [-1, 577, 3072]       2,362,368\n","             GELU-30            [-1, 577, 3072]               0\n","          Dropout-31            [-1, 577, 3072]               0\n","           Linear-32             [-1, 577, 768]       2,360,064\n","          Dropout-33             [-1, 577, 768]               0\n","              Mlp-34             [-1, 577, 768]               0\n","         Identity-35             [-1, 577, 768]               0\n","            Block-36             [-1, 577, 768]               0\n","        LayerNorm-37             [-1, 577, 768]           1,536\n","           Linear-38            [-1, 577, 2304]       1,771,776\n","          Dropout-39         [-1, 12, 577, 577]               0\n","           Linear-40             [-1, 577, 768]         590,592\n","          Dropout-41             [-1, 577, 768]               0\n","        Attention-42             [-1, 577, 768]               0\n","         Identity-43             [-1, 577, 768]               0\n","        LayerNorm-44             [-1, 577, 768]           1,536\n","           Linear-45            [-1, 577, 3072]       2,362,368\n","             GELU-46            [-1, 577, 3072]               0\n","          Dropout-47            [-1, 577, 3072]               0\n","           Linear-48             [-1, 577, 768]       2,360,064\n","          Dropout-49             [-1, 577, 768]               0\n","              Mlp-50             [-1, 577, 768]               0\n","         Identity-51             [-1, 577, 768]               0\n","            Block-52             [-1, 577, 768]               0\n","        LayerNorm-53             [-1, 577, 768]           1,536\n","           Linear-54            [-1, 577, 2304]       1,771,776\n","          Dropout-55         [-1, 12, 577, 577]               0\n","           Linear-56             [-1, 577, 768]         590,592\n","          Dropout-57             [-1, 577, 768]               0\n","        Attention-58             [-1, 577, 768]               0\n","         Identity-59             [-1, 577, 768]               0\n","        LayerNorm-60             [-1, 577, 768]           1,536\n","           Linear-61            [-1, 577, 3072]       2,362,368\n","             GELU-62            [-1, 577, 3072]               0\n","          Dropout-63            [-1, 577, 3072]               0\n","           Linear-64             [-1, 577, 768]       2,360,064\n","          Dropout-65             [-1, 577, 768]               0\n","              Mlp-66             [-1, 577, 768]               0\n","         Identity-67             [-1, 577, 768]               0\n","            Block-68             [-1, 577, 768]               0\n","        LayerNorm-69             [-1, 577, 768]           1,536\n","           Linear-70            [-1, 577, 2304]       1,771,776\n","          Dropout-71         [-1, 12, 577, 577]               0\n","           Linear-72             [-1, 577, 768]         590,592\n","          Dropout-73             [-1, 577, 768]               0\n","        Attention-74             [-1, 577, 768]               0\n","         Identity-75             [-1, 577, 768]               0\n","        LayerNorm-76             [-1, 577, 768]           1,536\n","           Linear-77            [-1, 577, 3072]       2,362,368\n","             GELU-78            [-1, 577, 3072]               0\n","          Dropout-79            [-1, 577, 3072]               0\n","           Linear-80             [-1, 577, 768]       2,360,064\n","          Dropout-81             [-1, 577, 768]               0\n","              Mlp-82             [-1, 577, 768]               0\n","         Identity-83             [-1, 577, 768]               0\n","            Block-84             [-1, 577, 768]               0\n","        LayerNorm-85             [-1, 577, 768]           1,536\n","           Linear-86            [-1, 577, 2304]       1,771,776\n","          Dropout-87         [-1, 12, 577, 577]               0\n","           Linear-88             [-1, 577, 768]         590,592\n","          Dropout-89             [-1, 577, 768]               0\n","        Attention-90             [-1, 577, 768]               0\n","         Identity-91             [-1, 577, 768]               0\n","        LayerNorm-92             [-1, 577, 768]           1,536\n","           Linear-93            [-1, 577, 3072]       2,362,368\n","             GELU-94            [-1, 577, 3072]               0\n","          Dropout-95            [-1, 577, 3072]               0\n","           Linear-96             [-1, 577, 768]       2,360,064\n","          Dropout-97             [-1, 577, 768]               0\n","              Mlp-98             [-1, 577, 768]               0\n","         Identity-99             [-1, 577, 768]               0\n","           Block-100             [-1, 577, 768]               0\n","       LayerNorm-101             [-1, 577, 768]           1,536\n","          Linear-102            [-1, 577, 2304]       1,771,776\n","         Dropout-103         [-1, 12, 577, 577]               0\n","          Linear-104             [-1, 577, 768]         590,592\n","         Dropout-105             [-1, 577, 768]               0\n","       Attention-106             [-1, 577, 768]               0\n","        Identity-107             [-1, 577, 768]               0\n","       LayerNorm-108             [-1, 577, 768]           1,536\n","          Linear-109            [-1, 577, 3072]       2,362,368\n","            GELU-110            [-1, 577, 3072]               0\n","         Dropout-111            [-1, 577, 3072]               0\n","          Linear-112             [-1, 577, 768]       2,360,064\n","         Dropout-113             [-1, 577, 768]               0\n","             Mlp-114             [-1, 577, 768]               0\n","        Identity-115             [-1, 577, 768]               0\n","           Block-116             [-1, 577, 768]               0\n","       LayerNorm-117             [-1, 577, 768]           1,536\n","          Linear-118            [-1, 577, 2304]       1,771,776\n","         Dropout-119         [-1, 12, 577, 577]               0\n","          Linear-120             [-1, 577, 768]         590,592\n","         Dropout-121             [-1, 577, 768]               0\n","       Attention-122             [-1, 577, 768]               0\n","        Identity-123             [-1, 577, 768]               0\n","       LayerNorm-124             [-1, 577, 768]           1,536\n","          Linear-125            [-1, 577, 3072]       2,362,368\n","            GELU-126            [-1, 577, 3072]               0\n","         Dropout-127            [-1, 577, 3072]               0\n","          Linear-128             [-1, 577, 768]       2,360,064\n","         Dropout-129             [-1, 577, 768]               0\n","             Mlp-130             [-1, 577, 768]               0\n","        Identity-131             [-1, 577, 768]               0\n","           Block-132             [-1, 577, 768]               0\n","       LayerNorm-133             [-1, 577, 768]           1,536\n","          Linear-134            [-1, 577, 2304]       1,771,776\n","         Dropout-135         [-1, 12, 577, 577]               0\n","          Linear-136             [-1, 577, 768]         590,592\n","         Dropout-137             [-1, 577, 768]               0\n","       Attention-138             [-1, 577, 768]               0\n","        Identity-139             [-1, 577, 768]               0\n","       LayerNorm-140             [-1, 577, 768]           1,536\n","          Linear-141            [-1, 577, 3072]       2,362,368\n","            GELU-142            [-1, 577, 3072]               0\n","         Dropout-143            [-1, 577, 3072]               0\n","          Linear-144             [-1, 577, 768]       2,360,064\n","         Dropout-145             [-1, 577, 768]               0\n","             Mlp-146             [-1, 577, 768]               0\n","        Identity-147             [-1, 577, 768]               0\n","           Block-148             [-1, 577, 768]               0\n","       LayerNorm-149             [-1, 577, 768]           1,536\n","          Linear-150            [-1, 577, 2304]       1,771,776\n","         Dropout-151         [-1, 12, 577, 577]               0\n","          Linear-152             [-1, 577, 768]         590,592\n","         Dropout-153             [-1, 577, 768]               0\n","       Attention-154             [-1, 577, 768]               0\n","        Identity-155             [-1, 577, 768]               0\n","       LayerNorm-156             [-1, 577, 768]           1,536\n","          Linear-157            [-1, 577, 3072]       2,362,368\n","            GELU-158            [-1, 577, 3072]               0\n","         Dropout-159            [-1, 577, 3072]               0\n","          Linear-160             [-1, 577, 768]       2,360,064\n","         Dropout-161             [-1, 577, 768]               0\n","             Mlp-162             [-1, 577, 768]               0\n","        Identity-163             [-1, 577, 768]               0\n","           Block-164             [-1, 577, 768]               0\n","       LayerNorm-165             [-1, 577, 768]           1,536\n","          Linear-166            [-1, 577, 2304]       1,771,776\n","         Dropout-167         [-1, 12, 577, 577]               0\n","          Linear-168             [-1, 577, 768]         590,592\n","         Dropout-169             [-1, 577, 768]               0\n","       Attention-170             [-1, 577, 768]               0\n","        Identity-171             [-1, 577, 768]               0\n","       LayerNorm-172             [-1, 577, 768]           1,536\n","          Linear-173            [-1, 577, 3072]       2,362,368\n","            GELU-174            [-1, 577, 3072]               0\n","         Dropout-175            [-1, 577, 3072]               0\n","          Linear-176             [-1, 577, 768]       2,360,064\n","         Dropout-177             [-1, 577, 768]               0\n","             Mlp-178             [-1, 577, 768]               0\n","        Identity-179             [-1, 577, 768]               0\n","           Block-180             [-1, 577, 768]               0\n","       LayerNorm-181             [-1, 577, 768]           1,536\n","          Linear-182            [-1, 577, 2304]       1,771,776\n","         Dropout-183         [-1, 12, 577, 577]               0\n","          Linear-184             [-1, 577, 768]         590,592\n","         Dropout-185             [-1, 577, 768]               0\n","       Attention-186             [-1, 577, 768]               0\n","        Identity-187             [-1, 577, 768]               0\n","       LayerNorm-188             [-1, 577, 768]           1,536\n","          Linear-189            [-1, 577, 3072]       2,362,368\n","            GELU-190            [-1, 577, 3072]               0\n","         Dropout-191            [-1, 577, 3072]               0\n","          Linear-192             [-1, 577, 768]       2,360,064\n","         Dropout-193             [-1, 577, 768]               0\n","             Mlp-194             [-1, 577, 768]               0\n","        Identity-195             [-1, 577, 768]               0\n","           Block-196             [-1, 577, 768]               0\n","       LayerNorm-197             [-1, 577, 768]           1,536\n","        Identity-198                  [-1, 768]               0\n","          Linear-199                    [-1, 2]           1,538\n","================================================================\n","Total params: 85,648,130\n","Trainable params: 85,648,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1437.49\n","Params size (MB): 326.72\n","Estimated Total Size (MB): 1765.90\n","----------------------------------------------------------------\n","model : ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 2.786527633666992\n","minibatch AVG loss: 1.7453954249620438\n","Epoch: 1     train index of 5 minibatch: 2      time used: 2.5736629962921143\n","minibatch AVG loss: 2.1521351754665377\n","Epoch: 1     train index of 5 minibatch: 3      time used: 2.5977823734283447\n","minibatch AVG loss: 1.5327860116958618\n","\n","Epoch: 1  train \n","Loss: 1.7825  Acc: 78.2609\n","benign precision: 71.4286  recall: 86.2069\n","benign sensitivity: 86.2069  specificity: 74.3590\n","benign FPR: 25.6410  NPV: 87.8788\n","benign TP: 25.0\n","benign TN: 29.0\n","benign FP: 10.0\n","benign FN: 4.0\n","malignant precision: 87.8788  recall: 74.3590\n","malignant sensitivity: 74.3590  specificity: 86.2069\n","malignant FPR: 13.7931  NPV: 71.4286\n","malignant TP: 29.0\n","malignant TN: 25.0\n","malignant FP: 4.0\n","malignant FN: 10.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 1.1884  Acc: 81.2500\n","benign precision: 75.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 87.5000\n","benign TP: 6.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 87.5000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 75.0000\n","malignant TP: 7.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 2.8296823501586914\n","minibatch AVG loss: 4.095940208435058\n","Epoch: 2     train index of 5 minibatch: 2      time used: 2.642282247543335\n","minibatch AVG loss: 1.0278393626213074\n","Epoch: 2     train index of 5 minibatch: 3      time used: 2.6696012020111084\n","minibatch AVG loss: 0.8538726031780243\n","\n","Epoch: 2  train \n","Loss: 1.8817  Acc: 88.4058\n","benign precision: 89.6552  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 89.7436\n","benign TP: 26.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 4.0\n","malignant precision: 89.7436  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 89.6552\n","malignant TP: 35.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.9783  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 3.0129244327545166\n","minibatch AVG loss: 0.9514702320098877\n","Epoch: 3     train index of 5 minibatch: 2      time used: 2.7378056049346924\n","minibatch AVG loss: 1.5467277884483337\n","Epoch: 3     train index of 5 minibatch: 3      time used: 2.7268192768096924\n","minibatch AVG loss: 1.3701984763145447\n","\n","Epoch: 3  train \n","Loss: 1.2989  Acc: 91.3043\n","benign precision: 100.0000  recall: 82.7586\n","benign sensitivity: 82.7586  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 88.6364\n","benign TP: 24.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 5.0\n","malignant precision: 88.6364  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 82.7586\n","malignant FPR: 17.2414  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 24.0\n","malignant FP: 5.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.4794  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 2.993705987930298\n","minibatch AVG loss: 0.9402817092835903\n","Epoch: 4     train index of 5 minibatch: 2      time used: 2.7069337368011475\n","minibatch AVG loss: 0.9640361800789833\n","Epoch: 4     train index of 5 minibatch: 3      time used: 2.684746503829956\n","minibatch AVG loss: 0.9972982719540596\n","\n","Epoch: 4  train \n","Loss: 0.8439  Acc: 94.2029\n","benign precision: 90.3226  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 92.5000\n","benign FPR: 7.5000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 37.0\n","benign FP: 3.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 92.5000\n","malignant sensitivity: 92.5000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 90.3226\n","malignant TP: 37.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.6130  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 2.967355728149414\n","minibatch AVG loss: 0.5100277911871671\n","Epoch: 5     train index of 5 minibatch: 2      time used: 2.6805548667907715\n","minibatch AVG loss: 1.4765325903892517\n","Epoch: 5     train index of 5 minibatch: 3      time used: 2.645472526550293\n","minibatch AVG loss: 0.8579239189624787\n","\n","Epoch: 5  train \n","Loss: 1.0366  Acc: 92.7536\n","benign precision: 96.2963  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 92.6829\n","benign TP: 26.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 92.6829  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 96.2963\n","malignant TP: 38.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.8470  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 2.9190497398376465\n","minibatch AVG loss: 0.44504288733005526\n","Epoch: 6     train index of 5 minibatch: 2      time used: 2.63519287109375\n","minibatch AVG loss: 0.30425052642822265\n","Epoch: 6     train index of 5 minibatch: 3      time used: 2.6274452209472656\n","minibatch AVG loss: 0.8534185662865639\n","\n","Epoch: 6  train \n","Loss: 0.4819  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.4359\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.3931  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 2.819225788116455\n","minibatch AVG loss: 0.8813179660588503\n","Epoch: 7     train index of 5 minibatch: 2      time used: 2.6134843826293945\n","minibatch AVG loss: 0.8271352171897888\n","Epoch: 7     train index of 5 minibatch: 3      time used: 2.6108126640319824\n","minibatch AVG loss: 0.12415208183228969\n","\n","Epoch: 7  train \n","Loss: 0.5342  Acc: 95.6522\n","benign precision: 93.5484  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.5484\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.4344  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 2.80747127532959\n","minibatch AVG loss: 0.9624748960137367\n","Epoch: 8     train index of 5 minibatch: 2      time used: 2.605283260345459\n","minibatch AVG loss: 1.400317046046257\n","Epoch: 8     train index of 5 minibatch: 3      time used: 2.600177049636841\n","minibatch AVG loss: 1.3321243047714233\n","\n","Epoch: 8  train \n","Loss: 1.0812  Acc: 92.7536\n","benign precision: 96.6667  recall: 90.6250\n","benign sensitivity: 90.6250  specificity: 97.2222\n","benign FPR: 2.7778  NPV: 92.1053\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 92.1053  recall: 97.2222\n","malignant sensitivity: 97.2222  specificity: 90.6250\n","malignant FPR: 9.3750  NPV: 96.6667\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.4593  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 2.8082263469696045\n","minibatch AVG loss: 0.49431216940283773\n","Epoch: 9     train index of 5 minibatch: 2      time used: 2.6127171516418457\n","minibatch AVG loss: 1.1612377136945724\n","Epoch: 9     train index of 5 minibatch: 3      time used: 2.61456298828125\n","minibatch AVG loss: 1.4181351244449616\n","\n","Epoch: 9  train \n","Loss: 1.0471  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.1220\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.1220  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.6731  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 2.830651044845581\n","minibatch AVG loss: 1.4409899950027465\n","Epoch: 10     train index of 5 minibatch: 2      time used: 2.6365010738372803\n","minibatch AVG loss: 1.217290323972702\n","Epoch: 10     train index of 5 minibatch: 3      time used: 2.6315386295318604\n","minibatch AVG loss: 0.7768396429717541\n","\n","Epoch: 10  train \n","Loss: 1.1510  Acc: 89.8551\n","benign precision: 89.6552  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 92.3077\n","benign TP: 26.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 92.3077  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 89.6552\n","malignant TP: 36.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.3801  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 2.8290653228759766\n","minibatch AVG loss: 0.3375423848628998\n","Epoch: 11     train index of 5 minibatch: 2      time used: 2.641649007797241\n","minibatch AVG loss: 0.7043644599616528\n","Epoch: 11     train index of 5 minibatch: 3      time used: 2.63985276222229\n","minibatch AVG loss: 0.7392746150493622\n","\n","Epoch: 11  train \n","Loss: 0.5282  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 40.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 40.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.2174  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 2.852231025695801\n","minibatch AVG loss: 0.8265598833560943\n","Epoch: 12     train index of 5 minibatch: 2      time used: 2.6594655513763428\n","minibatch AVG loss: 1.1063512951135634\n","Epoch: 12     train index of 5 minibatch: 3      time used: 2.6503875255584717\n","minibatch AVG loss: 0.8095831751823426\n","\n","Epoch: 12  train \n","Loss: 0.8790  Acc: 94.2029\n","benign precision: 93.7500  recall: 96.7742\n","benign sensitivity: 96.7742  specificity: 94.5946\n","benign FPR: 5.4054  NPV: 97.2222\n","benign TP: 30.0\n","benign TN: 35.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 94.5946\n","malignant sensitivity: 94.5946  specificity: 96.7742\n","malignant FPR: 3.2258  NPV: 93.7500\n","malignant TP: 35.0\n","malignant TN: 30.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.5416  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 2.847142457962036\n","minibatch AVG loss: 0.6506299450993538\n","Epoch: 13     train index of 5 minibatch: 2      time used: 2.659454822540283\n","minibatch AVG loss: 0.7364462152123451\n","Epoch: 13     train index of 5 minibatch: 3      time used: 2.6444716453552246\n","minibatch AVG loss: 1.7064237669110298\n","\n","Epoch: 13  train \n","Loss: 1.0048  Acc: 92.7536\n","benign precision: 90.0000  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 92.5000\n","benign FPR: 7.5000  NPV: 97.3684\n","benign TP: 27.0\n","benign TN: 37.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 92.5000\n","malignant sensitivity: 92.5000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 90.0000\n","malignant TP: 37.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.7409  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 2.8354618549346924\n","minibatch AVG loss: 1.897856855392456\n","Epoch: 14     train index of 5 minibatch: 2      time used: 2.6520872116088867\n","minibatch AVG loss: 1.069298803806305\n","Epoch: 14     train index of 5 minibatch: 3      time used: 2.6470227241516113\n","minibatch AVG loss: 0.8115991532802582\n","\n","Epoch: 14  train \n","Loss: 1.2101  Acc: 86.9565\n","benign precision: 87.0968  recall: 87.0968\n","benign sensitivity: 87.0968  specificity: 89.1892\n","benign FPR: 10.8108  NPV: 89.1892\n","benign TP: 27.0\n","benign TN: 33.0\n","benign FP: 4.0\n","benign FN: 4.0\n","malignant precision: 89.1892  recall: 89.1892\n","malignant sensitivity: 89.1892  specificity: 87.0968\n","malignant FPR: 12.9032  NPV: 87.0968\n","malignant TP: 33.0\n","malignant TN: 27.0\n","malignant FP: 4.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.3930  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 2.841907262802124\n","minibatch AVG loss: 0.5401589468121528\n","Epoch: 15     train index of 5 minibatch: 2      time used: 2.6538875102996826\n","minibatch AVG loss: 0.877219170331955\n","Epoch: 15     train index of 5 minibatch: 3      time used: 2.6342108249664307\n","minibatch AVG loss: 1.1338867723941803\n","\n","Epoch: 15  train \n","Loss: 0.8692  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.3050  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 2.8356239795684814\n","minibatch AVG loss: 0.8750583723187446\n","Epoch: 16     train index of 5 minibatch: 2      time used: 2.6474766731262207\n","minibatch AVG loss: 0.48746469616889954\n","Epoch: 16     train index of 5 minibatch: 3      time used: 2.6253135204315186\n","minibatch AVG loss: 0.5727642863988877\n","\n","Epoch: 16  train \n","Loss: 0.5747  Acc: 94.2029\n","benign precision: 93.3333  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 97.3684\n","benign TP: 28.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 93.3333\n","malignant TP: 37.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.3107  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 2.8531246185302734\n","minibatch AVG loss: 0.44164997190237043\n","Epoch: 17     train index of 5 minibatch: 2      time used: 2.6369879245758057\n","minibatch AVG loss: 0.05521953497081995\n","Epoch: 17     train index of 5 minibatch: 3      time used: 2.627060890197754\n","minibatch AVG loss: 0.5221732884645462\n","\n","Epoch: 17  train \n","Loss: 0.3040  Acc: 95.6522\n","benign precision: 96.8750  recall: 96.8750\n","benign sensitivity: 96.8750  specificity: 97.2222\n","benign FPR: 2.7778  NPV: 97.2222\n","benign TP: 31.0\n","benign TN: 35.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 97.2222\n","malignant sensitivity: 97.2222  specificity: 96.8750\n","malignant FPR: 3.1250  NPV: 96.8750\n","malignant TP: 35.0\n","malignant TN: 31.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.2552  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 2.8316938877105713\n","minibatch AVG loss: 0.7839914359152317\n","Epoch: 18     train index of 5 minibatch: 2      time used: 2.6342673301696777\n","minibatch AVG loss: 0.970545181259513\n","Epoch: 18     train index of 5 minibatch: 3      time used: 2.630567789077759\n","minibatch AVG loss: 1.0803541839122772\n","\n","Epoch: 18  train \n","Loss: 0.9330  Acc: 89.8551\n","benign precision: 86.6667  recall: 92.8571\n","benign sensitivity: 92.8571  specificity: 90.0000\n","benign FPR: 10.0000  NPV: 94.7368\n","benign TP: 26.0\n","benign TN: 36.0\n","benign FP: 4.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 90.0000\n","malignant sensitivity: 90.0000  specificity: 92.8571\n","malignant FPR: 7.1429  NPV: 86.6667\n","malignant TP: 36.0\n","malignant TN: 26.0\n","malignant FP: 2.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.2682  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 2.838965892791748\n","minibatch AVG loss: 1.9334986209869385\n","Epoch: 19     train index of 5 minibatch: 2      time used: 2.6435532569885254\n","minibatch AVG loss: 1.2016119264066218\n","Epoch: 19     train index of 5 minibatch: 3      time used: 2.624875545501709\n","minibatch AVG loss: 0.11109192594885826\n","\n","Epoch: 19  train \n","Loss: 0.9604  Acc: 95.6522\n","benign precision: 93.5484  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.5484\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.4208  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 2.8338112831115723\n","minibatch AVG loss: 0.746909087896347\n","Epoch: 20     train index of 5 minibatch: 2      time used: 2.638306140899658\n","minibatch AVG loss: 0.7319609530270099\n","Epoch: 20     train index of 5 minibatch: 3      time used: 2.628084182739258\n","minibatch AVG loss: 0.7822407633066177\n","\n","Epoch: 20  train \n","Loss: 0.8118  Acc: 95.6522\n","benign precision: 93.1034  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 95.1220\n","benign FPR: 4.8780  NPV: 100.0000\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 95.1220\n","malignant sensitivity: 95.1220  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.1034\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.5424  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 2.824397563934326\n","minibatch AVG loss: 0.6912105642259121\n","Epoch: 21     train index of 5 minibatch: 2      time used: 2.648104429244995\n","minibatch AVG loss: 1.4735313177108764\n","Epoch: 21     train index of 5 minibatch: 3      time used: 2.628995895385742\n","minibatch AVG loss: 1.0934779226779938\n","\n","Epoch: 21  train \n","Loss: 1.0158  Acc: 94.2029\n","benign precision: 96.5517  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 94.8718\n","benign TP: 28.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 96.5517\n","malignant TP: 37.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.3367  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 2.8331053256988525\n","minibatch AVG loss: 0.418181848526001\n","Epoch: 22     train index of 5 minibatch: 2      time used: 2.647914409637451\n","minibatch AVG loss: 0.5097818233072757\n","Epoch: 22     train index of 5 minibatch: 3      time used: 2.637631416320801\n","minibatch AVG loss: 0.3674115713685751\n","\n","Epoch: 22  train \n","Loss: 0.4445  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.3652  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 2.8417413234710693\n","minibatch AVG loss: 1.5907000303268433\n","Epoch: 23     train index of 5 minibatch: 2      time used: 2.652397394180298\n","minibatch AVG loss: 0.9716028716415167\n","Epoch: 23     train index of 5 minibatch: 3      time used: 2.623774766921997\n","minibatch AVG loss: 0.7717491067945957\n","\n","Epoch: 23  train \n","Loss: 0.9971  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.7500\n","benign sensitivity: 93.7500  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 94.7368\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.7500\n","malignant FPR: 6.2500  NPV: 100.0000\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.3618  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 2.842472553253174\n","minibatch AVG loss: 0.8055540569126606\n","Epoch: 24     train index of 5 minibatch: 2      time used: 2.650381088256836\n","minibatch AVG loss: 0.8299033239483833\n","Epoch: 24     train index of 5 minibatch: 3      time used: 2.6385364532470703\n","minibatch AVG loss: 1.2266734659671783\n","\n","Epoch: 24  train \n","Loss: 0.9208  Acc: 94.2029\n","benign precision: 93.7500  recall: 96.7742\n","benign sensitivity: 96.7742  specificity: 94.5946\n","benign FPR: 5.4054  NPV: 97.2222\n","benign TP: 30.0\n","benign TN: 35.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 94.5946\n","malignant sensitivity: 94.5946  specificity: 96.7742\n","malignant FPR: 3.2258  NPV: 93.7500\n","malignant TP: 35.0\n","malignant TN: 30.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.3397  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 2.8501076698303223\n","minibatch AVG loss: 0.5065072000026702\n","Epoch: 25     train index of 5 minibatch: 2      time used: 2.6565182209014893\n","minibatch AVG loss: 0.7186283659189939\n","Epoch: 25     train index of 5 minibatch: 3      time used: 2.6377665996551514\n","minibatch AVG loss: 1.5318511486053468\n","\n","Epoch: 25  train \n","Loss: 0.8715  Acc: 88.4058\n","benign precision: 90.0000  recall: 87.0968\n","benign sensitivity: 87.0968  specificity: 91.8919\n","benign FPR: 8.1081  NPV: 89.4737\n","benign TP: 27.0\n","benign TN: 34.0\n","benign FP: 3.0\n","benign FN: 4.0\n","malignant precision: 89.4737  recall: 91.8919\n","malignant sensitivity: 91.8919  specificity: 87.0968\n","malignant FPR: 12.9032  NPV: 90.0000\n","malignant TP: 34.0\n","malignant TN: 27.0\n","malignant FP: 4.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.2652  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 2.841972589492798\n","minibatch AVG loss: 0.8618675410747528\n","Epoch: 26     train index of 5 minibatch: 2      time used: 2.651124954223633\n","minibatch AVG loss: 1.1343483448028564\n","Epoch: 26     train index of 5 minibatch: 3      time used: 2.633859634399414\n","minibatch AVG loss: 1.0109972212463618\n","\n","Epoch: 26  train \n","Loss: 1.0106  Acc: 95.6522\n","benign precision: 100.0000  recall: 92.5926\n","benign sensitivity: 92.5926  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.3488\n","benign TP: 25.0\n","benign TN: 41.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.3488  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 92.5926\n","malignant FPR: 7.4074  NPV: 100.0000\n","malignant TP: 41.0\n","malignant TN: 25.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.3372  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 2.849172592163086\n","minibatch AVG loss: 0.8201802052557469\n","Epoch: 27     train index of 5 minibatch: 2      time used: 2.641798496246338\n","minibatch AVG loss: 0.4255063712596893\n","Epoch: 27     train index of 5 minibatch: 3      time used: 2.640529155731201\n","minibatch AVG loss: 0.21065097525715828\n","\n","Epoch: 27  train \n","Loss: 0.5144  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.2333  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 2.8552722930908203\n","minibatch AVG loss: 0.6820084556937218\n","Epoch: 28     train index of 5 minibatch: 2      time used: 2.657386302947998\n","minibatch AVG loss: 0.7400011494755745\n","Epoch: 28     train index of 5 minibatch: 3      time used: 2.63942289352417\n","minibatch AVG loss: 0.765420651435852\n","\n","Epoch: 28  train \n","Loss: 0.7565  Acc: 95.6522\n","benign precision: 96.9697  recall: 96.9697\n","benign sensitivity: 96.9697  specificity: 97.1429\n","benign FPR: 2.8571  NPV: 97.1429\n","benign TP: 32.0\n","benign TN: 34.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.1429  recall: 97.1429\n","malignant sensitivity: 97.1429  specificity: 96.9697\n","malignant FPR: 3.0303  NPV: 96.9697\n","malignant TP: 34.0\n","malignant TN: 32.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.2739  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 2.831123113632202\n","minibatch AVG loss: 1.421592029184103\n","Epoch: 29     train index of 5 minibatch: 2      time used: 2.655103921890259\n","minibatch AVG loss: 0.4035926178097725\n","Epoch: 29     train index of 5 minibatch: 3      time used: 2.630122423171997\n","minibatch AVG loss: 0.7344273284077645\n","\n","Epoch: 29  train \n","Loss: 0.8407  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.9697\n","benign sensitivity: 96.9697  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.2222\n","benign TP: 32.0\n","benign TN: 35.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.9697\n","malignant FPR: 3.0303  NPV: 100.0000\n","malignant TP: 35.0\n","malignant TN: 32.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.2649  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 2.849461078643799\n","minibatch AVG loss: 0.8668506171554327\n","Epoch: 30     train index of 5 minibatch: 2      time used: 2.646754264831543\n","minibatch AVG loss: 0.8060788057744503\n","Epoch: 30     train index of 5 minibatch: 3      time used: 2.643479824066162\n","minibatch AVG loss: 0.5472445528954267\n","\n","Epoch: 30  train \n","Loss: 0.7288  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.2609  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 2.8501806259155273\n","minibatch AVG loss: 1.1860837146639824\n","Epoch: 31     train index of 5 minibatch: 2      time used: 2.639611005783081\n","minibatch AVG loss: 0.5429133512079716\n","Epoch: 31     train index of 5 minibatch: 3      time used: 2.6356759071350098\n","minibatch AVG loss: 0.7747813642024994\n","\n","Epoch: 31  train \n","Loss: 0.8789  Acc: 94.2029\n","benign precision: 96.5517  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 94.8718\n","benign TP: 28.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 96.5517\n","malignant TP: 37.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.2302  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 2.838387966156006\n","minibatch AVG loss: 0.3856521464884281\n","Epoch: 32     train index of 5 minibatch: 2      time used: 2.651172399520874\n","minibatch AVG loss: 0.9114729348570109\n","Epoch: 32     train index of 5 minibatch: 3      time used: 2.62858247756958\n","minibatch AVG loss: 0.5452005371451378\n","\n","Epoch: 32  train \n","Loss: 0.6770  Acc: 97.1014\n","benign precision: 96.8750  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 100.0000\n","benign TP: 31.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.8750\n","malignant TP: 36.0\n","malignant TN: 31.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.2529  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 2.834707021713257\n","minibatch AVG loss: 1.2630023956298828\n","Epoch: 33     train index of 5 minibatch: 2      time used: 2.6391866207122803\n","minibatch AVG loss: 0.7535240858793258\n","Epoch: 33     train index of 5 minibatch: 3      time used: 2.633476495742798\n","minibatch AVG loss: 0.5985840655863285\n","\n","Epoch: 33  train \n","Loss: 0.7647  Acc: 92.7536\n","benign precision: 89.2857  recall: 96.1538\n","benign sensitivity: 96.1538  specificity: 92.8571\n","benign FPR: 7.1429  NPV: 97.5000\n","benign TP: 25.0\n","benign TN: 39.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 92.8571\n","malignant sensitivity: 92.8571  specificity: 96.1538\n","malignant FPR: 3.8462  NPV: 89.2857\n","malignant TP: 39.0\n","malignant TN: 25.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.2161  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 2.840609550476074\n","minibatch AVG loss: 0.42523277588188646\n","Epoch: 34     train index of 5 minibatch: 2      time used: 2.638227701187134\n","minibatch AVG loss: 0.8660812731832266\n","Epoch: 34     train index of 5 minibatch: 3      time used: 2.6319355964660645\n","minibatch AVG loss: 1.1653123140335082\n","\n","Epoch: 34  train \n","Loss: 0.7149  Acc: 95.6522\n","benign precision: 93.7500  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.7500\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.2998  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 2.836768627166748\n","minibatch AVG loss: 0.5430000505410135\n","Epoch: 35     train index of 5 minibatch: 2      time used: 2.643747568130493\n","minibatch AVG loss: 1.721878632903099\n","Epoch: 35     train index of 5 minibatch: 3      time used: 2.629849672317505\n","minibatch AVG loss: 0.32161845490336416\n","\n","Epoch: 35  train \n","Loss: 0.8844  Acc: 91.3043\n","benign precision: 93.1034  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 92.3077\n","benign TP: 27.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.3077  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 93.1034\n","malignant TP: 36.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.2064  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 2.8396551609039307\n","minibatch AVG loss: 0.20969136580824851\n","Epoch: 36     train index of 5 minibatch: 2      time used: 2.6374170780181885\n","minibatch AVG loss: 0.797811308503151\n","Epoch: 36     train index of 5 minibatch: 3      time used: 2.6407673358917236\n","minibatch AVG loss: 1.3072453618049622\n","\n","Epoch: 36  train \n","Loss: 0.8169  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.2266  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 2.83709716796875\n","minibatch AVG loss: 0.4978459673933685\n","Epoch: 37     train index of 5 minibatch: 2      time used: 2.6402785778045654\n","minibatch AVG loss: 0.4313313703984022\n","Epoch: 37     train index of 5 minibatch: 3      time used: 2.6346211433410645\n","minibatch AVG loss: 0.29426632411777975\n","\n","Epoch: 37  train \n","Loss: 0.4561  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.1905  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 2.852860450744629\n","minibatch AVG loss: 1.1209819242358208\n","Epoch: 38     train index of 5 minibatch: 2      time used: 2.64420485496521\n","minibatch AVG loss: 0.5687254994641989\n","Epoch: 38     train index of 5 minibatch: 3      time used: 2.6495401859283447\n","minibatch AVG loss: 0.602101806551218\n","\n","Epoch: 38  train \n","Loss: 0.7562  Acc: 91.3043\n","benign precision: 90.9091  recall: 93.7500\n","benign sensitivity: 93.7500  specificity: 91.6667\n","benign FPR: 8.3333  NPV: 94.2857\n","benign TP: 30.0\n","benign TN: 33.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.2857  recall: 91.6667\n","malignant sensitivity: 91.6667  specificity: 93.7500\n","malignant FPR: 6.2500  NPV: 90.9091\n","malignant TP: 33.0\n","malignant TN: 30.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.2210  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 2.845571517944336\n","minibatch AVG loss: 0.818595957197249\n","Epoch: 39     train index of 5 minibatch: 2      time used: 2.657031774520874\n","minibatch AVG loss: 0.8215347945690155\n","Epoch: 39     train index of 5 minibatch: 3      time used: 2.643378734588623\n","minibatch AVG loss: 0.5647668354213238\n","\n","Epoch: 39  train \n","Loss: 0.6421  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.1854  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 2.85258412361145\n","minibatch AVG loss: 1.2793620228767395\n","Epoch: 40     train index of 5 minibatch: 2      time used: 2.6596574783325195\n","minibatch AVG loss: 0.9870322832837701\n","Epoch: 40     train index of 5 minibatch: 3      time used: 2.62843656539917\n","minibatch AVG loss: 0.27064836490899324\n","\n","Epoch: 40  train \n","Loss: 0.8453  Acc: 92.7536\n","benign precision: 93.7500  recall: 93.7500\n","benign sensitivity: 93.7500  specificity: 94.4444\n","benign FPR: 5.5556  NPV: 94.4444\n","benign TP: 30.0\n","benign TN: 34.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.4444  recall: 94.4444\n","malignant sensitivity: 94.4444  specificity: 93.7500\n","malignant FPR: 6.2500  NPV: 93.7500\n","malignant TP: 34.0\n","malignant TN: 30.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.2151  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 2.8585364818573\n","minibatch AVG loss: 0.1730695702135563\n","Epoch: 41     train index of 5 minibatch: 2      time used: 2.658344030380249\n","minibatch AVG loss: 0.12743809521198274\n","Epoch: 41     train index of 5 minibatch: 3      time used: 2.626430034637451\n","minibatch AVG loss: 0.33106783404946327\n","\n","Epoch: 41  train \n","Loss: 0.2684  Acc: 97.1014\n","benign precision: 96.4286  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5610\n","benign FPR: 2.4390  NPV: 100.0000\n","benign TP: 27.0\n","benign TN: 40.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5610\n","malignant sensitivity: 97.5610  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.4286\n","malignant TP: 40.0\n","malignant TN: 27.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.1901  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 2.8484785556793213\n","minibatch AVG loss: 0.9273790717124939\n","Epoch: 42     train index of 5 minibatch: 2      time used: 2.662628173828125\n","minibatch AVG loss: 0.7113796878606081\n","Epoch: 42     train index of 5 minibatch: 3      time used: 2.6310901641845703\n","minibatch AVG loss: 0.7590176239609718\n","\n","Epoch: 42  train \n","Loss: 0.7767  Acc: 92.7536\n","benign precision: 89.2857  recall: 96.1538\n","benign sensitivity: 96.1538  specificity: 92.8571\n","benign FPR: 7.1429  NPV: 97.5000\n","benign TP: 25.0\n","benign TN: 39.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 92.8571\n","malignant sensitivity: 92.8571  specificity: 96.1538\n","malignant FPR: 3.8462  NPV: 89.2857\n","malignant TP: 39.0\n","malignant TN: 25.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.1828  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 2.8478689193725586\n","minibatch AVG loss: 0.9524086877703667\n","Epoch: 43     train index of 5 minibatch: 2      time used: 2.6562304496765137\n","minibatch AVG loss: 0.6256786368787288\n","Epoch: 43     train index of 5 minibatch: 3      time used: 2.644953489303589\n","minibatch AVG loss: 0.4818749278783798\n","\n","Epoch: 43  train \n","Loss: 0.6441  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.1840  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 2.8448266983032227\n","minibatch AVG loss: 0.48357103765010834\n","Epoch: 44     train index of 5 minibatch: 2      time used: 2.656816005706787\n","minibatch AVG loss: 0.82827757447958\n","Epoch: 44     train index of 5 minibatch: 3      time used: 2.644470691680908\n","minibatch AVG loss: 0.43812623657286165\n","\n","Epoch: 44  train \n","Loss: 0.5969  Acc: 94.2029\n","benign precision: 96.6667  recall: 93.5484\n","benign sensitivity: 93.5484  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 94.7368\n","benign TP: 29.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 93.5484\n","malignant FPR: 6.4516  NPV: 96.6667\n","malignant TP: 36.0\n","malignant TN: 29.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.1726  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 2.839869737625122\n","minibatch AVG loss: 0.35573789924383165\n","Epoch: 45     train index of 5 minibatch: 2      time used: 2.6597836017608643\n","minibatch AVG loss: 0.8556013273075223\n","Epoch: 45     train index of 5 minibatch: 3      time used: 2.645123243331909\n","minibatch AVG loss: 0.24907121928408743\n","\n","Epoch: 45  train \n","Loss: 0.4629  Acc: 97.1014\n","benign precision: 96.8750  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 100.0000\n","benign TP: 31.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.8750\n","malignant TP: 36.0\n","malignant TN: 31.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.1631  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 2.8447561264038086\n","minibatch AVG loss: 0.173198414593935\n","Epoch: 46     train index of 5 minibatch: 2      time used: 2.639422655105591\n","minibatch AVG loss: 0.7252963380888104\n","Epoch: 46     train index of 5 minibatch: 3      time used: 2.6374077796936035\n","minibatch AVG loss: 0.7814623888581991\n","\n","Epoch: 46  train \n","Loss: 0.5632  Acc: 97.1014\n","benign precision: 96.4286  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5610\n","benign FPR: 2.4390  NPV: 100.0000\n","benign TP: 27.0\n","benign TN: 40.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5610\n","malignant sensitivity: 97.5610  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.4286\n","malignant TP: 40.0\n","malignant TN: 27.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.1598  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 2.836484670639038\n","minibatch AVG loss: 0.7405325146391988\n","Epoch: 47     train index of 5 minibatch: 2      time used: 2.652472496032715\n","minibatch AVG loss: 0.9041279688477516\n","Epoch: 47     train index of 5 minibatch: 3      time used: 2.633246421813965\n","minibatch AVG loss: 0.9564203538000584\n","\n","Epoch: 47  train \n","Loss: 0.8294  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.1564  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 2.8516883850097656\n","minibatch AVG loss: 0.7006081253290176\n","Epoch: 48     train index of 5 minibatch: 2      time used: 2.6474461555480957\n","minibatch AVG loss: 0.5341654345393181\n","Epoch: 48     train index of 5 minibatch: 3      time used: 2.6319375038146973\n","minibatch AVG loss: 0.6297048205509782\n","\n","Epoch: 48  train \n","Loss: 0.6281  Acc: 91.3043\n","benign precision: 87.5000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 97.2222\n","benign TP: 28.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 87.5000\n","malignant TP: 35.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.1584  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 2.84108567237854\n","minibatch AVG loss: 0.5510712050832808\n","Epoch: 49     train index of 5 minibatch: 2      time used: 2.653113603591919\n","minibatch AVG loss: 0.2622477371245623\n","Epoch: 49     train index of 5 minibatch: 3      time used: 2.628598928451538\n","minibatch AVG loss: 0.6755859278142452\n","\n","Epoch: 49  train \n","Loss: 0.4895  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.1592  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 2.8408145904541016\n","minibatch AVG loss: 0.9101001620292664\n","Epoch: 50     train index of 5 minibatch: 2      time used: 2.6397194862365723\n","minibatch AVG loss: 0.32675123400986195\n","Epoch: 50     train index of 5 minibatch: 3      time used: 2.6311144828796387\n","minibatch AVG loss: 0.5138390701264143\n","\n","Epoch: 50  train \n","Loss: 0.5753  Acc: 95.6522\n","benign precision: 96.5517  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 97.4359\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 96.5517\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.1539  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 8m 26s\n","Best epoch idx:  49\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS.pth\n"]}],"source":["!python Train.py --model_idx ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS --augmentation_name CutMix --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YKeUdYjaOx3","executionInfo":{"status":"ok","timestamp":1651154958797,"user_tz":-480,"elapsed":515480,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"838ce230-05ea-492f-c8de-90e1e78ba447"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name='Mixup', backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 2.3224, -1.6767]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 24, 24]         590,592\n","          Identity-2             [-1, 576, 768]               0\n","        PatchEmbed-3             [-1, 576, 768]               0\n","           Dropout-4             [-1, 577, 768]               0\n","         LayerNorm-5             [-1, 577, 768]           1,536\n","            Linear-6            [-1, 577, 2304]       1,771,776\n","           Dropout-7         [-1, 12, 577, 577]               0\n","            Linear-8             [-1, 577, 768]         590,592\n","           Dropout-9             [-1, 577, 768]               0\n","        Attention-10             [-1, 577, 768]               0\n","         Identity-11             [-1, 577, 768]               0\n","        LayerNorm-12             [-1, 577, 768]           1,536\n","           Linear-13            [-1, 577, 3072]       2,362,368\n","             GELU-14            [-1, 577, 3072]               0\n","          Dropout-15            [-1, 577, 3072]               0\n","           Linear-16             [-1, 577, 768]       2,360,064\n","          Dropout-17             [-1, 577, 768]               0\n","              Mlp-18             [-1, 577, 768]               0\n","         Identity-19             [-1, 577, 768]               0\n","            Block-20             [-1, 577, 768]               0\n","        LayerNorm-21             [-1, 577, 768]           1,536\n","           Linear-22            [-1, 577, 2304]       1,771,776\n","          Dropout-23         [-1, 12, 577, 577]               0\n","           Linear-24             [-1, 577, 768]         590,592\n","          Dropout-25             [-1, 577, 768]               0\n","        Attention-26             [-1, 577, 768]               0\n","         Identity-27             [-1, 577, 768]               0\n","        LayerNorm-28             [-1, 577, 768]           1,536\n","           Linear-29            [-1, 577, 3072]       2,362,368\n","             GELU-30            [-1, 577, 3072]               0\n","          Dropout-31            [-1, 577, 3072]               0\n","           Linear-32             [-1, 577, 768]       2,360,064\n","          Dropout-33             [-1, 577, 768]               0\n","              Mlp-34             [-1, 577, 768]               0\n","         Identity-35             [-1, 577, 768]               0\n","            Block-36             [-1, 577, 768]               0\n","        LayerNorm-37             [-1, 577, 768]           1,536\n","           Linear-38            [-1, 577, 2304]       1,771,776\n","          Dropout-39         [-1, 12, 577, 577]               0\n","           Linear-40             [-1, 577, 768]         590,592\n","          Dropout-41             [-1, 577, 768]               0\n","        Attention-42             [-1, 577, 768]               0\n","         Identity-43             [-1, 577, 768]               0\n","        LayerNorm-44             [-1, 577, 768]           1,536\n","           Linear-45            [-1, 577, 3072]       2,362,368\n","             GELU-46            [-1, 577, 3072]               0\n","          Dropout-47            [-1, 577, 3072]               0\n","           Linear-48             [-1, 577, 768]       2,360,064\n","          Dropout-49             [-1, 577, 768]               0\n","              Mlp-50             [-1, 577, 768]               0\n","         Identity-51             [-1, 577, 768]               0\n","            Block-52             [-1, 577, 768]               0\n","        LayerNorm-53             [-1, 577, 768]           1,536\n","           Linear-54            [-1, 577, 2304]       1,771,776\n","          Dropout-55         [-1, 12, 577, 577]               0\n","           Linear-56             [-1, 577, 768]         590,592\n","          Dropout-57             [-1, 577, 768]               0\n","        Attention-58             [-1, 577, 768]               0\n","         Identity-59             [-1, 577, 768]               0\n","        LayerNorm-60             [-1, 577, 768]           1,536\n","           Linear-61            [-1, 577, 3072]       2,362,368\n","             GELU-62            [-1, 577, 3072]               0\n","          Dropout-63            [-1, 577, 3072]               0\n","           Linear-64             [-1, 577, 768]       2,360,064\n","          Dropout-65             [-1, 577, 768]               0\n","              Mlp-66             [-1, 577, 768]               0\n","         Identity-67             [-1, 577, 768]               0\n","            Block-68             [-1, 577, 768]               0\n","        LayerNorm-69             [-1, 577, 768]           1,536\n","           Linear-70            [-1, 577, 2304]       1,771,776\n","          Dropout-71         [-1, 12, 577, 577]               0\n","           Linear-72             [-1, 577, 768]         590,592\n","          Dropout-73             [-1, 577, 768]               0\n","        Attention-74             [-1, 577, 768]               0\n","         Identity-75             [-1, 577, 768]               0\n","        LayerNorm-76             [-1, 577, 768]           1,536\n","           Linear-77            [-1, 577, 3072]       2,362,368\n","             GELU-78            [-1, 577, 3072]               0\n","          Dropout-79            [-1, 577, 3072]               0\n","           Linear-80             [-1, 577, 768]       2,360,064\n","          Dropout-81             [-1, 577, 768]               0\n","              Mlp-82             [-1, 577, 768]               0\n","         Identity-83             [-1, 577, 768]               0\n","            Block-84             [-1, 577, 768]               0\n","        LayerNorm-85             [-1, 577, 768]           1,536\n","           Linear-86            [-1, 577, 2304]       1,771,776\n","          Dropout-87         [-1, 12, 577, 577]               0\n","           Linear-88             [-1, 577, 768]         590,592\n","          Dropout-89             [-1, 577, 768]               0\n","        Attention-90             [-1, 577, 768]               0\n","         Identity-91             [-1, 577, 768]               0\n","        LayerNorm-92             [-1, 577, 768]           1,536\n","           Linear-93            [-1, 577, 3072]       2,362,368\n","             GELU-94            [-1, 577, 3072]               0\n","          Dropout-95            [-1, 577, 3072]               0\n","           Linear-96             [-1, 577, 768]       2,360,064\n","          Dropout-97             [-1, 577, 768]               0\n","              Mlp-98             [-1, 577, 768]               0\n","         Identity-99             [-1, 577, 768]               0\n","           Block-100             [-1, 577, 768]               0\n","       LayerNorm-101             [-1, 577, 768]           1,536\n","          Linear-102            [-1, 577, 2304]       1,771,776\n","         Dropout-103         [-1, 12, 577, 577]               0\n","          Linear-104             [-1, 577, 768]         590,592\n","         Dropout-105             [-1, 577, 768]               0\n","       Attention-106             [-1, 577, 768]               0\n","        Identity-107             [-1, 577, 768]               0\n","       LayerNorm-108             [-1, 577, 768]           1,536\n","          Linear-109            [-1, 577, 3072]       2,362,368\n","            GELU-110            [-1, 577, 3072]               0\n","         Dropout-111            [-1, 577, 3072]               0\n","          Linear-112             [-1, 577, 768]       2,360,064\n","         Dropout-113             [-1, 577, 768]               0\n","             Mlp-114             [-1, 577, 768]               0\n","        Identity-115             [-1, 577, 768]               0\n","           Block-116             [-1, 577, 768]               0\n","       LayerNorm-117             [-1, 577, 768]           1,536\n","          Linear-118            [-1, 577, 2304]       1,771,776\n","         Dropout-119         [-1, 12, 577, 577]               0\n","          Linear-120             [-1, 577, 768]         590,592\n","         Dropout-121             [-1, 577, 768]               0\n","       Attention-122             [-1, 577, 768]               0\n","        Identity-123             [-1, 577, 768]               0\n","       LayerNorm-124             [-1, 577, 768]           1,536\n","          Linear-125            [-1, 577, 3072]       2,362,368\n","            GELU-126            [-1, 577, 3072]               0\n","         Dropout-127            [-1, 577, 3072]               0\n","          Linear-128             [-1, 577, 768]       2,360,064\n","         Dropout-129             [-1, 577, 768]               0\n","             Mlp-130             [-1, 577, 768]               0\n","        Identity-131             [-1, 577, 768]               0\n","           Block-132             [-1, 577, 768]               0\n","       LayerNorm-133             [-1, 577, 768]           1,536\n","          Linear-134            [-1, 577, 2304]       1,771,776\n","         Dropout-135         [-1, 12, 577, 577]               0\n","          Linear-136             [-1, 577, 768]         590,592\n","         Dropout-137             [-1, 577, 768]               0\n","       Attention-138             [-1, 577, 768]               0\n","        Identity-139             [-1, 577, 768]               0\n","       LayerNorm-140             [-1, 577, 768]           1,536\n","          Linear-141            [-1, 577, 3072]       2,362,368\n","            GELU-142            [-1, 577, 3072]               0\n","         Dropout-143            [-1, 577, 3072]               0\n","          Linear-144             [-1, 577, 768]       2,360,064\n","         Dropout-145             [-1, 577, 768]               0\n","             Mlp-146             [-1, 577, 768]               0\n","        Identity-147             [-1, 577, 768]               0\n","           Block-148             [-1, 577, 768]               0\n","       LayerNorm-149             [-1, 577, 768]           1,536\n","          Linear-150            [-1, 577, 2304]       1,771,776\n","         Dropout-151         [-1, 12, 577, 577]               0\n","          Linear-152             [-1, 577, 768]         590,592\n","         Dropout-153             [-1, 577, 768]               0\n","       Attention-154             [-1, 577, 768]               0\n","        Identity-155             [-1, 577, 768]               0\n","       LayerNorm-156             [-1, 577, 768]           1,536\n","          Linear-157            [-1, 577, 3072]       2,362,368\n","            GELU-158            [-1, 577, 3072]               0\n","         Dropout-159            [-1, 577, 3072]               0\n","          Linear-160             [-1, 577, 768]       2,360,064\n","         Dropout-161             [-1, 577, 768]               0\n","             Mlp-162             [-1, 577, 768]               0\n","        Identity-163             [-1, 577, 768]               0\n","           Block-164             [-1, 577, 768]               0\n","       LayerNorm-165             [-1, 577, 768]           1,536\n","          Linear-166            [-1, 577, 2304]       1,771,776\n","         Dropout-167         [-1, 12, 577, 577]               0\n","          Linear-168             [-1, 577, 768]         590,592\n","         Dropout-169             [-1, 577, 768]               0\n","       Attention-170             [-1, 577, 768]               0\n","        Identity-171             [-1, 577, 768]               0\n","       LayerNorm-172             [-1, 577, 768]           1,536\n","          Linear-173            [-1, 577, 3072]       2,362,368\n","            GELU-174            [-1, 577, 3072]               0\n","         Dropout-175            [-1, 577, 3072]               0\n","          Linear-176             [-1, 577, 768]       2,360,064\n","         Dropout-177             [-1, 577, 768]               0\n","             Mlp-178             [-1, 577, 768]               0\n","        Identity-179             [-1, 577, 768]               0\n","           Block-180             [-1, 577, 768]               0\n","       LayerNorm-181             [-1, 577, 768]           1,536\n","          Linear-182            [-1, 577, 2304]       1,771,776\n","         Dropout-183         [-1, 12, 577, 577]               0\n","          Linear-184             [-1, 577, 768]         590,592\n","         Dropout-185             [-1, 577, 768]               0\n","       Attention-186             [-1, 577, 768]               0\n","        Identity-187             [-1, 577, 768]               0\n","       LayerNorm-188             [-1, 577, 768]           1,536\n","          Linear-189            [-1, 577, 3072]       2,362,368\n","            GELU-190            [-1, 577, 3072]               0\n","         Dropout-191            [-1, 577, 3072]               0\n","          Linear-192             [-1, 577, 768]       2,360,064\n","         Dropout-193             [-1, 577, 768]               0\n","             Mlp-194             [-1, 577, 768]               0\n","        Identity-195             [-1, 577, 768]               0\n","           Block-196             [-1, 577, 768]               0\n","       LayerNorm-197             [-1, 577, 768]           1,536\n","        Identity-198                  [-1, 768]               0\n","          Linear-199                    [-1, 2]           1,538\n","================================================================\n","Total params: 85,648,130\n","Trainable params: 85,648,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1437.49\n","Params size (MB): 326.72\n","Estimated Total Size (MB): 1765.90\n","----------------------------------------------------------------\n","model : ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 2.7622451782226562\n","minibatch AVG loss: 3.6687752664089204\n","Epoch: 1     train index of 5 minibatch: 2      time used: 2.5718343257904053\n","minibatch AVG loss: 7.570777893066406\n","Epoch: 1     train index of 5 minibatch: 3      time used: 2.5905303955078125\n","minibatch AVG loss: 3.7298343658447264\n","\n","Epoch: 1  train \n","Loss: 4.5738  Acc: 56.5217\n","benign precision: 53.3333  recall: 51.6129\n","benign sensitivity: 51.6129  specificity: 62.1622\n","benign FPR: 37.8378  NPV: 60.5263\n","benign TP: 16.0\n","benign TN: 23.0\n","benign FP: 14.0\n","benign FN: 15.0\n","malignant precision: 60.5263  recall: 62.1622\n","malignant sensitivity: 62.1622  specificity: 51.6129\n","malignant FPR: 48.3871  NPV: 53.3333\n","malignant TP: 23.0\n","malignant TN: 16.0\n","malignant FP: 15.0\n","malignant FN: 14.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 2.2523  Acc: 68.7500\n","benign precision: 100.0000  recall: 28.5714\n","benign sensitivity: 28.5714  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 64.2857\n","benign TP: 2.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 5.0\n","malignant precision: 64.2857  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 28.5714\n","malignant FPR: 71.4286  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 2.0\n","malignant FP: 5.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 2.8267335891723633\n","minibatch AVG loss: 1.0902635335922242\n","Epoch: 2     train index of 5 minibatch: 2      time used: 2.6658637523651123\n","minibatch AVG loss: 1.510281229019165\n","Epoch: 2     train index of 5 minibatch: 3      time used: 2.6788487434387207\n","minibatch AVG loss: 1.055170053243637\n","\n","Epoch: 2  train \n","Loss: 1.2285  Acc: 97.1014\n","benign precision: 96.5517  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5000\n","benign FPR: 2.5000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5000\n","malignant sensitivity: 97.5000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.5517\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.9711  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 3.009028434753418\n","minibatch AVG loss: 1.516948437690735\n","Epoch: 3     train index of 5 minibatch: 2      time used: 2.7447502613067627\n","minibatch AVG loss: 1.3812364965677262\n","Epoch: 3     train index of 5 minibatch: 3      time used: 2.74536395072937\n","minibatch AVG loss: 1.61913743019104\n","\n","Epoch: 3  train \n","Loss: 1.6895  Acc: 91.3043\n","benign precision: 92.0000  recall: 88.4615\n","benign sensitivity: 88.4615  specificity: 95.2381\n","benign FPR: 4.7619  NPV: 93.0233\n","benign TP: 23.0\n","benign TN: 40.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 93.0233  recall: 95.2381\n","malignant sensitivity: 95.2381  specificity: 88.4615\n","malignant FPR: 11.5385  NPV: 92.0000\n","malignant TP: 40.0\n","malignant TN: 23.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 1.5329  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 2.996419668197632\n","minibatch AVG loss: 0.9721949905157089\n","Epoch: 4     train index of 5 minibatch: 2      time used: 2.715249538421631\n","minibatch AVG loss: 2.223447299003601\n","Epoch: 4     train index of 5 minibatch: 3      time used: 2.696291446685791\n","minibatch AVG loss: 0.9673059582710266\n","\n","Epoch: 4  train \n","Loss: 1.2222  Acc: 94.2029\n","benign precision: 92.8571  recall: 96.2963\n","benign sensitivity: 96.2963  specificity: 95.1220\n","benign FPR: 4.8780  NPV: 97.5000\n","benign TP: 26.0\n","benign TN: 39.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 95.1220\n","malignant sensitivity: 95.1220  specificity: 96.2963\n","malignant FPR: 3.7037  NPV: 92.8571\n","malignant TP: 39.0\n","malignant TN: 26.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.8422  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 2.9709529876708984\n","minibatch AVG loss: 1.4234110713005066\n","Epoch: 5     train index of 5 minibatch: 2      time used: 2.672309398651123\n","minibatch AVG loss: 1.2876343972980977\n","Epoch: 5     train index of 5 minibatch: 3      time used: 2.649740219116211\n","minibatch AVG loss: 1.1535551220178604\n","\n","Epoch: 5  train \n","Loss: 1.3411  Acc: 91.3043\n","benign precision: 90.3226  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 94.5946\n","benign TP: 28.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.5946  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 90.3226\n","malignant TP: 35.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 1.0657  Acc: 81.2500\n","benign precision: 70.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 66.6667\n","benign FPR: 33.3333  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 6.0\n","benign FP: 3.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 66.6667\n","malignant sensitivity: 66.6667  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 70.0000\n","malignant TP: 6.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 2.9121596813201904\n","minibatch AVG loss: 0.9023692786693573\n","Epoch: 6     train index of 5 minibatch: 2      time used: 2.623467445373535\n","minibatch AVG loss: 0.9143170990049839\n","Epoch: 6     train index of 5 minibatch: 3      time used: 2.6168155670166016\n","minibatch AVG loss: 0.8138455629348755\n","\n","Epoch: 6  train \n","Loss: 0.8084  Acc: 97.1014\n","benign precision: 96.4286  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5610\n","benign FPR: 2.4390  NPV: 100.0000\n","benign TP: 27.0\n","benign TN: 40.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5610\n","malignant sensitivity: 97.5610  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.4286\n","malignant TP: 40.0\n","malignant TN: 27.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.4148  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 2.898993492126465\n","minibatch AVG loss: 1.8802382946014404\n","Epoch: 7     train index of 5 minibatch: 2      time used: 2.610396385192871\n","minibatch AVG loss: 1.968974757194519\n","Epoch: 7     train index of 5 minibatch: 3      time used: 2.5905659198760986\n","minibatch AVG loss: 0.4264683037996292\n","\n","Epoch: 7  train \n","Loss: 1.2630  Acc: 92.7536\n","benign precision: 93.5484  recall: 93.5484\n","benign sensitivity: 93.5484  specificity: 94.5946\n","benign FPR: 5.4054  NPV: 94.5946\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.5946  recall: 94.5946\n","malignant sensitivity: 94.5946  specificity: 93.5484\n","malignant FPR: 6.4516  NPV: 93.5484\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.6787  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 2.8861212730407715\n","minibatch AVG loss: 1.1748355269432067\n","Epoch: 8     train index of 5 minibatch: 2      time used: 2.592162609100342\n","minibatch AVG loss: 1.7997578144073487\n","Epoch: 8     train index of 5 minibatch: 3      time used: 2.5966451168060303\n","minibatch AVG loss: 1.5882765531539917\n","\n","Epoch: 8  train \n","Loss: 1.3366  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 40.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 40.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.4861  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 2.7961597442626953\n","minibatch AVG loss: 0.8851803958415985\n","Epoch: 9     train index of 5 minibatch: 2      time used: 2.604769706726074\n","minibatch AVG loss: 1.7014833122491837\n","Epoch: 9     train index of 5 minibatch: 3      time used: 2.6006314754486084\n","minibatch AVG loss: 1.5928293585777282\n","\n","Epoch: 9  train \n","Loss: 1.3581  Acc: 88.4058\n","benign precision: 85.1852  recall: 88.4615\n","benign sensitivity: 88.4615  specificity: 90.4762\n","benign FPR: 9.5238  NPV: 92.6829\n","benign TP: 23.0\n","benign TN: 38.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 92.6829  recall: 90.4762\n","malignant sensitivity: 90.4762  specificity: 88.4615\n","malignant FPR: 11.5385  NPV: 85.1852\n","malignant TP: 38.0\n","malignant TN: 23.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.5988  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 2.816927909851074\n","minibatch AVG loss: 1.6539009928703308\n","Epoch: 10     train index of 5 minibatch: 2      time used: 2.6236915588378906\n","minibatch AVG loss: 1.0550287961959839\n","Epoch: 10     train index of 5 minibatch: 3      time used: 2.624634027481079\n","minibatch AVG loss: 0.7848143868148327\n","\n","Epoch: 10  train \n","Loss: 1.1129  Acc: 91.3043\n","benign precision: 91.1765  recall: 93.9394\n","benign sensitivity: 93.9394  specificity: 91.4286\n","benign FPR: 8.5714  NPV: 94.1176\n","benign TP: 31.0\n","benign TN: 32.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.1176  recall: 91.4286\n","malignant sensitivity: 91.4286  specificity: 93.9394\n","malignant FPR: 6.0606  NPV: 91.1765\n","malignant TP: 32.0\n","malignant TN: 31.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.3993  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 2.829627752304077\n","minibatch AVG loss: 0.9959939569234848\n","Epoch: 11     train index of 5 minibatch: 2      time used: 2.6443276405334473\n","minibatch AVG loss: 0.6765902727842331\n","Epoch: 11     train index of 5 minibatch: 3      time used: 2.636219024658203\n","minibatch AVG loss: 1.4185567051172256\n","\n","Epoch: 11  train \n","Loss: 0.9092  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.4411  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 2.842834949493408\n","minibatch AVG loss: 1.4311052799224853\n","Epoch: 12     train index of 5 minibatch: 2      time used: 2.6633455753326416\n","minibatch AVG loss: 1.470722595602274\n","Epoch: 12     train index of 5 minibatch: 3      time used: 2.6438541412353516\n","minibatch AVG loss: 1.5887068033218383\n","\n","Epoch: 12  train \n","Loss: 1.4192  Acc: 91.3043\n","benign precision: 82.1429  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 23.0\n","benign TN: 40.0\n","benign FP: 5.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 82.1429\n","malignant TP: 40.0\n","malignant TN: 23.0\n","malignant FP: 0.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.6364  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 2.8545241355895996\n","minibatch AVG loss: 0.7505326062440872\n","Epoch: 13     train index of 5 minibatch: 2      time used: 2.6585230827331543\n","minibatch AVG loss: 1.0464153349399568\n","Epoch: 13     train index of 5 minibatch: 3      time used: 2.653228521347046\n","minibatch AVG loss: 1.448187468200922\n","\n","Epoch: 13  train \n","Loss: 1.0487  Acc: 95.6522\n","benign precision: 96.4286  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 97.5000\n","benign FPR: 2.5000  NPV: 97.5000\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 97.5000\n","malignant sensitivity: 97.5000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 96.4286\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.5876  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 2.8468070030212402\n","minibatch AVG loss: 1.2886996030807496\n","Epoch: 14     train index of 5 minibatch: 2      time used: 2.649425745010376\n","minibatch AVG loss: 1.111800593882799\n","Epoch: 14     train index of 5 minibatch: 3      time used: 2.634434700012207\n","minibatch AVG loss: 1.1760000228881835\n","\n","Epoch: 14  train \n","Loss: 1.0805  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.3814  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 2.821915864944458\n","minibatch AVG loss: 0.4616123795509338\n","Epoch: 15     train index of 5 minibatch: 2      time used: 2.6438143253326416\n","minibatch AVG loss: 0.7433915436267853\n","Epoch: 15     train index of 5 minibatch: 3      time used: 2.62449312210083\n","minibatch AVG loss: 1.2348547875881195\n","\n","Epoch: 15  train \n","Loss: 0.9377  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.3190  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 2.8224785327911377\n","minibatch AVG loss: 1.1277145385742187\n","Epoch: 16     train index of 5 minibatch: 2      time used: 2.63285231590271\n","minibatch AVG loss: 1.1843361854553223\n","Epoch: 16     train index of 5 minibatch: 3      time used: 2.617069959640503\n","minibatch AVG loss: 1.3423335313796998\n","\n","Epoch: 16  train \n","Loss: 1.1173  Acc: 94.2029\n","benign precision: 93.5484  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 97.2973\n","benign TP: 29.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 93.5484\n","malignant TP: 36.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.4282  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 2.8082115650177\n","minibatch AVG loss: 1.074584424495697\n","Epoch: 17     train index of 5 minibatch: 2      time used: 2.625709056854248\n","minibatch AVG loss: 0.5920577332377434\n","Epoch: 17     train index of 5 minibatch: 3      time used: 2.6085095405578613\n","minibatch AVG loss: 0.6071138799190521\n","\n","Epoch: 17  train \n","Loss: 0.6756  Acc: 92.7536\n","benign precision: 93.1034  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 94.8718\n","benign TP: 27.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 93.1034\n","malignant TP: 37.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.3123  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 2.8194000720977783\n","minibatch AVG loss: 0.6952256672084332\n","Epoch: 18     train index of 5 minibatch: 2      time used: 2.6182100772857666\n","minibatch AVG loss: 0.9112268090248108\n","Epoch: 18     train index of 5 minibatch: 3      time used: 2.614351987838745\n","minibatch AVG loss: 1.2899267941713333\n","\n","Epoch: 18  train \n","Loss: 0.9846  Acc: 97.1014\n","benign precision: 96.5517  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5000\n","benign FPR: 2.5000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5000\n","malignant sensitivity: 97.5000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.5517\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.3529  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 2.826094150543213\n","minibatch AVG loss: 1.7680922627449036\n","Epoch: 19     train index of 5 minibatch: 2      time used: 2.6389191150665283\n","minibatch AVG loss: 1.0379356145858765\n","Epoch: 19     train index of 5 minibatch: 3      time used: 2.625020980834961\n","minibatch AVG loss: 0.7048861265182496\n","\n","Epoch: 19  train \n","Loss: 1.1058  Acc: 95.6522\n","benign precision: 96.8750  recall: 96.8750\n","benign sensitivity: 96.8750  specificity: 97.2222\n","benign FPR: 2.7778  NPV: 97.2222\n","benign TP: 31.0\n","benign TN: 35.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 97.2222\n","malignant sensitivity: 97.2222  specificity: 96.8750\n","malignant FPR: 3.1250  NPV: 96.8750\n","malignant TP: 35.0\n","malignant TN: 31.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.4933  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 2.8370883464813232\n","minibatch AVG loss: 0.715410441160202\n","Epoch: 20     train index of 5 minibatch: 2      time used: 2.633887767791748\n","minibatch AVG loss: 0.8951643645763397\n","Epoch: 20     train index of 5 minibatch: 3      time used: 2.628669500350952\n","minibatch AVG loss: 1.1267685890197754\n","\n","Epoch: 20  train \n","Loss: 0.9823  Acc: 92.7536\n","benign precision: 90.3226  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 97.2973\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 90.3226\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.3598  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 2.8369147777557373\n","minibatch AVG loss: 0.5853398069739342\n","Epoch: 21     train index of 5 minibatch: 2      time used: 2.641484022140503\n","minibatch AVG loss: 1.0733947038650513\n","Epoch: 21     train index of 5 minibatch: 3      time used: 2.626905679702759\n","minibatch AVG loss: 1.327180525660515\n","\n","Epoch: 21  train \n","Loss: 0.9441  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.7742\n","benign sensitivity: 96.7742  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.3684\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.7742\n","malignant FPR: 3.2258  NPV: 100.0000\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.3046  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 2.8285765647888184\n","minibatch AVG loss: 0.8475468695163727\n","Epoch: 22     train index of 5 minibatch: 2      time used: 2.630746364593506\n","minibatch AVG loss: 1.1335631415247918\n","Epoch: 22     train index of 5 minibatch: 3      time used: 2.6418638229370117\n","minibatch AVG loss: 0.3178201586008072\n","\n","Epoch: 22  train \n","Loss: 0.8863  Acc: 97.1014\n","benign precision: 96.9697  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.2222\n","benign FPR: 2.7778  NPV: 100.0000\n","benign TP: 32.0\n","benign TN: 35.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.2222\n","malignant sensitivity: 97.2222  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.9697\n","malignant TP: 35.0\n","malignant TN: 32.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.6099  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 2.836272954940796\n","minibatch AVG loss: 1.8571684598922729\n","Epoch: 23     train index of 5 minibatch: 2      time used: 2.653376579284668\n","minibatch AVG loss: 1.6430218935012817\n","Epoch: 23     train index of 5 minibatch: 3      time used: 2.6339972019195557\n","minibatch AVG loss: 1.0799182206392288\n","\n","Epoch: 23  train \n","Loss: 1.4246  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.3630  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 2.8488612174987793\n","minibatch AVG loss: 0.3793129831552505\n","Epoch: 24     train index of 5 minibatch: 2      time used: 2.653691291809082\n","minibatch AVG loss: 1.236920565366745\n","Epoch: 24     train index of 5 minibatch: 3      time used: 2.646552324295044\n","minibatch AVG loss: 1.311296033859253\n","\n","Epoch: 24  train \n","Loss: 0.9417  Acc: 92.7536\n","benign precision: 90.6250  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 97.2222\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 90.6250\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.2528  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 2.8427205085754395\n","minibatch AVG loss: 0.6395874738693237\n","Epoch: 25     train index of 5 minibatch: 2      time used: 2.659407615661621\n","minibatch AVG loss: 0.9921695530414582\n","Epoch: 25     train index of 5 minibatch: 3      time used: 2.6442086696624756\n","minibatch AVG loss: 1.6062154293060302\n","\n","Epoch: 25  train \n","Loss: 1.0123  Acc: 94.2029\n","benign precision: 96.4286  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 95.0000\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 95.0000  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 96.4286\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.3168  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 2.8328726291656494\n","minibatch AVG loss: 1.1769685842096806\n","Epoch: 26     train index of 5 minibatch: 2      time used: 2.653653383255005\n","minibatch AVG loss: 1.193901252746582\n","Epoch: 26     train index of 5 minibatch: 3      time used: 2.6451570987701416\n","minibatch AVG loss: 1.3840530633926391\n","\n","Epoch: 26  train \n","Loss: 1.1614  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.5607  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 2.8406453132629395\n","minibatch AVG loss: 0.9492335170507431\n","Epoch: 27     train index of 5 minibatch: 2      time used: 2.652106523513794\n","minibatch AVG loss: 0.5368976145982742\n","Epoch: 27     train index of 5 minibatch: 3      time used: 2.6410319805145264\n","minibatch AVG loss: 0.35356453657150266\n","\n","Epoch: 27  train \n","Loss: 0.5828  Acc: 92.7536\n","benign precision: 100.0000  recall: 87.5000\n","benign sensitivity: 87.5000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 87.5000\n","malignant FPR: 12.5000  NPV: 100.0000\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.3197  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 2.8323476314544678\n","minibatch AVG loss: 0.8847260355949402\n","Epoch: 28     train index of 5 minibatch: 2      time used: 2.6502885818481445\n","minibatch AVG loss: 1.073709988594055\n","Epoch: 28     train index of 5 minibatch: 3      time used: 2.6296634674072266\n","minibatch AVG loss: 0.9690504193305969\n","\n","Epoch: 28  train \n","Loss: 1.0257  Acc: 91.3043\n","benign precision: 92.8571  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 92.5000\n","benign TP: 26.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.5000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 92.8571\n","malignant TP: 37.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.4287  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 2.8370208740234375\n","minibatch AVG loss: 0.7680596560239792\n","Epoch: 29     train index of 5 minibatch: 2      time used: 2.640946388244629\n","minibatch AVG loss: 0.5448404714465142\n","Epoch: 29     train index of 5 minibatch: 3      time used: 2.6343231201171875\n","minibatch AVG loss: 0.5804515108466148\n","\n","Epoch: 29  train \n","Loss: 0.6917  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.2452  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 2.850919008255005\n","minibatch AVG loss: 1.7385220795869827\n","Epoch: 30     train index of 5 minibatch: 2      time used: 2.638640880584717\n","minibatch AVG loss: 1.0063090175390244\n","Epoch: 30     train index of 5 minibatch: 3      time used: 2.6316723823547363\n","minibatch AVG loss: 1.058499526977539\n","\n","Epoch: 30  train \n","Loss: 1.2467  Acc: 92.7536\n","benign precision: 96.9697  recall: 91.4286\n","benign sensitivity: 91.4286  specificity: 96.9697\n","benign FPR: 3.0303  NPV: 91.4286\n","benign TP: 32.0\n","benign TN: 32.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 91.4286  recall: 96.9697\n","malignant sensitivity: 96.9697  specificity: 91.4286\n","malignant FPR: 8.5714  NPV: 96.9697\n","malignant TP: 32.0\n","malignant TN: 32.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.3902  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 2.844735860824585\n","minibatch AVG loss: 0.9068535730242729\n","Epoch: 31     train index of 5 minibatch: 2      time used: 2.6511483192443848\n","minibatch AVG loss: 0.9362437546253204\n","Epoch: 31     train index of 5 minibatch: 3      time used: 2.6262223720550537\n","minibatch AVG loss: 1.3711767077445984\n","\n","Epoch: 31  train \n","Loss: 1.0277  Acc: 89.8551\n","benign precision: 93.5484  recall: 87.8788\n","benign sensitivity: 87.8788  specificity: 94.2857\n","benign FPR: 5.7143  NPV: 89.1892\n","benign TP: 29.0\n","benign TN: 33.0\n","benign FP: 2.0\n","benign FN: 4.0\n","malignant precision: 89.1892  recall: 94.2857\n","malignant sensitivity: 94.2857  specificity: 87.8788\n","malignant FPR: 12.1212  NPV: 93.5484\n","malignant TP: 33.0\n","malignant TN: 29.0\n","malignant FP: 4.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.3583  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 2.8337647914886475\n","minibatch AVG loss: 0.7067427575588227\n","Epoch: 32     train index of 5 minibatch: 2      time used: 2.635956048965454\n","minibatch AVG loss: 0.5804029248654843\n","Epoch: 32     train index of 5 minibatch: 3      time used: 2.628662586212158\n","minibatch AVG loss: 0.8931174159049988\n","\n","Epoch: 32  train \n","Loss: 0.7372  Acc: 95.6522\n","benign precision: 93.5484  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.5484\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.3258  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 2.8268375396728516\n","minibatch AVG loss: 1.6236171841621398\n","Epoch: 33     train index of 5 minibatch: 2      time used: 2.6412689685821533\n","minibatch AVG loss: 1.1788923740386963\n","Epoch: 33     train index of 5 minibatch: 3      time used: 2.634204864501953\n","minibatch AVG loss: 0.7707098394632339\n","\n","Epoch: 33  train \n","Loss: 1.1408  Acc: 95.6522\n","benign precision: 92.5926  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 95.3488\n","benign FPR: 4.6512  NPV: 100.0000\n","benign TP: 25.0\n","benign TN: 41.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 95.3488\n","malignant sensitivity: 95.3488  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 92.5926\n","malignant TP: 41.0\n","malignant TN: 25.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.2952  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 2.8337562084198\n","minibatch AVG loss: 0.7867922186851501\n","Epoch: 34     train index of 5 minibatch: 2      time used: 2.6403188705444336\n","minibatch AVG loss: 0.47239611446857455\n","Epoch: 34     train index of 5 minibatch: 3      time used: 2.628774404525757\n","minibatch AVG loss: 1.3736692547798157\n","\n","Epoch: 34  train \n","Loss: 0.7747  Acc: 95.6522\n","benign precision: 96.2963  recall: 96.2963\n","benign sensitivity: 96.2963  specificity: 97.5610\n","benign FPR: 2.4390  NPV: 97.5610\n","benign TP: 26.0\n","benign TN: 40.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.5610  recall: 97.5610\n","malignant sensitivity: 97.5610  specificity: 96.2963\n","malignant FPR: 3.7037  NPV: 96.2963\n","malignant TP: 40.0\n","malignant TN: 26.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.3258  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 2.8369665145874023\n","minibatch AVG loss: 0.5866216018795967\n","Epoch: 35     train index of 5 minibatch: 2      time used: 2.6405138969421387\n","minibatch AVG loss: 1.3350484639406204\n","Epoch: 35     train index of 5 minibatch: 3      time used: 2.633148193359375\n","minibatch AVG loss: 0.1669160470366478\n","\n","Epoch: 35  train \n","Loss: 0.6706  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.7742\n","benign sensitivity: 96.7742  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.3684\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.7742\n","malignant FPR: 3.2258  NPV: 100.0000\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.2359  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 2.8357651233673096\n","minibatch AVG loss: 0.8528209015727043\n","Epoch: 36     train index of 5 minibatch: 2      time used: 2.6413278579711914\n","minibatch AVG loss: 0.9849356293678284\n","Epoch: 36     train index of 5 minibatch: 3      time used: 2.631791830062866\n","minibatch AVG loss: 0.9469382807612419\n","\n","Epoch: 36  train \n","Loss: 0.9847  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.2423  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 2.835873603820801\n","minibatch AVG loss: 0.9257538259029389\n","Epoch: 37     train index of 5 minibatch: 2      time used: 2.6555309295654297\n","minibatch AVG loss: 0.2989647924900055\n","Epoch: 37     train index of 5 minibatch: 3      time used: 2.627171516418457\n","minibatch AVG loss: 0.7323665633797646\n","\n","Epoch: 37  train \n","Loss: 0.6148  Acc: 95.6522\n","benign precision: 93.3333  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.3333\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.2587  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 2.8363101482391357\n","minibatch AVG loss: 1.3977117747068406\n","Epoch: 38     train index of 5 minibatch: 2      time used: 2.649097204208374\n","minibatch AVG loss: 0.9475944891571999\n","Epoch: 38     train index of 5 minibatch: 3      time used: 2.6428062915802\n","minibatch AVG loss: 0.9341781958937645\n","\n","Epoch: 38  train \n","Loss: 1.1244  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5610\n","benign TP: 27.0\n","benign TN: 40.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5610  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 100.0000\n","malignant TP: 40.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.2426  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 2.8373541831970215\n","minibatch AVG loss: 0.8339040040969848\n","Epoch: 39     train index of 5 minibatch: 2      time used: 2.6514394283294678\n","minibatch AVG loss: 1.1741438508033752\n","Epoch: 39     train index of 5 minibatch: 3      time used: 2.6306135654449463\n","minibatch AVG loss: 0.7786183059215546\n","\n","Epoch: 39  train \n","Loss: 0.9184  Acc: 95.6522\n","benign precision: 93.9394  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.5946\n","benign FPR: 5.4054  NPV: 100.0000\n","benign TP: 31.0\n","benign TN: 35.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.5946\n","malignant sensitivity: 94.5946  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.9394\n","malignant TP: 35.0\n","malignant TN: 31.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.2725  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 2.835313558578491\n","minibatch AVG loss: 1.2112611293792725\n","Epoch: 40     train index of 5 minibatch: 2      time used: 2.6521058082580566\n","minibatch AVG loss: 0.9937446355819702\n","Epoch: 40     train index of 5 minibatch: 3      time used: 2.6378884315490723\n","minibatch AVG loss: 0.8290455751121044\n","\n","Epoch: 40  train \n","Loss: 1.0005  Acc: 97.1014\n","benign precision: 100.0000  recall: 97.1429\n","benign sensitivity: 97.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.0588\n","benign TP: 34.0\n","benign TN: 33.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.0588  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 97.1429\n","malignant FPR: 2.8571  NPV: 100.0000\n","malignant TP: 33.0\n","malignant TN: 34.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.2877  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 2.8291776180267334\n","minibatch AVG loss: 0.360945862531662\n","Epoch: 41     train index of 5 minibatch: 2      time used: 2.6427130699157715\n","minibatch AVG loss: 0.6764381259679795\n","Epoch: 41     train index of 5 minibatch: 3      time used: 2.636962413787842\n","minibatch AVG loss: 0.5829239472746849\n","\n","Epoch: 41  train \n","Loss: 0.5733  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 25.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 25.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.2506  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 2.838435411453247\n","minibatch AVG loss: 1.0305887907743454\n","Epoch: 42     train index of 5 minibatch: 2      time used: 2.649401903152466\n","minibatch AVG loss: 0.7320554502308368\n","Epoch: 42     train index of 5 minibatch: 3      time used: 2.631787061691284\n","minibatch AVG loss: 0.9799264162778855\n","\n","Epoch: 42  train \n","Loss: 0.9268  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.2302  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 2.8376593589782715\n","minibatch AVG loss: 0.7825192838907242\n","Epoch: 43     train index of 5 minibatch: 2      time used: 2.6495203971862793\n","minibatch AVG loss: 0.7407818228006363\n","Epoch: 43     train index of 5 minibatch: 3      time used: 2.6365551948547363\n","minibatch AVG loss: 1.4319301426410675\n","\n","Epoch: 43  train \n","Loss: 0.9245  Acc: 92.7536\n","benign precision: 86.6667  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 90.4762\n","benign FPR: 9.5238  NPV: 100.0000\n","benign TP: 26.0\n","benign TN: 38.0\n","benign FP: 4.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 90.4762\n","malignant sensitivity: 90.4762  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 86.6667\n","malignant TP: 38.0\n","malignant TN: 26.0\n","malignant FP: 0.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.2514  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 2.8451755046844482\n","minibatch AVG loss: 0.7364541158080101\n","Epoch: 44     train index of 5 minibatch: 2      time used: 2.6543633937835693\n","minibatch AVG loss: 0.6853869974613189\n","Epoch: 44     train index of 5 minibatch: 3      time used: 2.632934093475342\n","minibatch AVG loss: 1.141390573978424\n","\n","Epoch: 44  train \n","Loss: 0.8559  Acc: 95.6522\n","benign precision: 96.7742  recall: 96.7742\n","benign sensitivity: 96.7742  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 97.2973\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 96.7742\n","malignant FPR: 3.2258  NPV: 96.7742\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.2695  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 2.847426414489746\n","minibatch AVG loss: 0.775562871992588\n","Epoch: 45     train index of 5 minibatch: 2      time used: 2.6539406776428223\n","minibatch AVG loss: 0.9550624407827855\n","Epoch: 45     train index of 5 minibatch: 3      time used: 2.635524034500122\n","minibatch AVG loss: 0.19156823307275772\n","\n","Epoch: 45  train \n","Loss: 0.6001  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 31.0\n","benign TN: 37.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 37.0\n","malignant TN: 31.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.2675  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 2.8497679233551025\n","minibatch AVG loss: 0.7821157708764076\n","Epoch: 46     train index of 5 minibatch: 2      time used: 2.659670114517212\n","minibatch AVG loss: 0.7784400328993797\n","Epoch: 46     train index of 5 minibatch: 3      time used: 2.63519549369812\n","minibatch AVG loss: 0.7405585408210754\n","\n","Epoch: 46  train \n","Loss: 0.7616  Acc: 92.7536\n","benign precision: 89.6552  recall: 96.2963\n","benign sensitivity: 96.2963  specificity: 92.6829\n","benign FPR: 7.3171  NPV: 97.4359\n","benign TP: 26.0\n","benign TN: 38.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 92.6829\n","malignant sensitivity: 92.6829  specificity: 96.2963\n","malignant FPR: 3.7037  NPV: 89.6552\n","malignant TP: 38.0\n","malignant TN: 26.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.2246  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 2.846829652786255\n","minibatch AVG loss: 1.008537870645523\n","Epoch: 47     train index of 5 minibatch: 2      time used: 2.651240348815918\n","minibatch AVG loss: 0.5129161283373833\n","Epoch: 47     train index of 5 minibatch: 3      time used: 2.629769802093506\n","minibatch AVG loss: 0.8735064148902894\n","\n","Epoch: 47  train \n","Loss: 0.7325  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 32.0\n","benign TN: 36.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 36.0\n","malignant TN: 32.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.2139  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 2.842158794403076\n","minibatch AVG loss: 1.6146043181419372\n","Epoch: 48     train index of 5 minibatch: 2      time used: 2.651427745819092\n","minibatch AVG loss: 0.6193066447973251\n","Epoch: 48     train index of 5 minibatch: 3      time used: 2.6294450759887695\n","minibatch AVG loss: 0.5147448822855949\n","\n","Epoch: 48  train \n","Loss: 0.8602  Acc: 95.6522\n","benign precision: 93.3333  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.3333\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.2260  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 2.8410212993621826\n","minibatch AVG loss: 0.7959728471934795\n","Epoch: 49     train index of 5 minibatch: 2      time used: 2.6560685634613037\n","minibatch AVG loss: 0.7675910085439682\n","Epoch: 49     train index of 5 minibatch: 3      time used: 2.631614923477173\n","minibatch AVG loss: 0.8133109152317047\n","\n","Epoch: 49  train \n","Loss: 0.8287  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.2368  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 2.840409755706787\n","minibatch AVG loss: 0.4629516497254372\n","Epoch: 50     train index of 5 minibatch: 2      time used: 2.6514647006988525\n","minibatch AVG loss: 0.38110963478684423\n","Epoch: 50     train index of 5 minibatch: 3      time used: 2.6316092014312744\n","minibatch AVG loss: 0.5668440222740173\n","\n","Epoch: 50  train \n","Loss: 0.5267  Acc: 95.6522\n","benign precision: 92.8571  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 95.2381\n","benign FPR: 4.7619  NPV: 100.0000\n","benign TP: 26.0\n","benign TN: 40.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 95.2381\n","malignant sensitivity: 95.2381  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 92.8571\n","malignant TP: 40.0\n","malignant TN: 26.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.2473  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 8m 26s\n","Best epoch idx:  47\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS.pth\n"]}],"source":["!python Train.py --model_idx ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS --augmentation_name Mixup --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FSe1lRILyYK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651155144851,"user_tz":-480,"elapsed":186062,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"8b50093e-b8b1-411e-8dbf-e03f1a63de45"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ResNet50_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100% 97.8M/97.8M [00:00<00:00, 168MB/s]\n","test model output： tensor([[0.1968, 0.2936]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 192, 192]           9,408\n","       BatchNorm2d-2         [-1, 64, 192, 192]             128\n","              ReLU-3         [-1, 64, 192, 192]               0\n","         MaxPool2d-4           [-1, 64, 96, 96]               0\n","            Conv2d-5           [-1, 64, 96, 96]           4,096\n","       BatchNorm2d-6           [-1, 64, 96, 96]             128\n","              ReLU-7           [-1, 64, 96, 96]               0\n","            Conv2d-8           [-1, 64, 96, 96]          36,864\n","       BatchNorm2d-9           [-1, 64, 96, 96]             128\n","             ReLU-10           [-1, 64, 96, 96]               0\n","           Conv2d-11          [-1, 256, 96, 96]          16,384\n","      BatchNorm2d-12          [-1, 256, 96, 96]             512\n","           Conv2d-13          [-1, 256, 96, 96]          16,384\n","      BatchNorm2d-14          [-1, 256, 96, 96]             512\n","             ReLU-15          [-1, 256, 96, 96]               0\n","       Bottleneck-16          [-1, 256, 96, 96]               0\n","           Conv2d-17           [-1, 64, 96, 96]          16,384\n","      BatchNorm2d-18           [-1, 64, 96, 96]             128\n","             ReLU-19           [-1, 64, 96, 96]               0\n","           Conv2d-20           [-1, 64, 96, 96]          36,864\n","      BatchNorm2d-21           [-1, 64, 96, 96]             128\n","             ReLU-22           [-1, 64, 96, 96]               0\n","           Conv2d-23          [-1, 256, 96, 96]          16,384\n","      BatchNorm2d-24          [-1, 256, 96, 96]             512\n","             ReLU-25          [-1, 256, 96, 96]               0\n","       Bottleneck-26          [-1, 256, 96, 96]               0\n","           Conv2d-27           [-1, 64, 96, 96]          16,384\n","      BatchNorm2d-28           [-1, 64, 96, 96]             128\n","             ReLU-29           [-1, 64, 96, 96]               0\n","           Conv2d-30           [-1, 64, 96, 96]          36,864\n","      BatchNorm2d-31           [-1, 64, 96, 96]             128\n","             ReLU-32           [-1, 64, 96, 96]               0\n","           Conv2d-33          [-1, 256, 96, 96]          16,384\n","      BatchNorm2d-34          [-1, 256, 96, 96]             512\n","             ReLU-35          [-1, 256, 96, 96]               0\n","       Bottleneck-36          [-1, 256, 96, 96]               0\n","           Conv2d-37          [-1, 128, 96, 96]          32,768\n","      BatchNorm2d-38          [-1, 128, 96, 96]             256\n","             ReLU-39          [-1, 128, 96, 96]               0\n","           Conv2d-40          [-1, 128, 48, 48]         147,456\n","      BatchNorm2d-41          [-1, 128, 48, 48]             256\n","             ReLU-42          [-1, 128, 48, 48]               0\n","           Conv2d-43          [-1, 512, 48, 48]          65,536\n","      BatchNorm2d-44          [-1, 512, 48, 48]           1,024\n","           Conv2d-45          [-1, 512, 48, 48]         131,072\n","      BatchNorm2d-46          [-1, 512, 48, 48]           1,024\n","             ReLU-47          [-1, 512, 48, 48]               0\n","       Bottleneck-48          [-1, 512, 48, 48]               0\n","           Conv2d-49          [-1, 128, 48, 48]          65,536\n","      BatchNorm2d-50          [-1, 128, 48, 48]             256\n","             ReLU-51          [-1, 128, 48, 48]               0\n","           Conv2d-52          [-1, 128, 48, 48]         147,456\n","      BatchNorm2d-53          [-1, 128, 48, 48]             256\n","             ReLU-54          [-1, 128, 48, 48]               0\n","           Conv2d-55          [-1, 512, 48, 48]          65,536\n","      BatchNorm2d-56          [-1, 512, 48, 48]           1,024\n","             ReLU-57          [-1, 512, 48, 48]               0\n","       Bottleneck-58          [-1, 512, 48, 48]               0\n","           Conv2d-59          [-1, 128, 48, 48]          65,536\n","      BatchNorm2d-60          [-1, 128, 48, 48]             256\n","             ReLU-61          [-1, 128, 48, 48]               0\n","           Conv2d-62          [-1, 128, 48, 48]         147,456\n","      BatchNorm2d-63          [-1, 128, 48, 48]             256\n","             ReLU-64          [-1, 128, 48, 48]               0\n","           Conv2d-65          [-1, 512, 48, 48]          65,536\n","      BatchNorm2d-66          [-1, 512, 48, 48]           1,024\n","             ReLU-67          [-1, 512, 48, 48]               0\n","       Bottleneck-68          [-1, 512, 48, 48]               0\n","           Conv2d-69          [-1, 128, 48, 48]          65,536\n","      BatchNorm2d-70          [-1, 128, 48, 48]             256\n","             ReLU-71          [-1, 128, 48, 48]               0\n","           Conv2d-72          [-1, 128, 48, 48]         147,456\n","      BatchNorm2d-73          [-1, 128, 48, 48]             256\n","             ReLU-74          [-1, 128, 48, 48]               0\n","           Conv2d-75          [-1, 512, 48, 48]          65,536\n","      BatchNorm2d-76          [-1, 512, 48, 48]           1,024\n","             ReLU-77          [-1, 512, 48, 48]               0\n","       Bottleneck-78          [-1, 512, 48, 48]               0\n","           Conv2d-79          [-1, 256, 48, 48]         131,072\n","      BatchNorm2d-80          [-1, 256, 48, 48]             512\n","             ReLU-81          [-1, 256, 48, 48]               0\n","           Conv2d-82          [-1, 256, 24, 24]         589,824\n","      BatchNorm2d-83          [-1, 256, 24, 24]             512\n","             ReLU-84          [-1, 256, 24, 24]               0\n","           Conv2d-85         [-1, 1024, 24, 24]         262,144\n","      BatchNorm2d-86         [-1, 1024, 24, 24]           2,048\n","           Conv2d-87         [-1, 1024, 24, 24]         524,288\n","      BatchNorm2d-88         [-1, 1024, 24, 24]           2,048\n","             ReLU-89         [-1, 1024, 24, 24]               0\n","       Bottleneck-90         [-1, 1024, 24, 24]               0\n","           Conv2d-91          [-1, 256, 24, 24]         262,144\n","      BatchNorm2d-92          [-1, 256, 24, 24]             512\n","             ReLU-93          [-1, 256, 24, 24]               0\n","           Conv2d-94          [-1, 256, 24, 24]         589,824\n","      BatchNorm2d-95          [-1, 256, 24, 24]             512\n","             ReLU-96          [-1, 256, 24, 24]               0\n","           Conv2d-97         [-1, 1024, 24, 24]         262,144\n","      BatchNorm2d-98         [-1, 1024, 24, 24]           2,048\n","             ReLU-99         [-1, 1024, 24, 24]               0\n","      Bottleneck-100         [-1, 1024, 24, 24]               0\n","          Conv2d-101          [-1, 256, 24, 24]         262,144\n","     BatchNorm2d-102          [-1, 256, 24, 24]             512\n","            ReLU-103          [-1, 256, 24, 24]               0\n","          Conv2d-104          [-1, 256, 24, 24]         589,824\n","     BatchNorm2d-105          [-1, 256, 24, 24]             512\n","            ReLU-106          [-1, 256, 24, 24]               0\n","          Conv2d-107         [-1, 1024, 24, 24]         262,144\n","     BatchNorm2d-108         [-1, 1024, 24, 24]           2,048\n","            ReLU-109         [-1, 1024, 24, 24]               0\n","      Bottleneck-110         [-1, 1024, 24, 24]               0\n","          Conv2d-111          [-1, 256, 24, 24]         262,144\n","     BatchNorm2d-112          [-1, 256, 24, 24]             512\n","            ReLU-113          [-1, 256, 24, 24]               0\n","          Conv2d-114          [-1, 256, 24, 24]         589,824\n","     BatchNorm2d-115          [-1, 256, 24, 24]             512\n","            ReLU-116          [-1, 256, 24, 24]               0\n","          Conv2d-117         [-1, 1024, 24, 24]         262,144\n","     BatchNorm2d-118         [-1, 1024, 24, 24]           2,048\n","            ReLU-119         [-1, 1024, 24, 24]               0\n","      Bottleneck-120         [-1, 1024, 24, 24]               0\n","          Conv2d-121          [-1, 256, 24, 24]         262,144\n","     BatchNorm2d-122          [-1, 256, 24, 24]             512\n","            ReLU-123          [-1, 256, 24, 24]               0\n","          Conv2d-124          [-1, 256, 24, 24]         589,824\n","     BatchNorm2d-125          [-1, 256, 24, 24]             512\n","            ReLU-126          [-1, 256, 24, 24]               0\n","          Conv2d-127         [-1, 1024, 24, 24]         262,144\n","     BatchNorm2d-128         [-1, 1024, 24, 24]           2,048\n","            ReLU-129         [-1, 1024, 24, 24]               0\n","      Bottleneck-130         [-1, 1024, 24, 24]               0\n","          Conv2d-131          [-1, 256, 24, 24]         262,144\n","     BatchNorm2d-132          [-1, 256, 24, 24]             512\n","            ReLU-133          [-1, 256, 24, 24]               0\n","          Conv2d-134          [-1, 256, 24, 24]         589,824\n","     BatchNorm2d-135          [-1, 256, 24, 24]             512\n","            ReLU-136          [-1, 256, 24, 24]               0\n","          Conv2d-137         [-1, 1024, 24, 24]         262,144\n","     BatchNorm2d-138         [-1, 1024, 24, 24]           2,048\n","            ReLU-139         [-1, 1024, 24, 24]               0\n","      Bottleneck-140         [-1, 1024, 24, 24]               0\n","          Conv2d-141          [-1, 512, 24, 24]         524,288\n","     BatchNorm2d-142          [-1, 512, 24, 24]           1,024\n","            ReLU-143          [-1, 512, 24, 24]               0\n","          Conv2d-144          [-1, 512, 12, 12]       2,359,296\n","     BatchNorm2d-145          [-1, 512, 12, 12]           1,024\n","            ReLU-146          [-1, 512, 12, 12]               0\n","          Conv2d-147         [-1, 2048, 12, 12]       1,048,576\n","     BatchNorm2d-148         [-1, 2048, 12, 12]           4,096\n","          Conv2d-149         [-1, 2048, 12, 12]       2,097,152\n","     BatchNorm2d-150         [-1, 2048, 12, 12]           4,096\n","            ReLU-151         [-1, 2048, 12, 12]               0\n","      Bottleneck-152         [-1, 2048, 12, 12]               0\n","          Conv2d-153          [-1, 512, 12, 12]       1,048,576\n","     BatchNorm2d-154          [-1, 512, 12, 12]           1,024\n","            ReLU-155          [-1, 512, 12, 12]               0\n","          Conv2d-156          [-1, 512, 12, 12]       2,359,296\n","     BatchNorm2d-157          [-1, 512, 12, 12]           1,024\n","            ReLU-158          [-1, 512, 12, 12]               0\n","          Conv2d-159         [-1, 2048, 12, 12]       1,048,576\n","     BatchNorm2d-160         [-1, 2048, 12, 12]           4,096\n","            ReLU-161         [-1, 2048, 12, 12]               0\n","      Bottleneck-162         [-1, 2048, 12, 12]               0\n","          Conv2d-163          [-1, 512, 12, 12]       1,048,576\n","     BatchNorm2d-164          [-1, 512, 12, 12]           1,024\n","            ReLU-165          [-1, 512, 12, 12]               0\n","          Conv2d-166          [-1, 512, 12, 12]       2,359,296\n","     BatchNorm2d-167          [-1, 512, 12, 12]           1,024\n","            ReLU-168          [-1, 512, 12, 12]               0\n","          Conv2d-169         [-1, 2048, 12, 12]       1,048,576\n","     BatchNorm2d-170         [-1, 2048, 12, 12]           4,096\n","            ReLU-171         [-1, 2048, 12, 12]               0\n","      Bottleneck-172         [-1, 2048, 12, 12]               0\n","AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n","          Linear-174                    [-1, 2]           4,098\n","================================================================\n","Total params: 23,512,130\n","Trainable params: 23,512,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 842.08\n","Params size (MB): 89.69\n","Estimated Total Size (MB): 933.46\n","----------------------------------------------------------------\n","model : ResNet50_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 1.2120461463928223\n","minibatch AVG loss: 0.6562297940254211\n","Epoch: 1     train index of 5 minibatch: 2      time used: 0.8683364391326904\n","minibatch AVG loss: 0.6783500552177429\n","Epoch: 1     train index of 5 minibatch: 3      time used: 0.8690407276153564\n","minibatch AVG loss: 0.6549811959266663\n","\n","Epoch: 1  train \n","Loss: 0.6451  Acc: 71.0145\n","benign precision: 76.1905  recall: 53.3333\n","benign sensitivity: 53.3333  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 70.2128\n","benign TP: 16.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 14.0\n","malignant precision: 70.2128  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 53.3333\n","malignant FPR: 46.6667  NPV: 76.1905\n","malignant TP: 33.0\n","malignant TN: 16.0\n","malignant FP: 14.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.6875  Acc: 56.2500\n","benign precision: 0.0000  recall: 0.0000\n","benign sensitivity: 0.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 56.2500\n","benign TP: 0.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 7.0\n","malignant precision: 56.2500  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 0.0000\n","malignant FPR: 100.0000  NPV: 0.0000\n","malignant TP: 9.0\n","malignant TN: 0.0\n","malignant FP: 7.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 1.0813531875610352\n","minibatch AVG loss: 0.5668581128120422\n","Epoch: 2     train index of 5 minibatch: 2      time used: 0.8722612857818604\n","minibatch AVG loss: 0.6134514570236206\n","Epoch: 2     train index of 5 minibatch: 3      time used: 0.8690547943115234\n","minibatch AVG loss: 0.5467077732086182\n","\n","Epoch: 2  train \n","Loss: 0.5701  Acc: 82.6087\n","benign precision: 87.5000  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 81.8182\n","benign TP: 21.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 8.0\n","malignant precision: 81.8182  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 87.5000\n","malignant TP: 36.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.6539  Acc: 62.5000\n","benign precision: 100.0000  recall: 14.2857\n","benign sensitivity: 14.2857  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 60.0000\n","benign TP: 1.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 6.0\n","malignant precision: 60.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 14.2857\n","malignant FPR: 85.7143  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 1.0\n","malignant FP: 6.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 1.0848212242126465\n","minibatch AVG loss: 0.5247325360774994\n","Epoch: 3     train index of 5 minibatch: 2      time used: 0.8712453842163086\n","minibatch AVG loss: 0.561178570985794\n","Epoch: 3     train index of 5 minibatch: 3      time used: 0.8759860992431641\n","minibatch AVG loss: 0.4812006175518036\n","\n","Epoch: 3  train \n","Loss: 0.5042  Acc: 84.0580\n","benign precision: 85.1852  recall: 79.3103\n","benign sensitivity: 79.3103  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 85.3659\n","benign TP: 23.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 6.0\n","malignant precision: 85.3659  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 79.3103\n","malignant FPR: 20.6897  NPV: 85.1852\n","malignant TP: 35.0\n","malignant TN: 23.0\n","malignant FP: 6.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.6322  Acc: 62.5000\n","benign precision: 100.0000  recall: 14.2857\n","benign sensitivity: 14.2857  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 60.0000\n","benign TP: 1.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 6.0\n","malignant precision: 60.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 14.2857\n","malignant FPR: 85.7143  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 1.0\n","malignant FP: 6.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 1.0925264358520508\n","minibatch AVG loss: 0.39902878403663633\n","Epoch: 4     train index of 5 minibatch: 2      time used: 0.8750081062316895\n","minibatch AVG loss: 0.4536194264888763\n","Epoch: 4     train index of 5 minibatch: 3      time used: 0.8786451816558838\n","minibatch AVG loss: 0.4867986232042313\n","\n","Epoch: 4  train \n","Loss: 0.4419  Acc: 86.9565\n","benign precision: 95.8333  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 84.0909\n","benign TP: 23.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 7.0\n","malignant precision: 84.0909  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 95.8333\n","malignant TP: 37.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.5748  Acc: 68.7500\n","benign precision: 100.0000  recall: 28.5714\n","benign sensitivity: 28.5714  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 64.2857\n","benign TP: 2.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 5.0\n","malignant precision: 64.2857  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 28.5714\n","malignant FPR: 71.4286  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 2.0\n","malignant FP: 5.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 1.093095064163208\n","minibatch AVG loss: 0.5055190443992614\n","Epoch: 5     train index of 5 minibatch: 2      time used: 0.878676176071167\n","minibatch AVG loss: 0.34350896179676055\n","Epoch: 5     train index of 5 minibatch: 3      time used: 0.8837592601776123\n","minibatch AVG loss: 0.525288599729538\n","\n","Epoch: 5  train \n","Loss: 0.4714  Acc: 76.8116\n","benign precision: 80.0000  recall: 66.6667\n","benign sensitivity: 66.6667  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 76.7442\n","benign TP: 20.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 10.0\n","malignant precision: 76.7442  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 66.6667\n","malignant FPR: 33.3333  NPV: 80.0000\n","malignant TP: 33.0\n","malignant TN: 20.0\n","malignant FP: 10.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.4805  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 1.09688138961792\n","minibatch AVG loss: 0.35554036498069763\n","Epoch: 6     train index of 5 minibatch: 2      time used: 0.8832640647888184\n","minibatch AVG loss: 0.336121267080307\n","Epoch: 6     train index of 5 minibatch: 3      time used: 0.8839449882507324\n","minibatch AVG loss: 0.22230699956417083\n","\n","Epoch: 6  train \n","Loss: 0.2946  Acc: 89.8551\n","benign precision: 96.1538  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 88.0952\n","benign TP: 25.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 5.0\n","malignant precision: 88.0952  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 96.1538\n","malignant TP: 37.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.4063  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 1.1036198139190674\n","minibatch AVG loss: 0.3235905855894089\n","Epoch: 7     train index of 5 minibatch: 2      time used: 0.885446310043335\n","minibatch AVG loss: 0.24937833547592164\n","Epoch: 7     train index of 5 minibatch: 3      time used: 0.8868381977081299\n","minibatch AVG loss: 0.46514472365379333\n","\n","Epoch: 7  train \n","Loss: 0.3296  Acc: 84.0580\n","benign precision: 88.0000  recall: 75.8621\n","benign sensitivity: 75.8621  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 83.7209\n","benign TP: 22.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 7.0\n","malignant precision: 83.7209  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 75.8621\n","malignant FPR: 24.1379  NPV: 88.0000\n","malignant TP: 36.0\n","malignant TN: 22.0\n","malignant FP: 7.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.3968  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 1.1119577884674072\n","minibatch AVG loss: 0.2552457734942436\n","Epoch: 8     train index of 5 minibatch: 2      time used: 0.8894524574279785\n","minibatch AVG loss: 0.556588976085186\n","Epoch: 8     train index of 5 minibatch: 3      time used: 0.8853058815002441\n","minibatch AVG loss: 0.27595517933368685\n","\n","Epoch: 8  train \n","Loss: 0.3558  Acc: 78.2609\n","benign precision: 80.0000  recall: 68.9655\n","benign sensitivity: 68.9655  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 79.0698\n","benign TP: 20.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 9.0\n","malignant precision: 79.0698  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 68.9655\n","malignant FPR: 31.0345  NPV: 80.0000\n","malignant TP: 34.0\n","malignant TN: 20.0\n","malignant FP: 9.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.4360  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 1.0982139110565186\n","minibatch AVG loss: 0.34384727478027344\n","Epoch: 9     train index of 5 minibatch: 2      time used: 0.8846421241760254\n","minibatch AVG loss: 0.20114645957946778\n","Epoch: 9     train index of 5 minibatch: 3      time used: 0.8858227729797363\n","minibatch AVG loss: 0.2551891103386879\n","\n","Epoch: 9  train \n","Loss: 0.2686  Acc: 88.4058\n","benign precision: 87.0968  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 91.8919\n","benign TP: 27.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 91.8919  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 87.0968\n","malignant TP: 34.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.3737  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 1.0940697193145752\n","minibatch AVG loss: 0.3727891147136688\n","Epoch: 10     train index of 5 minibatch: 2      time used: 0.884476900100708\n","minibatch AVG loss: 0.11442341357469558\n","Epoch: 10     train index of 5 minibatch: 3      time used: 0.8845760822296143\n","minibatch AVG loss: 0.2014850467443466\n","\n","Epoch: 10  train \n","Loss: 0.2228  Acc: 89.8551\n","benign precision: 84.8485  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 97.1429\n","benign TP: 28.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 1.0\n","malignant precision: 97.1429  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 84.8485\n","malignant TP: 34.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.3449  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 1.1006109714508057\n","minibatch AVG loss: 0.21092648655176163\n","Epoch: 11     train index of 5 minibatch: 2      time used: 0.8836634159088135\n","minibatch AVG loss: 0.12820705771446228\n","Epoch: 11     train index of 5 minibatch: 3      time used: 0.8842267990112305\n","minibatch AVG loss: 0.30707254707813264\n","\n","Epoch: 11  train \n","Loss: 0.1965  Acc: 94.2029\n","benign precision: 96.5517  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 94.8718\n","benign TP: 28.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 96.5517\n","malignant TP: 37.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.3188  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 1.0948550701141357\n","minibatch AVG loss: 0.11133447214961052\n","Epoch: 12     train index of 5 minibatch: 2      time used: 0.8828213214874268\n","minibatch AVG loss: 0.42414960637688637\n","Epoch: 12     train index of 5 minibatch: 3      time used: 0.8785116672515869\n","minibatch AVG loss: 0.18589853197336198\n","\n","Epoch: 12  train \n","Loss: 0.2414  Acc: 85.5072\n","benign precision: 92.0000  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 83.7209\n","benign TP: 23.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 7.0\n","malignant precision: 83.7209  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 92.0000\n","malignant TP: 36.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.3862  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 1.0987317562103271\n","minibatch AVG loss: 0.28478692919015886\n","Epoch: 13     train index of 5 minibatch: 2      time used: 0.878333330154419\n","minibatch AVG loss: 0.19910366386175155\n","Epoch: 13     train index of 5 minibatch: 3      time used: 0.8830399513244629\n","minibatch AVG loss: 0.22455388456583023\n","\n","Epoch: 13  train \n","Loss: 0.2156  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.2765  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 1.0882282257080078\n","minibatch AVG loss: 0.6271206110715866\n","Epoch: 14     train index of 5 minibatch: 2      time used: 0.8754382133483887\n","minibatch AVG loss: 0.1685888722538948\n","Epoch: 14     train index of 5 minibatch: 3      time used: 0.8769171237945557\n","minibatch AVG loss: 0.1788745865225792\n","\n","Epoch: 14  train \n","Loss: 0.3033  Acc: 86.9565\n","benign precision: 89.2857  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 87.5000\n","benign TP: 25.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 5.0\n","malignant precision: 87.5000  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 89.2857\n","malignant TP: 35.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.2653  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 1.0905377864837646\n","minibatch AVG loss: 0.2770580291748047\n","Epoch: 15     train index of 5 minibatch: 2      time used: 0.8724322319030762\n","minibatch AVG loss: 0.1421222060918808\n","Epoch: 15     train index of 5 minibatch: 3      time used: 0.8778603076934814\n","minibatch AVG loss: 0.08211047649383545\n","\n","Epoch: 15  train \n","Loss: 0.1701  Acc: 92.7536\n","benign precision: 90.6250  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 97.2222\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 90.6250\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.2147  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 1.0935821533203125\n","minibatch AVG loss: 0.12834111526608466\n","Epoch: 16     train index of 5 minibatch: 2      time used: 0.8764300346374512\n","minibatch AVG loss: 0.39229249507188796\n","Epoch: 16     train index of 5 minibatch: 3      time used: 0.8763523101806641\n","minibatch AVG loss: 0.2608757302165031\n","\n","Epoch: 16  train \n","Loss: 0.2369  Acc: 86.9565\n","benign precision: 84.3750  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 91.6667\n","benign TP: 27.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 3.0\n","malignant precision: 91.6667  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 84.3750\n","malignant TP: 33.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.2512  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 1.0923027992248535\n","minibatch AVG loss: 0.22133377939462662\n","Epoch: 17     train index of 5 minibatch: 2      time used: 0.8761546611785889\n","minibatch AVG loss: 0.1366019070148468\n","Epoch: 17     train index of 5 minibatch: 3      time used: 0.8766217231750488\n","minibatch AVG loss: 0.1276617795228958\n","\n","Epoch: 17  train \n","Loss: 0.1447  Acc: 92.7536\n","benign precision: 90.3226  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 97.2973\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 90.3226\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.2641  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 1.0891802310943604\n","minibatch AVG loss: 0.0763446819037199\n","Epoch: 18     train index of 5 minibatch: 2      time used: 0.8769960403442383\n","minibatch AVG loss: 0.3819126032292843\n","Epoch: 18     train index of 5 minibatch: 3      time used: 0.8746821880340576\n","minibatch AVG loss: 0.26961841285228727\n","\n","Epoch: 18  train \n","Loss: 0.2209  Acc: 88.4058\n","benign precision: 92.5926  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 87.8049\n","benign TP: 25.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 5.0\n","malignant precision: 87.8049  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 92.5926\n","malignant TP: 36.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.2517  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 1.0827856063842773\n","minibatch AVG loss: 0.3101337246596813\n","Epoch: 19     train index of 5 minibatch: 2      time used: 0.8764853477478027\n","minibatch AVG loss: 0.11425382904708385\n","Epoch: 19     train index of 5 minibatch: 3      time used: 0.8755218982696533\n","minibatch AVG loss: 0.09864431731402874\n","\n","Epoch: 19  train \n","Loss: 0.1547  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.0000\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.2252  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 1.0814893245697021\n","minibatch AVG loss: 0.46536985039711\n","Epoch: 20     train index of 5 minibatch: 2      time used: 0.8710377216339111\n","minibatch AVG loss: 0.10449182987213135\n","Epoch: 20     train index of 5 minibatch: 3      time used: 0.8743953704833984\n","minibatch AVG loss: 0.4556363258510828\n","\n","Epoch: 20  train \n","Loss: 0.3251  Acc: 86.9565\n","benign precision: 80.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 96.9697\n","benign TP: 28.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 1.0\n","malignant precision: 96.9697  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 80.0000\n","malignant TP: 32.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.2479  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 1.0851917266845703\n","minibatch AVG loss: 0.0827957957983017\n","Epoch: 21     train index of 5 minibatch: 2      time used: 0.8753597736358643\n","minibatch AVG loss: 0.33249820172786715\n","Epoch: 21     train index of 5 minibatch: 3      time used: 0.8714900016784668\n","minibatch AVG loss: 0.2669688295572996\n","\n","Epoch: 21  train \n","Loss: 0.2555  Acc: 88.4058\n","benign precision: 86.6667  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 92.1053\n","benign TP: 26.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 92.1053  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 86.6667\n","malignant TP: 35.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.2005  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 1.0901336669921875\n","minibatch AVG loss: 0.04009017646312714\n","Epoch: 22     train index of 5 minibatch: 2      time used: 0.8765699863433838\n","minibatch AVG loss: 0.05900672301650047\n","Epoch: 22     train index of 5 minibatch: 3      time used: 0.8739957809448242\n","minibatch AVG loss: 0.1423963725566864\n","\n","Epoch: 22  train \n","Loss: 0.0818  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.4359\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.2358  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 1.0858640670776367\n","minibatch AVG loss: 0.31223557963967324\n","Epoch: 23     train index of 5 minibatch: 2      time used: 0.8757634162902832\n","minibatch AVG loss: 0.07148089595139026\n","Epoch: 23     train index of 5 minibatch: 3      time used: 0.8759682178497314\n","minibatch AVG loss: 0.44358975514769555\n","\n","Epoch: 23  train \n","Loss: 0.3019  Acc: 82.6087\n","benign precision: 88.0000  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 81.3953\n","benign TP: 22.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 8.0\n","malignant precision: 81.3953  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 88.0000\n","malignant TP: 35.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.2741  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 1.0905027389526367\n","minibatch AVG loss: 0.04969851076602936\n","Epoch: 24     train index of 5 minibatch: 2      time used: 0.8740901947021484\n","minibatch AVG loss: 0.03677212670445442\n","Epoch: 24     train index of 5 minibatch: 3      time used: 0.8767645359039307\n","minibatch AVG loss: 0.18987909518182278\n","\n","Epoch: 24  train \n","Loss: 0.1289  Acc: 91.3043\n","benign precision: 92.8571  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 92.5000\n","benign TP: 26.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.5000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 92.8571\n","malignant TP: 37.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.2859  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 1.0877671241760254\n","minibatch AVG loss: 0.2178142812103033\n","Epoch: 25     train index of 5 minibatch: 2      time used: 0.8753330707550049\n","minibatch AVG loss: 0.3523961462080479\n","Epoch: 25     train index of 5 minibatch: 3      time used: 0.8750951290130615\n","minibatch AVG loss: 0.17957524806261063\n","\n","Epoch: 25  train \n","Loss: 0.2536  Acc: 88.4058\n","benign precision: 82.3529  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 97.0588\n","benign TP: 28.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 1.0\n","malignant precision: 97.0588  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 82.3529\n","malignant TP: 33.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.2749  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 1.0884768962860107\n","minibatch AVG loss: 0.36298196725547316\n","Epoch: 26     train index of 5 minibatch: 2      time used: 0.8768844604492188\n","minibatch AVG loss: 0.19704973250627517\n","Epoch: 26     train index of 5 minibatch: 3      time used: 0.8763289451599121\n","minibatch AVG loss: 0.2315538339316845\n","\n","Epoch: 26  train \n","Loss: 0.2365  Acc: 86.9565\n","benign precision: 88.8889  recall: 82.7586\n","benign sensitivity: 82.7586  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 87.8049\n","benign TP: 24.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 5.0\n","malignant precision: 87.8049  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 82.7586\n","malignant FPR: 17.2414  NPV: 88.8889\n","malignant TP: 36.0\n","malignant TN: 24.0\n","malignant FP: 5.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.2888  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 1.0901479721069336\n","minibatch AVG loss: 0.07741968855261802\n","Epoch: 27     train index of 5 minibatch: 2      time used: 0.8751316070556641\n","minibatch AVG loss: 0.31662828102707863\n","Epoch: 27     train index of 5 minibatch: 3      time used: 0.8759710788726807\n","minibatch AVG loss: 0.2970257386565208\n","\n","Epoch: 27  train \n","Loss: 0.2344  Acc: 89.8551\n","benign precision: 84.8485  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 97.1429\n","benign TP: 28.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 1.0\n","malignant precision: 97.1429  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 84.8485\n","malignant TP: 34.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.2685  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 1.0881803035736084\n","minibatch AVG loss: 0.1487347509711981\n","Epoch: 28     train index of 5 minibatch: 2      time used: 0.8759989738464355\n","minibatch AVG loss: 0.24073447808623313\n","Epoch: 28     train index of 5 minibatch: 3      time used: 0.8778805732727051\n","minibatch AVG loss: 0.09071659445762634\n","\n","Epoch: 28  train \n","Loss: 0.2064  Acc: 89.8551\n","benign precision: 92.8571  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 90.0000\n","benign TP: 26.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 4.0\n","malignant precision: 90.0000  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 92.8571\n","malignant TP: 36.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.2461  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 1.0924010276794434\n","minibatch AVG loss: 0.35385976955294607\n","Epoch: 29     train index of 5 minibatch: 2      time used: 0.875744104385376\n","minibatch AVG loss: 0.22920124866068364\n","Epoch: 29     train index of 5 minibatch: 3      time used: 0.878976583480835\n","minibatch AVG loss: 0.046254797279834746\n","\n","Epoch: 29  train \n","Loss: 0.1873  Acc: 91.3043\n","benign precision: 93.1034  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 92.3077\n","benign TP: 27.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.3077  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 93.1034\n","malignant TP: 36.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.2249  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 1.094984531402588\n","minibatch AVG loss: 0.430676206573844\n","Epoch: 30     train index of 5 minibatch: 2      time used: 0.8743240833282471\n","minibatch AVG loss: 0.07889638841152191\n","Epoch: 30     train index of 5 minibatch: 3      time used: 0.8763577938079834\n","minibatch AVG loss: 0.16183065213263034\n","\n","Epoch: 30  train \n","Loss: 0.2037  Acc: 88.4058\n","benign precision: 87.0968  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 91.8919\n","benign TP: 27.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 91.8919  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 87.0968\n","malignant TP: 34.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.2578  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 1.0923736095428467\n","minibatch AVG loss: 0.0552157636731863\n","Epoch: 31     train index of 5 minibatch: 2      time used: 0.8774783611297607\n","minibatch AVG loss: 0.4224556811153889\n","Epoch: 31     train index of 5 minibatch: 3      time used: 0.8799302577972412\n","minibatch AVG loss: 0.18862005304545165\n","\n","Epoch: 31  train \n","Loss: 0.2008  Acc: 88.4058\n","benign precision: 89.6552  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 89.7436\n","benign TP: 26.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 4.0\n","malignant precision: 89.7436  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 89.6552\n","malignant TP: 35.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.2614  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 1.0869524478912354\n","minibatch AVG loss: 0.03496869206428528\n","Epoch: 32     train index of 5 minibatch: 2      time used: 0.8785018920898438\n","minibatch AVG loss: 0.2848056675866246\n","Epoch: 32     train index of 5 minibatch: 3      time used: 0.8795270919799805\n","minibatch AVG loss: 0.08452838771045208\n","\n","Epoch: 32  train \n","Loss: 0.1205  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.2431  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 1.0947017669677734\n","minibatch AVG loss: 0.2764190800487995\n","Epoch: 33     train index of 5 minibatch: 2      time used: 0.8775556087493896\n","minibatch AVG loss: 0.053860075399279596\n","Epoch: 33     train index of 5 minibatch: 3      time used: 0.8796560764312744\n","minibatch AVG loss: 0.17809176146984101\n","\n","Epoch: 33  train \n","Loss: 0.1506  Acc: 92.7536\n","benign precision: 96.2963  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 92.6829\n","benign TP: 26.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 92.6829  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 96.2963\n","malignant TP: 38.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.2212  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 1.0968804359436035\n","minibatch AVG loss: 0.15748582482337953\n","Epoch: 34     train index of 5 minibatch: 2      time used: 0.8778395652770996\n","minibatch AVG loss: 0.03482305072247982\n","Epoch: 34     train index of 5 minibatch: 3      time used: 0.8802480697631836\n","minibatch AVG loss: 0.1803439199924469\n","\n","Epoch: 34  train \n","Loss: 0.1138  Acc: 94.2029\n","benign precision: 100.0000  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 92.6829\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 92.6829  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.2222  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 1.08292818069458\n","minibatch AVG loss: 0.4122148063033819\n","Epoch: 35     train index of 5 minibatch: 2      time used: 0.8850350379943848\n","minibatch AVG loss: 0.2295089468359947\n","Epoch: 35     train index of 5 minibatch: 3      time used: 0.8798744678497314\n","minibatch AVG loss: 0.12131402660161257\n","\n","Epoch: 35  train \n","Loss: 0.2249  Acc: 88.4058\n","benign precision: 82.8571  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 96.9697\n","benign TP: 29.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 1.0\n","malignant precision: 96.9697  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 82.8571\n","malignant TP: 32.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.2366  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 1.091536283493042\n","minibatch AVG loss: 0.04015262164175511\n","Epoch: 36     train index of 5 minibatch: 2      time used: 0.8775675296783447\n","minibatch AVG loss: 0.09442604668438434\n","Epoch: 36     train index of 5 minibatch: 3      time used: 0.8764607906341553\n","minibatch AVG loss: 0.0577699800953269\n","\n","Epoch: 36  train \n","Loss: 0.1101  Acc: 95.6522\n","benign precision: 96.5517  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 97.4359\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 96.5517\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.2202  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 1.0944108963012695\n","minibatch AVG loss: 0.0500975776463747\n","Epoch: 37     train index of 5 minibatch: 2      time used: 0.8764104843139648\n","minibatch AVG loss: 0.19801621437072753\n","Epoch: 37     train index of 5 minibatch: 3      time used: 0.8794350624084473\n","minibatch AVG loss: 0.23969590961933135\n","\n","Epoch: 37  train \n","Loss: 0.2094  Acc: 88.4058\n","benign precision: 89.2857  recall: 86.2069\n","benign sensitivity: 86.2069  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 90.0000\n","benign TP: 25.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 4.0\n","malignant precision: 90.0000  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 86.2069\n","malignant FPR: 13.7931  NPV: 89.2857\n","malignant TP: 36.0\n","malignant TN: 25.0\n","malignant FP: 4.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.2330  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 1.0862486362457275\n","minibatch AVG loss: 0.15344971492886544\n","Epoch: 38     train index of 5 minibatch: 2      time used: 0.8801651000976562\n","minibatch AVG loss: 0.20246433056890964\n","Epoch: 38     train index of 5 minibatch: 3      time used: 0.8801174163818359\n","minibatch AVG loss: 0.14797972068190574\n","\n","Epoch: 38  train \n","Loss: 0.1534  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.2705  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 1.0865812301635742\n","minibatch AVG loss: 0.20878919567912818\n","Epoch: 39     train index of 5 minibatch: 2      time used: 0.876253604888916\n","minibatch AVG loss: 0.09877022504806518\n","Epoch: 39     train index of 5 minibatch: 3      time used: 0.8783414363861084\n","minibatch AVG loss: 0.17977655231952666\n","\n","Epoch: 39  train \n","Loss: 0.1505  Acc: 91.3043\n","benign precision: 96.2963  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 90.2439\n","benign TP: 26.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 90.2439  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 96.2963\n","malignant TP: 37.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.2364  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 1.0876274108886719\n","minibatch AVG loss: 0.2568328559398651\n","Epoch: 40     train index of 5 minibatch: 2      time used: 0.8774998188018799\n","minibatch AVG loss: 0.23068359345197678\n","Epoch: 40     train index of 5 minibatch: 3      time used: 0.8792052268981934\n","minibatch AVG loss: 0.05210461542010307\n","\n","Epoch: 40  train \n","Loss: 0.1606  Acc: 95.6522\n","benign precision: 93.7500  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.7500\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.2350  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 1.0952777862548828\n","minibatch AVG loss: 0.06848166938871145\n","Epoch: 41     train index of 5 minibatch: 2      time used: 0.8779962062835693\n","minibatch AVG loss: 0.03846777603030205\n","Epoch: 41     train index of 5 minibatch: 3      time used: 0.877723217010498\n","minibatch AVG loss: 0.11747292540967465\n","\n","Epoch: 41  train \n","Loss: 0.0711  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.2324  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 1.0839264392852783\n","minibatch AVG loss: 0.2873920498415828\n","Epoch: 42     train index of 5 minibatch: 2      time used: 0.8770921230316162\n","minibatch AVG loss: 0.19712499789893628\n","Epoch: 42     train index of 5 minibatch: 3      time used: 0.8783655166625977\n","minibatch AVG loss: 0.11623756997287274\n","\n","Epoch: 42  train \n","Loss: 0.1957  Acc: 91.3043\n","benign precision: 87.5000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 97.2222\n","benign TP: 28.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 87.5000\n","malignant TP: 35.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.2402  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 1.0771522521972656\n","minibatch AVG loss: 0.09923750013113022\n","Epoch: 43     train index of 5 minibatch: 2      time used: 0.8768017292022705\n","minibatch AVG loss: 0.31359824538230896\n","Epoch: 43     train index of 5 minibatch: 3      time used: 0.8766570091247559\n","minibatch AVG loss: 0.08036812692880631\n","\n","Epoch: 43  train \n","Loss: 0.1541  Acc: 95.6522\n","benign precision: 96.5517  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 97.4359\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 96.5517\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.1671  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 1.0892724990844727\n","minibatch AVG loss: 0.07165120840072632\n","Epoch: 44     train index of 5 minibatch: 2      time used: 0.8771867752075195\n","minibatch AVG loss: 0.054962954670190814\n","Epoch: 44     train index of 5 minibatch: 3      time used: 0.8787319660186768\n","minibatch AVG loss: 0.03598583564162254\n","\n","Epoch: 44  train \n","Loss: 0.1164  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.1220\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.1220  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.2074  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 1.0860540866851807\n","minibatch AVG loss: 0.04239606969058514\n","Epoch: 45     train index of 5 minibatch: 2      time used: 0.8739457130432129\n","minibatch AVG loss: 0.5925477601587772\n","Epoch: 45     train index of 5 minibatch: 3      time used: 0.8767907619476318\n","minibatch AVG loss: 0.34947315100580456\n","\n","Epoch: 45  train \n","Loss: 0.2901  Acc: 85.5072\n","benign precision: 86.2069  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 87.1795\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 87.1795  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 86.2069\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.2139  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 1.0849976539611816\n","minibatch AVG loss: 0.043920989707112314\n","Epoch: 46     train index of 5 minibatch: 2      time used: 0.8754634857177734\n","minibatch AVG loss: 0.08965419419109821\n","Epoch: 46     train index of 5 minibatch: 3      time used: 0.8792581558227539\n","minibatch AVG loss: 0.15962549075484275\n","\n","Epoch: 46  train \n","Loss: 0.0921  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.2263  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 1.1676840782165527\n","minibatch AVG loss: 0.21322389636188746\n","Epoch: 47     train index of 5 minibatch: 2      time used: 0.8752260208129883\n","minibatch AVG loss: 0.36663124710321426\n","Epoch: 47     train index of 5 minibatch: 3      time used: 0.877617359161377\n","minibatch AVG loss: 0.2908417284488678\n","\n","Epoch: 47  train \n","Loss: 0.2590  Acc: 85.5072\n","benign precision: 86.2069  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 87.1795\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 87.1795  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 86.2069\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.2334  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 1.1713416576385498\n","minibatch AVG loss: 0.2593271277844906\n","Epoch: 48     train index of 5 minibatch: 2      time used: 0.8767530918121338\n","minibatch AVG loss: 0.19042746275663375\n","Epoch: 48     train index of 5 minibatch: 3      time used: 0.878258466720581\n","minibatch AVG loss: 0.0825840812176466\n","\n","Epoch: 48  train \n","Loss: 0.1645  Acc: 92.7536\n","benign precision: 90.3226  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 97.2973\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 90.3226\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.2089  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 1.1650631427764893\n","minibatch AVG loss: 0.04057330060750246\n","Epoch: 49     train index of 5 minibatch: 2      time used: 0.8741135597229004\n","minibatch AVG loss: 0.054276100173592565\n","Epoch: 49     train index of 5 minibatch: 3      time used: 0.8755078315734863\n","minibatch AVG loss: 0.06617454960942268\n","\n","Epoch: 49  train \n","Loss: 0.0505  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.2045  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 1.0904409885406494\n","minibatch AVG loss: 0.5151585325598717\n","Epoch: 50     train index of 5 minibatch: 2      time used: 0.8797361850738525\n","minibatch AVG loss: 0.14546032547950744\n","Epoch: 50     train index of 5 minibatch: 3      time used: 0.8758022785186768\n","minibatch AVG loss: 0.40967104732990267\n","\n","Epoch: 50  train \n","Loss: 0.3132  Acc: 85.5072\n","benign precision: 86.2069  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 87.1795\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 87.1795  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 86.2069\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.2249  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 2m 59s\n","Best epoch idx:  49\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 93.750000\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ResNet50_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx ResNet50_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ruKV2IJrSKij","executionInfo":{"status":"ok","timestamp":1651155920416,"user_tz":-480,"elapsed":775573,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"6250050f-3a00-4cc6-fdf3-2ca954bd470b"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224']\n","Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_resnet50_384-9fd3c705.pth\" to /root/.cache/torch/hub/checkpoints/jx_vit_base_resnet50_384-9fd3c705.pth\n","test model output： tensor([[-0.5185,  0.0907]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","     StdConv2dSame-1         [-1, 64, 192, 192]           9,408\n","              ReLU-2         [-1, 64, 192, 192]               0\n","      GroupNormAct-3         [-1, 64, 192, 192]             128\n","     MaxPool2dSame-4           [-1, 64, 96, 96]               0\n","     StdConv2dSame-5          [-1, 256, 96, 96]          16,384\n","          Identity-6          [-1, 256, 96, 96]               0\n","      GroupNormAct-7          [-1, 256, 96, 96]             512\n","    DownsampleConv-8          [-1, 256, 96, 96]               0\n","     StdConv2dSame-9           [-1, 64, 96, 96]           4,096\n","             ReLU-10           [-1, 64, 96, 96]               0\n","     GroupNormAct-11           [-1, 64, 96, 96]             128\n","    StdConv2dSame-12           [-1, 64, 96, 96]          36,864\n","             ReLU-13           [-1, 64, 96, 96]               0\n","     GroupNormAct-14           [-1, 64, 96, 96]             128\n","    StdConv2dSame-15          [-1, 256, 96, 96]          16,384\n","         Identity-16          [-1, 256, 96, 96]               0\n","     GroupNormAct-17          [-1, 256, 96, 96]             512\n","         Identity-18          [-1, 256, 96, 96]               0\n","             ReLU-19          [-1, 256, 96, 96]               0\n","       Bottleneck-20          [-1, 256, 96, 96]               0\n","    StdConv2dSame-21           [-1, 64, 96, 96]          16,384\n","             ReLU-22           [-1, 64, 96, 96]               0\n","     GroupNormAct-23           [-1, 64, 96, 96]             128\n","    StdConv2dSame-24           [-1, 64, 96, 96]          36,864\n","             ReLU-25           [-1, 64, 96, 96]               0\n","     GroupNormAct-26           [-1, 64, 96, 96]             128\n","    StdConv2dSame-27          [-1, 256, 96, 96]          16,384\n","         Identity-28          [-1, 256, 96, 96]               0\n","     GroupNormAct-29          [-1, 256, 96, 96]             512\n","         Identity-30          [-1, 256, 96, 96]               0\n","             ReLU-31          [-1, 256, 96, 96]               0\n","       Bottleneck-32          [-1, 256, 96, 96]               0\n","    StdConv2dSame-33           [-1, 64, 96, 96]          16,384\n","             ReLU-34           [-1, 64, 96, 96]               0\n","     GroupNormAct-35           [-1, 64, 96, 96]             128\n","    StdConv2dSame-36           [-1, 64, 96, 96]          36,864\n","             ReLU-37           [-1, 64, 96, 96]               0\n","     GroupNormAct-38           [-1, 64, 96, 96]             128\n","    StdConv2dSame-39          [-1, 256, 96, 96]          16,384\n","         Identity-40          [-1, 256, 96, 96]               0\n","     GroupNormAct-41          [-1, 256, 96, 96]             512\n","         Identity-42          [-1, 256, 96, 96]               0\n","             ReLU-43          [-1, 256, 96, 96]               0\n","       Bottleneck-44          [-1, 256, 96, 96]               0\n","      ResNetStage-45          [-1, 256, 96, 96]               0\n","    StdConv2dSame-46          [-1, 512, 48, 48]         131,072\n","         Identity-47          [-1, 512, 48, 48]               0\n","     GroupNormAct-48          [-1, 512, 48, 48]           1,024\n","   DownsampleConv-49          [-1, 512, 48, 48]               0\n","    StdConv2dSame-50          [-1, 128, 96, 96]          32,768\n","             ReLU-51          [-1, 128, 96, 96]               0\n","     GroupNormAct-52          [-1, 128, 96, 96]             256\n","    StdConv2dSame-53          [-1, 128, 48, 48]         147,456\n","             ReLU-54          [-1, 128, 48, 48]               0\n","     GroupNormAct-55          [-1, 128, 48, 48]             256\n","    StdConv2dSame-56          [-1, 512, 48, 48]          65,536\n","         Identity-57          [-1, 512, 48, 48]               0\n","     GroupNormAct-58          [-1, 512, 48, 48]           1,024\n","         Identity-59          [-1, 512, 48, 48]               0\n","             ReLU-60          [-1, 512, 48, 48]               0\n","       Bottleneck-61          [-1, 512, 48, 48]               0\n","    StdConv2dSame-62          [-1, 128, 48, 48]          65,536\n","             ReLU-63          [-1, 128, 48, 48]               0\n","     GroupNormAct-64          [-1, 128, 48, 48]             256\n","    StdConv2dSame-65          [-1, 128, 48, 48]         147,456\n","             ReLU-66          [-1, 128, 48, 48]               0\n","     GroupNormAct-67          [-1, 128, 48, 48]             256\n","    StdConv2dSame-68          [-1, 512, 48, 48]          65,536\n","         Identity-69          [-1, 512, 48, 48]               0\n","     GroupNormAct-70          [-1, 512, 48, 48]           1,024\n","         Identity-71          [-1, 512, 48, 48]               0\n","             ReLU-72          [-1, 512, 48, 48]               0\n","       Bottleneck-73          [-1, 512, 48, 48]               0\n","    StdConv2dSame-74          [-1, 128, 48, 48]          65,536\n","             ReLU-75          [-1, 128, 48, 48]               0\n","     GroupNormAct-76          [-1, 128, 48, 48]             256\n","    StdConv2dSame-77          [-1, 128, 48, 48]         147,456\n","             ReLU-78          [-1, 128, 48, 48]               0\n","     GroupNormAct-79          [-1, 128, 48, 48]             256\n","    StdConv2dSame-80          [-1, 512, 48, 48]          65,536\n","         Identity-81          [-1, 512, 48, 48]               0\n","     GroupNormAct-82          [-1, 512, 48, 48]           1,024\n","         Identity-83          [-1, 512, 48, 48]               0\n","             ReLU-84          [-1, 512, 48, 48]               0\n","       Bottleneck-85          [-1, 512, 48, 48]               0\n","    StdConv2dSame-86          [-1, 128, 48, 48]          65,536\n","             ReLU-87          [-1, 128, 48, 48]               0\n","     GroupNormAct-88          [-1, 128, 48, 48]             256\n","    StdConv2dSame-89          [-1, 128, 48, 48]         147,456\n","             ReLU-90          [-1, 128, 48, 48]               0\n","     GroupNormAct-91          [-1, 128, 48, 48]             256\n","    StdConv2dSame-92          [-1, 512, 48, 48]          65,536\n","         Identity-93          [-1, 512, 48, 48]               0\n","     GroupNormAct-94          [-1, 512, 48, 48]           1,024\n","         Identity-95          [-1, 512, 48, 48]               0\n","             ReLU-96          [-1, 512, 48, 48]               0\n","       Bottleneck-97          [-1, 512, 48, 48]               0\n","      ResNetStage-98          [-1, 512, 48, 48]               0\n","    StdConv2dSame-99         [-1, 1024, 24, 24]         524,288\n","        Identity-100         [-1, 1024, 24, 24]               0\n","    GroupNormAct-101         [-1, 1024, 24, 24]           2,048\n","  DownsampleConv-102         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-103          [-1, 256, 48, 48]         131,072\n","            ReLU-104          [-1, 256, 48, 48]               0\n","    GroupNormAct-105          [-1, 256, 48, 48]             512\n","   StdConv2dSame-106          [-1, 256, 24, 24]         589,824\n","            ReLU-107          [-1, 256, 24, 24]               0\n","    GroupNormAct-108          [-1, 256, 24, 24]             512\n","   StdConv2dSame-109         [-1, 1024, 24, 24]         262,144\n","        Identity-110         [-1, 1024, 24, 24]               0\n","    GroupNormAct-111         [-1, 1024, 24, 24]           2,048\n","        Identity-112         [-1, 1024, 24, 24]               0\n","            ReLU-113         [-1, 1024, 24, 24]               0\n","      Bottleneck-114         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-115          [-1, 256, 24, 24]         262,144\n","            ReLU-116          [-1, 256, 24, 24]               0\n","    GroupNormAct-117          [-1, 256, 24, 24]             512\n","   StdConv2dSame-118          [-1, 256, 24, 24]         589,824\n","            ReLU-119          [-1, 256, 24, 24]               0\n","    GroupNormAct-120          [-1, 256, 24, 24]             512\n","   StdConv2dSame-121         [-1, 1024, 24, 24]         262,144\n","        Identity-122         [-1, 1024, 24, 24]               0\n","    GroupNormAct-123         [-1, 1024, 24, 24]           2,048\n","        Identity-124         [-1, 1024, 24, 24]               0\n","            ReLU-125         [-1, 1024, 24, 24]               0\n","      Bottleneck-126         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-127          [-1, 256, 24, 24]         262,144\n","            ReLU-128          [-1, 256, 24, 24]               0\n","    GroupNormAct-129          [-1, 256, 24, 24]             512\n","   StdConv2dSame-130          [-1, 256, 24, 24]         589,824\n","            ReLU-131          [-1, 256, 24, 24]               0\n","    GroupNormAct-132          [-1, 256, 24, 24]             512\n","   StdConv2dSame-133         [-1, 1024, 24, 24]         262,144\n","        Identity-134         [-1, 1024, 24, 24]               0\n","    GroupNormAct-135         [-1, 1024, 24, 24]           2,048\n","        Identity-136         [-1, 1024, 24, 24]               0\n","            ReLU-137         [-1, 1024, 24, 24]               0\n","      Bottleneck-138         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-139          [-1, 256, 24, 24]         262,144\n","            ReLU-140          [-1, 256, 24, 24]               0\n","    GroupNormAct-141          [-1, 256, 24, 24]             512\n","   StdConv2dSame-142          [-1, 256, 24, 24]         589,824\n","            ReLU-143          [-1, 256, 24, 24]               0\n","    GroupNormAct-144          [-1, 256, 24, 24]             512\n","   StdConv2dSame-145         [-1, 1024, 24, 24]         262,144\n","        Identity-146         [-1, 1024, 24, 24]               0\n","    GroupNormAct-147         [-1, 1024, 24, 24]           2,048\n","        Identity-148         [-1, 1024, 24, 24]               0\n","            ReLU-149         [-1, 1024, 24, 24]               0\n","      Bottleneck-150         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-151          [-1, 256, 24, 24]         262,144\n","            ReLU-152          [-1, 256, 24, 24]               0\n","    GroupNormAct-153          [-1, 256, 24, 24]             512\n","   StdConv2dSame-154          [-1, 256, 24, 24]         589,824\n","            ReLU-155          [-1, 256, 24, 24]               0\n","    GroupNormAct-156          [-1, 256, 24, 24]             512\n","   StdConv2dSame-157         [-1, 1024, 24, 24]         262,144\n","        Identity-158         [-1, 1024, 24, 24]               0\n","    GroupNormAct-159         [-1, 1024, 24, 24]           2,048\n","        Identity-160         [-1, 1024, 24, 24]               0\n","            ReLU-161         [-1, 1024, 24, 24]               0\n","      Bottleneck-162         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-163          [-1, 256, 24, 24]         262,144\n","            ReLU-164          [-1, 256, 24, 24]               0\n","    GroupNormAct-165          [-1, 256, 24, 24]             512\n","   StdConv2dSame-166          [-1, 256, 24, 24]         589,824\n","            ReLU-167          [-1, 256, 24, 24]               0\n","    GroupNormAct-168          [-1, 256, 24, 24]             512\n","   StdConv2dSame-169         [-1, 1024, 24, 24]         262,144\n","        Identity-170         [-1, 1024, 24, 24]               0\n","    GroupNormAct-171         [-1, 1024, 24, 24]           2,048\n","        Identity-172         [-1, 1024, 24, 24]               0\n","            ReLU-173         [-1, 1024, 24, 24]               0\n","      Bottleneck-174         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-175          [-1, 256, 24, 24]         262,144\n","            ReLU-176          [-1, 256, 24, 24]               0\n","    GroupNormAct-177          [-1, 256, 24, 24]             512\n","   StdConv2dSame-178          [-1, 256, 24, 24]         589,824\n","            ReLU-179          [-1, 256, 24, 24]               0\n","    GroupNormAct-180          [-1, 256, 24, 24]             512\n","   StdConv2dSame-181         [-1, 1024, 24, 24]         262,144\n","        Identity-182         [-1, 1024, 24, 24]               0\n","    GroupNormAct-183         [-1, 1024, 24, 24]           2,048\n","        Identity-184         [-1, 1024, 24, 24]               0\n","            ReLU-185         [-1, 1024, 24, 24]               0\n","      Bottleneck-186         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-187          [-1, 256, 24, 24]         262,144\n","            ReLU-188          [-1, 256, 24, 24]               0\n","    GroupNormAct-189          [-1, 256, 24, 24]             512\n","   StdConv2dSame-190          [-1, 256, 24, 24]         589,824\n","            ReLU-191          [-1, 256, 24, 24]               0\n","    GroupNormAct-192          [-1, 256, 24, 24]             512\n","   StdConv2dSame-193         [-1, 1024, 24, 24]         262,144\n","        Identity-194         [-1, 1024, 24, 24]               0\n","    GroupNormAct-195         [-1, 1024, 24, 24]           2,048\n","        Identity-196         [-1, 1024, 24, 24]               0\n","            ReLU-197         [-1, 1024, 24, 24]               0\n","      Bottleneck-198         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-199          [-1, 256, 24, 24]         262,144\n","            ReLU-200          [-1, 256, 24, 24]               0\n","    GroupNormAct-201          [-1, 256, 24, 24]             512\n","   StdConv2dSame-202          [-1, 256, 24, 24]         589,824\n","            ReLU-203          [-1, 256, 24, 24]               0\n","    GroupNormAct-204          [-1, 256, 24, 24]             512\n","   StdConv2dSame-205         [-1, 1024, 24, 24]         262,144\n","        Identity-206         [-1, 1024, 24, 24]               0\n","    GroupNormAct-207         [-1, 1024, 24, 24]           2,048\n","        Identity-208         [-1, 1024, 24, 24]               0\n","            ReLU-209         [-1, 1024, 24, 24]               0\n","      Bottleneck-210         [-1, 1024, 24, 24]               0\n","     ResNetStage-211         [-1, 1024, 24, 24]               0\n","        Identity-212         [-1, 1024, 24, 24]               0\n","        Identity-213         [-1, 1024, 24, 24]               0\n","        Identity-214         [-1, 1024, 24, 24]               0\n","SelectAdaptivePool2d-215         [-1, 1024, 24, 24]               0\n","        Identity-216         [-1, 1024, 24, 24]               0\n","        Identity-217         [-1, 1024, 24, 24]               0\n","  ClassifierHead-218         [-1, 1024, 24, 24]               0\n","        ResNetV2-219         [-1, 1024, 24, 24]               0\n","          Conv2d-220          [-1, 768, 24, 24]         787,200\n","     HybridEmbed-221             [-1, 576, 768]               0\n","         Dropout-222             [-1, 577, 768]               0\n","       LayerNorm-223             [-1, 577, 768]           1,536\n","          Linear-224            [-1, 577, 2304]       1,771,776\n","         Dropout-225         [-1, 12, 577, 577]               0\n","          Linear-226             [-1, 577, 768]         590,592\n","         Dropout-227             [-1, 577, 768]               0\n","       Attention-228             [-1, 577, 768]               0\n","        Identity-229             [-1, 577, 768]               0\n","       LayerNorm-230             [-1, 577, 768]           1,536\n","          Linear-231            [-1, 577, 3072]       2,362,368\n","            GELU-232            [-1, 577, 3072]               0\n","         Dropout-233            [-1, 577, 3072]               0\n","          Linear-234             [-1, 577, 768]       2,360,064\n","         Dropout-235             [-1, 577, 768]               0\n","             Mlp-236             [-1, 577, 768]               0\n","        Identity-237             [-1, 577, 768]               0\n","           Block-238             [-1, 577, 768]               0\n","       LayerNorm-239             [-1, 577, 768]           1,536\n","          Linear-240            [-1, 577, 2304]       1,771,776\n","         Dropout-241         [-1, 12, 577, 577]               0\n","          Linear-242             [-1, 577, 768]         590,592\n","         Dropout-243             [-1, 577, 768]               0\n","       Attention-244             [-1, 577, 768]               0\n","        Identity-245             [-1, 577, 768]               0\n","       LayerNorm-246             [-1, 577, 768]           1,536\n","          Linear-247            [-1, 577, 3072]       2,362,368\n","            GELU-248            [-1, 577, 3072]               0\n","         Dropout-249            [-1, 577, 3072]               0\n","          Linear-250             [-1, 577, 768]       2,360,064\n","         Dropout-251             [-1, 577, 768]               0\n","             Mlp-252             [-1, 577, 768]               0\n","        Identity-253             [-1, 577, 768]               0\n","           Block-254             [-1, 577, 768]               0\n","       LayerNorm-255             [-1, 577, 768]           1,536\n","          Linear-256            [-1, 577, 2304]       1,771,776\n","         Dropout-257         [-1, 12, 577, 577]               0\n","          Linear-258             [-1, 577, 768]         590,592\n","         Dropout-259             [-1, 577, 768]               0\n","       Attention-260             [-1, 577, 768]               0\n","        Identity-261             [-1, 577, 768]               0\n","       LayerNorm-262             [-1, 577, 768]           1,536\n","          Linear-263            [-1, 577, 3072]       2,362,368\n","            GELU-264            [-1, 577, 3072]               0\n","         Dropout-265            [-1, 577, 3072]               0\n","          Linear-266             [-1, 577, 768]       2,360,064\n","         Dropout-267             [-1, 577, 768]               0\n","             Mlp-268             [-1, 577, 768]               0\n","        Identity-269             [-1, 577, 768]               0\n","           Block-270             [-1, 577, 768]               0\n","       LayerNorm-271             [-1, 577, 768]           1,536\n","          Linear-272            [-1, 577, 2304]       1,771,776\n","         Dropout-273         [-1, 12, 577, 577]               0\n","          Linear-274             [-1, 577, 768]         590,592\n","         Dropout-275             [-1, 577, 768]               0\n","       Attention-276             [-1, 577, 768]               0\n","        Identity-277             [-1, 577, 768]               0\n","       LayerNorm-278             [-1, 577, 768]           1,536\n","          Linear-279            [-1, 577, 3072]       2,362,368\n","            GELU-280            [-1, 577, 3072]               0\n","         Dropout-281            [-1, 577, 3072]               0\n","          Linear-282             [-1, 577, 768]       2,360,064\n","         Dropout-283             [-1, 577, 768]               0\n","             Mlp-284             [-1, 577, 768]               0\n","        Identity-285             [-1, 577, 768]               0\n","           Block-286             [-1, 577, 768]               0\n","       LayerNorm-287             [-1, 577, 768]           1,536\n","          Linear-288            [-1, 577, 2304]       1,771,776\n","         Dropout-289         [-1, 12, 577, 577]               0\n","          Linear-290             [-1, 577, 768]         590,592\n","         Dropout-291             [-1, 577, 768]               0\n","       Attention-292             [-1, 577, 768]               0\n","        Identity-293             [-1, 577, 768]               0\n","       LayerNorm-294             [-1, 577, 768]           1,536\n","          Linear-295            [-1, 577, 3072]       2,362,368\n","            GELU-296            [-1, 577, 3072]               0\n","         Dropout-297            [-1, 577, 3072]               0\n","          Linear-298             [-1, 577, 768]       2,360,064\n","         Dropout-299             [-1, 577, 768]               0\n","             Mlp-300             [-1, 577, 768]               0\n","        Identity-301             [-1, 577, 768]               0\n","           Block-302             [-1, 577, 768]               0\n","       LayerNorm-303             [-1, 577, 768]           1,536\n","          Linear-304            [-1, 577, 2304]       1,771,776\n","         Dropout-305         [-1, 12, 577, 577]               0\n","          Linear-306             [-1, 577, 768]         590,592\n","         Dropout-307             [-1, 577, 768]               0\n","       Attention-308             [-1, 577, 768]               0\n","        Identity-309             [-1, 577, 768]               0\n","       LayerNorm-310             [-1, 577, 768]           1,536\n","          Linear-311            [-1, 577, 3072]       2,362,368\n","            GELU-312            [-1, 577, 3072]               0\n","         Dropout-313            [-1, 577, 3072]               0\n","          Linear-314             [-1, 577, 768]       2,360,064\n","         Dropout-315             [-1, 577, 768]               0\n","             Mlp-316             [-1, 577, 768]               0\n","        Identity-317             [-1, 577, 768]               0\n","           Block-318             [-1, 577, 768]               0\n","       LayerNorm-319             [-1, 577, 768]           1,536\n","          Linear-320            [-1, 577, 2304]       1,771,776\n","         Dropout-321         [-1, 12, 577, 577]               0\n","          Linear-322             [-1, 577, 768]         590,592\n","         Dropout-323             [-1, 577, 768]               0\n","       Attention-324             [-1, 577, 768]               0\n","        Identity-325             [-1, 577, 768]               0\n","       LayerNorm-326             [-1, 577, 768]           1,536\n","          Linear-327            [-1, 577, 3072]       2,362,368\n","            GELU-328            [-1, 577, 3072]               0\n","         Dropout-329            [-1, 577, 3072]               0\n","          Linear-330             [-1, 577, 768]       2,360,064\n","         Dropout-331             [-1, 577, 768]               0\n","             Mlp-332             [-1, 577, 768]               0\n","        Identity-333             [-1, 577, 768]               0\n","           Block-334             [-1, 577, 768]               0\n","       LayerNorm-335             [-1, 577, 768]           1,536\n","          Linear-336            [-1, 577, 2304]       1,771,776\n","         Dropout-337         [-1, 12, 577, 577]               0\n","          Linear-338             [-1, 577, 768]         590,592\n","         Dropout-339             [-1, 577, 768]               0\n","       Attention-340             [-1, 577, 768]               0\n","        Identity-341             [-1, 577, 768]               0\n","       LayerNorm-342             [-1, 577, 768]           1,536\n","          Linear-343            [-1, 577, 3072]       2,362,368\n","            GELU-344            [-1, 577, 3072]               0\n","         Dropout-345            [-1, 577, 3072]               0\n","          Linear-346             [-1, 577, 768]       2,360,064\n","         Dropout-347             [-1, 577, 768]               0\n","             Mlp-348             [-1, 577, 768]               0\n","        Identity-349             [-1, 577, 768]               0\n","           Block-350             [-1, 577, 768]               0\n","       LayerNorm-351             [-1, 577, 768]           1,536\n","          Linear-352            [-1, 577, 2304]       1,771,776\n","         Dropout-353         [-1, 12, 577, 577]               0\n","          Linear-354             [-1, 577, 768]         590,592\n","         Dropout-355             [-1, 577, 768]               0\n","       Attention-356             [-1, 577, 768]               0\n","        Identity-357             [-1, 577, 768]               0\n","       LayerNorm-358             [-1, 577, 768]           1,536\n","          Linear-359            [-1, 577, 3072]       2,362,368\n","            GELU-360            [-1, 577, 3072]               0\n","         Dropout-361            [-1, 577, 3072]               0\n","          Linear-362             [-1, 577, 768]       2,360,064\n","         Dropout-363             [-1, 577, 768]               0\n","             Mlp-364             [-1, 577, 768]               0\n","        Identity-365             [-1, 577, 768]               0\n","           Block-366             [-1, 577, 768]               0\n","       LayerNorm-367             [-1, 577, 768]           1,536\n","          Linear-368            [-1, 577, 2304]       1,771,776\n","         Dropout-369         [-1, 12, 577, 577]               0\n","          Linear-370             [-1, 577, 768]         590,592\n","         Dropout-371             [-1, 577, 768]               0\n","       Attention-372             [-1, 577, 768]               0\n","        Identity-373             [-1, 577, 768]               0\n","       LayerNorm-374             [-1, 577, 768]           1,536\n","          Linear-375            [-1, 577, 3072]       2,362,368\n","            GELU-376            [-1, 577, 3072]               0\n","         Dropout-377            [-1, 577, 3072]               0\n","          Linear-378             [-1, 577, 768]       2,360,064\n","         Dropout-379             [-1, 577, 768]               0\n","             Mlp-380             [-1, 577, 768]               0\n","        Identity-381             [-1, 577, 768]               0\n","           Block-382             [-1, 577, 768]               0\n","       LayerNorm-383             [-1, 577, 768]           1,536\n","          Linear-384            [-1, 577, 2304]       1,771,776\n","         Dropout-385         [-1, 12, 577, 577]               0\n","          Linear-386             [-1, 577, 768]         590,592\n","         Dropout-387             [-1, 577, 768]               0\n","       Attention-388             [-1, 577, 768]               0\n","        Identity-389             [-1, 577, 768]               0\n","       LayerNorm-390             [-1, 577, 768]           1,536\n","          Linear-391            [-1, 577, 3072]       2,362,368\n","            GELU-392            [-1, 577, 3072]               0\n","         Dropout-393            [-1, 577, 3072]               0\n","          Linear-394             [-1, 577, 768]       2,360,064\n","         Dropout-395             [-1, 577, 768]               0\n","             Mlp-396             [-1, 577, 768]               0\n","        Identity-397             [-1, 577, 768]               0\n","           Block-398             [-1, 577, 768]               0\n","       LayerNorm-399             [-1, 577, 768]           1,536\n","          Linear-400            [-1, 577, 2304]       1,771,776\n","         Dropout-401         [-1, 12, 577, 577]               0\n","          Linear-402             [-1, 577, 768]         590,592\n","         Dropout-403             [-1, 577, 768]               0\n","       Attention-404             [-1, 577, 768]               0\n","        Identity-405             [-1, 577, 768]               0\n","       LayerNorm-406             [-1, 577, 768]           1,536\n","          Linear-407            [-1, 577, 3072]       2,362,368\n","            GELU-408            [-1, 577, 3072]               0\n","         Dropout-409            [-1, 577, 3072]               0\n","          Linear-410             [-1, 577, 768]       2,360,064\n","         Dropout-411             [-1, 577, 768]               0\n","             Mlp-412             [-1, 577, 768]               0\n","        Identity-413             [-1, 577, 768]               0\n","           Block-414             [-1, 577, 768]               0\n","       LayerNorm-415             [-1, 577, 768]           1,536\n","        Identity-416                  [-1, 768]               0\n","          Linear-417                    [-1, 2]           1,538\n","================================================================\n","Total params: 97,739,586\n","Trainable params: 97,739,586\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 2695.24\n","Params size (MB): 372.85\n","Estimated Total Size (MB): 3069.77\n","----------------------------------------------------------------\n","model : ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 3.450443983078003\n","minibatch AVG loss: 0.5796204328536987\n","Epoch: 1     train index of 5 minibatch: 2      time used: 3.2967076301574707\n","minibatch AVG loss: 0.19583420902490617\n","Epoch: 1     train index of 5 minibatch: 3      time used: 3.337553024291992\n","minibatch AVG loss: 0.45129478015005586\n","\n","Epoch: 1  train \n","Loss: 0.4231  Acc: 79.7101\n","benign precision: 78.5714  recall: 75.8621\n","benign sensitivity: 75.8621  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 82.5000\n","benign TP: 22.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 7.0\n","malignant precision: 82.5000  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 75.8621\n","malignant FPR: 24.1379  NPV: 78.5714\n","malignant TP: 33.0\n","malignant TN: 22.0\n","malignant FP: 7.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.1493  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 3.5569064617156982\n","minibatch AVG loss: 0.15675554387271404\n","Epoch: 2     train index of 5 minibatch: 2      time used: 3.3908448219299316\n","minibatch AVG loss: 0.11544114872813224\n","Epoch: 2     train index of 5 minibatch: 3      time used: 3.4242968559265137\n","minibatch AVG loss: 0.0641151338815689\n","\n","Epoch: 2  train \n","Loss: 0.1023  Acc: 95.6522\n","benign precision: 93.7500  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.7500\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.1283  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 3.712024450302124\n","minibatch AVG loss: 0.04651650115847587\n","Epoch: 3     train index of 5 minibatch: 2      time used: 3.5391573905944824\n","minibatch AVG loss: 0.03649363871663809\n","Epoch: 3     train index of 5 minibatch: 3      time used: 3.5857865810394287\n","minibatch AVG loss: 0.06646495899185538\n","\n","Epoch: 3  train \n","Loss: 0.0456  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.0670  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 3.855499744415283\n","minibatch AVG loss: 0.03001983780413866\n","Epoch: 4     train index of 5 minibatch: 2      time used: 3.6936285495758057\n","minibatch AVG loss: 0.01118871634826064\n","Epoch: 4     train index of 5 minibatch: 3      time used: 3.718334436416626\n","minibatch AVG loss: 0.04056049510836601\n","\n","Epoch: 4  train \n","Loss: 0.0254  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.0529  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 3.898207187652588\n","minibatch AVG loss: 0.011322924587875604\n","Epoch: 5     train index of 5 minibatch: 2      time used: 3.650219202041626\n","minibatch AVG loss: 0.009482457209378481\n","Epoch: 5     train index of 5 minibatch: 3      time used: 3.623918294906616\n","minibatch AVG loss: 0.005727259069681167\n","\n","Epoch: 5  train \n","Loss: 0.0107  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.0384  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 3.79082989692688\n","minibatch AVG loss: 0.008244012435898184\n","Epoch: 6     train index of 5 minibatch: 2      time used: 3.5445215702056885\n","minibatch AVG loss: 0.011270407401025296\n","Epoch: 6     train index of 5 minibatch: 3      time used: 3.535537004470825\n","minibatch AVG loss: 0.007157555548474193\n","\n","Epoch: 6  train \n","Loss: 0.0083  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.0349  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 3.7148866653442383\n","minibatch AVG loss: 0.010111052868887782\n","Epoch: 7     train index of 5 minibatch: 2      time used: 3.482377052307129\n","minibatch AVG loss: 0.006286203861236572\n","Epoch: 7     train index of 5 minibatch: 3      time used: 3.478766441345215\n","minibatch AVG loss: 0.0027546761790290474\n","\n","Epoch: 7  train \n","Loss: 0.0066  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.0704  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 3.6710290908813477\n","minibatch AVG loss: 0.0026923148427158592\n","Epoch: 8     train index of 5 minibatch: 2      time used: 3.4810400009155273\n","minibatch AVG loss: 0.01025581604335457\n","Epoch: 8     train index of 5 minibatch: 3      time used: 3.483961582183838\n","minibatch AVG loss: 0.003620717185549438\n","\n","Epoch: 8  train \n","Loss: 0.0050  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.0425  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 3.691617488861084\n","minibatch AVG loss: 0.00440159123390913\n","Epoch: 9     train index of 5 minibatch: 2      time used: 3.504728078842163\n","minibatch AVG loss: 0.007013003434985876\n","Epoch: 9     train index of 5 minibatch: 3      time used: 3.5328421592712402\n","minibatch AVG loss: 0.002438347530551255\n","\n","Epoch: 9  train \n","Loss: 0.0044  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.0412  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 3.7677998542785645\n","minibatch AVG loss: 0.0026213279692456127\n","Epoch: 10     train index of 5 minibatch: 2      time used: 3.5516693592071533\n","minibatch AVG loss: 0.006122990744188428\n","Epoch: 10     train index of 5 minibatch: 3      time used: 3.5634663105010986\n","minibatch AVG loss: 0.002507027331739664\n","\n","Epoch: 10  train \n","Loss: 0.0038  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.0493  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 3.7728757858276367\n","minibatch AVG loss: 0.0072174654342234135\n","Epoch: 11     train index of 5 minibatch: 2      time used: 3.5693118572235107\n","minibatch AVG loss: 0.003977596131153405\n","Epoch: 11     train index of 5 minibatch: 3      time used: 3.570565938949585\n","minibatch AVG loss: 0.003302162699401379\n","\n","Epoch: 11  train \n","Loss: 0.0047  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.0379  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 3.774360418319702\n","minibatch AVG loss: 0.0021856608334928753\n","Epoch: 12     train index of 5 minibatch: 2      time used: 3.566589117050171\n","minibatch AVG loss: 0.004401089111343026\n","Epoch: 12     train index of 5 minibatch: 3      time used: 3.55717396736145\n","minibatch AVG loss: 0.01917074965313077\n","\n","Epoch: 12  train \n","Loss: 0.0079  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.0865  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 3.760857343673706\n","minibatch AVG loss: 0.017150971665978433\n","Epoch: 13     train index of 5 minibatch: 2      time used: 3.5448648929595947\n","minibatch AVG loss: 0.0025698025012388824\n","Epoch: 13     train index of 5 minibatch: 3      time used: 3.537569761276245\n","minibatch AVG loss: 0.0030362886609509587\n","\n","Epoch: 13  train \n","Loss: 0.0069  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.1329  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 3.738521099090576\n","minibatch AVG loss: 0.0017043545027263462\n","Epoch: 14     train index of 5 minibatch: 2      time used: 3.530954599380493\n","minibatch AVG loss: 0.023832843312993644\n","Epoch: 14     train index of 5 minibatch: 3      time used: 3.52398419380188\n","minibatch AVG loss: 0.006281721813138574\n","\n","Epoch: 14  train \n","Loss: 0.0124  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.0844  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 3.710315465927124\n","minibatch AVG loss: 0.008516957564279438\n","Epoch: 15     train index of 5 minibatch: 2      time used: 3.520850896835327\n","minibatch AVG loss: 0.0014924000948667525\n","Epoch: 15     train index of 5 minibatch: 3      time used: 3.5237746238708496\n","minibatch AVG loss: 0.007570043951272964\n","\n","Epoch: 15  train \n","Loss: 0.0052  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.1143  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 3.7406020164489746\n","minibatch AVG loss: 0.003820165852084756\n","Epoch: 16     train index of 5 minibatch: 2      time used: 3.526573419570923\n","minibatch AVG loss: 0.0028956919966731222\n","Epoch: 16     train index of 5 minibatch: 3      time used: 3.529841661453247\n","minibatch AVG loss: 0.011860354233067482\n","\n","Epoch: 16  train \n","Loss: 0.0056  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.0262  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 3.750079870223999\n","minibatch AVG loss: 0.008941391622647643\n","Epoch: 17     train index of 5 minibatch: 2      time used: 3.5468668937683105\n","minibatch AVG loss: 0.0008237185480538756\n","Epoch: 17     train index of 5 minibatch: 3      time used: 3.554309606552124\n","minibatch AVG loss: 0.0014755872078239917\n","\n","Epoch: 17  train \n","Loss: 0.0035  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.0388  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 3.747511148452759\n","minibatch AVG loss: 0.0017309939954429864\n","Epoch: 18     train index of 5 minibatch: 2      time used: 3.5526556968688965\n","minibatch AVG loss: 0.0012284777942113579\n","Epoch: 18     train index of 5 minibatch: 3      time used: 3.5548593997955322\n","minibatch AVG loss: 0.001285152602940798\n","\n","Epoch: 18  train \n","Loss: 0.0013  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.0404  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 3.7485013008117676\n","minibatch AVG loss: 0.0015847248723730445\n","Epoch: 19     train index of 5 minibatch: 2      time used: 3.5644216537475586\n","minibatch AVG loss: 0.0012777507887221872\n","Epoch: 19     train index of 5 minibatch: 3      time used: 3.5538792610168457\n","minibatch AVG loss: 0.0016253906302154063\n","\n","Epoch: 19  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.0498  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 3.777959108352661\n","minibatch AVG loss: 0.0013604524370748549\n","Epoch: 20     train index of 5 minibatch: 2      time used: 3.5514848232269287\n","minibatch AVG loss: 0.0024334521731361747\n","Epoch: 20     train index of 5 minibatch: 3      time used: 3.5667669773101807\n","minibatch AVG loss: 0.0016143194632604718\n","\n","Epoch: 20  train \n","Loss: 0.0017  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.0391  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 3.742302894592285\n","minibatch AVG loss: 0.0020939296460710465\n","Epoch: 21     train index of 5 minibatch: 2      time used: 3.5460593700408936\n","minibatch AVG loss: 0.001572449062950909\n","Epoch: 21     train index of 5 minibatch: 3      time used: 3.5519208908081055\n","minibatch AVG loss: 0.0013858151156455278\n","\n","Epoch: 21  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.0540  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 3.751473903656006\n","minibatch AVG loss: 0.0015744737233035267\n","Epoch: 22     train index of 5 minibatch: 2      time used: 3.5495595932006836\n","minibatch AVG loss: 0.002176283439621329\n","Epoch: 22     train index of 5 minibatch: 3      time used: 3.552663564682007\n","minibatch AVG loss: 0.0010578017099760473\n","\n","Epoch: 22  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.0534  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 3.744509220123291\n","minibatch AVG loss: 0.0016811886336654425\n","Epoch: 23     train index of 5 minibatch: 2      time used: 3.5414276123046875\n","minibatch AVG loss: 0.0016048469115048647\n","Epoch: 23     train index of 5 minibatch: 3      time used: 3.5490591526031494\n","minibatch AVG loss: 0.0016047371900640428\n","\n","Epoch: 23  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.0489  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 3.7664153575897217\n","minibatch AVG loss: 0.0017499150009825826\n","Epoch: 24     train index of 5 minibatch: 2      time used: 3.544295310974121\n","minibatch AVG loss: 0.0022264627739787103\n","Epoch: 24     train index of 5 minibatch: 3      time used: 3.5551364421844482\n","minibatch AVG loss: 0.0013925625593401493\n","\n","Epoch: 24  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.0406  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 3.7460265159606934\n","minibatch AVG loss: 0.0017447369173169136\n","Epoch: 25     train index of 5 minibatch: 2      time used: 3.544640302658081\n","minibatch AVG loss: 0.0013504682341590525\n","Epoch: 25     train index of 5 minibatch: 3      time used: 3.544783353805542\n","minibatch AVG loss: 0.0010768077685497702\n","\n","Epoch: 25  train \n","Loss: 0.0013  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.0422  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 3.7387146949768066\n","minibatch AVG loss: 0.0010526028578169643\n","Epoch: 26     train index of 5 minibatch: 2      time used: 3.5483860969543457\n","minibatch AVG loss: 0.0017041170387528837\n","Epoch: 26     train index of 5 minibatch: 3      time used: 3.5439462661743164\n","minibatch AVG loss: 0.0014558200957253576\n","\n","Epoch: 26  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.0454  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 3.7568647861480713\n","minibatch AVG loss: 0.0030243630753830075\n","Epoch: 27     train index of 5 minibatch: 2      time used: 3.5429508686065674\n","minibatch AVG loss: 0.0011515214573591948\n","Epoch: 27     train index of 5 minibatch: 3      time used: 3.5450923442840576\n","minibatch AVG loss: 0.0014552010223269462\n","\n","Epoch: 27  train \n","Loss: 0.0018  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.0438  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 3.7458438873291016\n","minibatch AVG loss: 0.0018174713593907654\n","Epoch: 28     train index of 5 minibatch: 2      time used: 3.5461833477020264\n","minibatch AVG loss: 0.0018494105665013195\n","Epoch: 28     train index of 5 minibatch: 3      time used: 3.5474212169647217\n","minibatch AVG loss: 0.0009922124445438385\n","\n","Epoch: 28  train \n","Loss: 0.0017  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.0446  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 3.735290050506592\n","minibatch AVG loss: 0.0011914444272406398\n","Epoch: 29     train index of 5 minibatch: 2      time used: 3.550495147705078\n","minibatch AVG loss: 0.0020666767260991035\n","Epoch: 29     train index of 5 minibatch: 3      time used: 3.557158946990967\n","minibatch AVG loss: 0.0012246150290593505\n","\n","Epoch: 29  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.0443  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 3.7381434440612793\n","minibatch AVG loss: 0.0014354223036207258\n","Epoch: 30     train index of 5 minibatch: 2      time used: 3.541826009750366\n","minibatch AVG loss: 0.0010422285529784858\n","Epoch: 30     train index of 5 minibatch: 3      time used: 3.5405750274658203\n","minibatch AVG loss: 0.0012561669340357185\n","\n","Epoch: 30  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.0453  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 3.7569007873535156\n","minibatch AVG loss: 0.0014499813667498529\n","Epoch: 31     train index of 5 minibatch: 2      time used: 3.5512218475341797\n","minibatch AVG loss: 0.0011733809835277498\n","Epoch: 31     train index of 5 minibatch: 3      time used: 3.5517807006835938\n","minibatch AVG loss: 0.0013166001997888088\n","\n","Epoch: 31  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.0463  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 3.734971046447754\n","minibatch AVG loss: 0.0017089240485802294\n","Epoch: 32     train index of 5 minibatch: 2      time used: 3.550347089767456\n","minibatch AVG loss: 0.0038534749415703117\n","Epoch: 32     train index of 5 minibatch: 3      time used: 3.556978464126587\n","minibatch AVG loss: 0.0013343215803615748\n","\n","Epoch: 32  train \n","Loss: 0.0023  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.0603  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 3.7495477199554443\n","minibatch AVG loss: 0.002442875481210649\n","Epoch: 33     train index of 5 minibatch: 2      time used: 3.54349422454834\n","minibatch AVG loss: 0.0012845155142713338\n","Epoch: 33     train index of 5 minibatch: 3      time used: 3.5489234924316406\n","minibatch AVG loss: 0.0017775774584151804\n","\n","Epoch: 33  train \n","Loss: 0.0017  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.0498  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 3.755842685699463\n","minibatch AVG loss: 0.0008527848985977471\n","Epoch: 34     train index of 5 minibatch: 2      time used: 3.5435850620269775\n","minibatch AVG loss: 0.0014555134694091976\n","Epoch: 34     train index of 5 minibatch: 3      time used: 3.5524215698242188\n","minibatch AVG loss: 0.0012250178027898074\n","\n","Epoch: 34  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.0482  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 3.756680488586426\n","minibatch AVG loss: 0.0011087743914686143\n","Epoch: 35     train index of 5 minibatch: 2      time used: 3.5449140071868896\n","minibatch AVG loss: 0.0013288351125083863\n","Epoch: 35     train index of 5 minibatch: 3      time used: 3.547684669494629\n","minibatch AVG loss: 0.0011035322677344084\n","\n","Epoch: 35  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.0483  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 3.7543466091156006\n","minibatch AVG loss: 0.002069875004235655\n","Epoch: 36     train index of 5 minibatch: 2      time used: 3.540377378463745\n","minibatch AVG loss: 0.0011121980904135853\n","Epoch: 36     train index of 5 minibatch: 3      time used: 3.543290376663208\n","minibatch AVG loss: 0.0009112857282161712\n","\n","Epoch: 36  train \n","Loss: 0.0013  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.0488  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 3.7454898357391357\n","minibatch AVG loss: 0.0015704492921940981\n","Epoch: 37     train index of 5 minibatch: 2      time used: 3.5439016819000244\n","minibatch AVG loss: 0.0011974024353548884\n","Epoch: 37     train index of 5 minibatch: 3      time used: 3.5475711822509766\n","minibatch AVG loss: 0.00083866334753111\n","\n","Epoch: 37  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.0492  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 3.7598037719726562\n","minibatch AVG loss: 0.0014814356225542724\n","Epoch: 38     train index of 5 minibatch: 2      time used: 3.546635389328003\n","minibatch AVG loss: 0.0012722996296361088\n","Epoch: 38     train index of 5 minibatch: 3      time used: 3.550950765609741\n","minibatch AVG loss: 0.0018354336847551168\n","\n","Epoch: 38  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.0507  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 3.74924898147583\n","minibatch AVG loss: 0.0008411236805841327\n","Epoch: 39     train index of 5 minibatch: 2      time used: 3.5496158599853516\n","minibatch AVG loss: 0.002445026836358011\n","Epoch: 39     train index of 5 minibatch: 3      time used: 3.5530824661254883\n","minibatch AVG loss: 0.002002906205598265\n","\n","Epoch: 39  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.0481  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 3.750016212463379\n","minibatch AVG loss: 0.0011476018698886037\n","Epoch: 40     train index of 5 minibatch: 2      time used: 3.5427730083465576\n","minibatch AVG loss: 0.002649974450469017\n","Epoch: 40     train index of 5 minibatch: 3      time used: 3.550731897354126\n","minibatch AVG loss: 0.0009275223477743566\n","\n","Epoch: 40  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.0479  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 3.7587437629699707\n","minibatch AVG loss: 0.001796318800188601\n","Epoch: 41     train index of 5 minibatch: 2      time used: 3.5470328330993652\n","minibatch AVG loss: 0.0025155111216008663\n","Epoch: 41     train index of 5 minibatch: 3      time used: 3.549752950668335\n","minibatch AVG loss: 0.0007201721600722522\n","\n","Epoch: 41  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.0502  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 3.7554445266723633\n","minibatch AVG loss: 0.0016679235966876149\n","Epoch: 42     train index of 5 minibatch: 2      time used: 3.54626727104187\n","minibatch AVG loss: 0.0009297527140006423\n","Epoch: 42     train index of 5 minibatch: 3      time used: 3.5550472736358643\n","minibatch AVG loss: 0.0012461598846130074\n","\n","Epoch: 42  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.0527  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 3.7496509552001953\n","minibatch AVG loss: 0.0007934022520203144\n","Epoch: 43     train index of 5 minibatch: 2      time used: 3.55228590965271\n","minibatch AVG loss: 0.0011539210798218846\n","Epoch: 43     train index of 5 minibatch: 3      time used: 3.553884506225586\n","minibatch AVG loss: 0.001764601084869355\n","\n","Epoch: 43  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.0528  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 3.739118814468384\n","minibatch AVG loss: 0.0010459223412908614\n","Epoch: 44     train index of 5 minibatch: 2      time used: 3.551316499710083\n","minibatch AVG loss: 0.0008232883526943624\n","Epoch: 44     train index of 5 minibatch: 3      time used: 3.545997381210327\n","minibatch AVG loss: 0.001857066701631993\n","\n","Epoch: 44  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.0524  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 3.7505595684051514\n","minibatch AVG loss: 0.0014991071540862322\n","Epoch: 45     train index of 5 minibatch: 2      time used: 3.5440680980682373\n","minibatch AVG loss: 0.0010576873202808202\n","Epoch: 45     train index of 5 minibatch: 3      time used: 3.5521421432495117\n","minibatch AVG loss: 0.0010244560544379056\n","\n","Epoch: 45  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.0532  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 3.755824327468872\n","minibatch AVG loss: 0.0017472750041633844\n","Epoch: 46     train index of 5 minibatch: 2      time used: 3.5453059673309326\n","minibatch AVG loss: 0.0009500907035544515\n","Epoch: 46     train index of 5 minibatch: 3      time used: 3.55165433883667\n","minibatch AVG loss: 0.0009216265752911568\n","\n","Epoch: 46  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.0537  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 3.7577877044677734\n","minibatch AVG loss: 0.0012438822421245276\n","Epoch: 47     train index of 5 minibatch: 2      time used: 3.5540199279785156\n","minibatch AVG loss: 0.0013215214887168259\n","Epoch: 47     train index of 5 minibatch: 3      time used: 3.552145481109619\n","minibatch AVG loss: 0.000923700473504141\n","\n","Epoch: 47  train \n","Loss: 0.0013  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.0535  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 3.7495012283325195\n","minibatch AVG loss: 0.001608903717715293\n","Epoch: 48     train index of 5 minibatch: 2      time used: 3.551133871078491\n","minibatch AVG loss: 0.0015031155431643128\n","Epoch: 48     train index of 5 minibatch: 3      time used: 3.5564138889312744\n","minibatch AVG loss: 0.0010949274990707635\n","\n","Epoch: 48  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.0517  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 3.7678730487823486\n","minibatch AVG loss: 0.0011747189913876356\n","Epoch: 49     train index of 5 minibatch: 2      time used: 3.5557005405426025\n","minibatch AVG loss: 0.0009676698478870094\n","Epoch: 49     train index of 5 minibatch: 3      time used: 3.5564403533935547\n","minibatch AVG loss: 0.0007434899860527366\n","\n","Epoch: 49  train \n","Loss: 0.0010  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.0509  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 3.759040117263794\n","minibatch AVG loss: 0.0014203786646248772\n","Epoch: 50     train index of 5 minibatch: 2      time used: 3.5508551597595215\n","minibatch AVG loss: 0.0013416648493148387\n","Epoch: 50     train index of 5 minibatch: 3      time used: 3.551348924636841\n","minibatch AVG loss: 0.0009547373047098517\n","\n","Epoch: 50  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.0515  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 11m 11s\n","Best epoch idx:  50\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aU1iyR9aSK1X","executionInfo":{"status":"ok","timestamp":1651156094070,"user_tz":-480,"elapsed":173661,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"f4f60f16-aea4-4377-ebd6-eacad394a354"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['efficientnet_b0',\n"," 'efficientnet_b1',\n"," 'efficientnet_b1_pruned',\n"," 'efficientnet_b2',\n"," 'efficientnet_b2_pruned',\n"," 'efficientnet_b2a',\n"," 'efficientnet_b3',\n"," 'efficientnet_b3_pruned',\n"," 'efficientnet_b3a',\n"," 'efficientnet_b4',\n"," 'efficientnet_b5',\n"," 'efficientnet_b6',\n"," 'efficientnet_b7',\n"," 'efficientnet_b8',\n"," 'efficientnet_cc_b0_4e',\n"," 'efficientnet_cc_b0_8e',\n"," 'efficientnet_cc_b1_8e',\n"," 'efficientnet_el',\n"," 'efficientnet_el_pruned',\n"," 'efficientnet_em',\n"," 'efficientnet_es',\n"," 'efficientnet_es_pruned',\n"," 'efficientnet_l2',\n"," 'efficientnet_lite0',\n"," 'efficientnet_lite1',\n"," 'efficientnet_lite2',\n"," 'efficientnet_lite3',\n"," 'efficientnet_lite4',\n"," 'efficientnetv2_l',\n"," 'efficientnetv2_m',\n"," 'efficientnetv2_rw_m',\n"," 'efficientnetv2_rw_s',\n"," 'efficientnetv2_rw_t',\n"," 'efficientnetv2_s',\n"," 'efficientnetv2_xl',\n"," 'gc_efficientnetv2_rw_t',\n"," 'tf_efficientnet_b0',\n"," 'tf_efficientnet_b0_ap',\n"," 'tf_efficientnet_b0_ns',\n"," 'tf_efficientnet_b1',\n"," 'tf_efficientnet_b1_ap',\n"," 'tf_efficientnet_b1_ns',\n"," 'tf_efficientnet_b2',\n"," 'tf_efficientnet_b2_ap',\n"," 'tf_efficientnet_b2_ns',\n"," 'tf_efficientnet_b3',\n"," 'tf_efficientnet_b3_ap',\n"," 'tf_efficientnet_b3_ns',\n"," 'tf_efficientnet_b4',\n"," 'tf_efficientnet_b4_ap',\n"," 'tf_efficientnet_b4_ns',\n"," 'tf_efficientnet_b5',\n"," 'tf_efficientnet_b5_ap',\n"," 'tf_efficientnet_b5_ns',\n"," 'tf_efficientnet_b6',\n"," 'tf_efficientnet_b6_ap',\n"," 'tf_efficientnet_b6_ns',\n"," 'tf_efficientnet_b7',\n"," 'tf_efficientnet_b7_ap',\n"," 'tf_efficientnet_b7_ns',\n"," 'tf_efficientnet_b8',\n"," 'tf_efficientnet_b8_ap',\n"," 'tf_efficientnet_cc_b0_4e',\n"," 'tf_efficientnet_cc_b0_8e',\n"," 'tf_efficientnet_cc_b1_8e',\n"," 'tf_efficientnet_el',\n"," 'tf_efficientnet_em',\n"," 'tf_efficientnet_es',\n"," 'tf_efficientnet_l2_ns',\n"," 'tf_efficientnet_l2_ns_475',\n"," 'tf_efficientnet_lite0',\n"," 'tf_efficientnet_lite1',\n"," 'tf_efficientnet_lite2',\n"," 'tf_efficientnet_lite3',\n"," 'tf_efficientnet_lite4',\n"," 'tf_efficientnetv2_b0',\n"," 'tf_efficientnetv2_b1',\n"," 'tf_efficientnetv2_b2',\n"," 'tf_efficientnetv2_b3',\n"," 'tf_efficientnetv2_l',\n"," 'tf_efficientnetv2_l_in21ft1k',\n"," 'tf_efficientnetv2_l_in21k',\n"," 'tf_efficientnetv2_m',\n"," 'tf_efficientnetv2_m_in21ft1k',\n"," 'tf_efficientnetv2_m_in21k',\n"," 'tf_efficientnetv2_s',\n"," 'tf_efficientnetv2_s_in21ft1k',\n"," 'tf_efficientnetv2_s_in21k',\n"," 'tf_efficientnetv2_xl_in21ft1k',\n"," 'tf_efficientnetv2_xl_in21k']\n","Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth\n","test model output： tensor([[-0.9696, -0.4129]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 40, 192, 192]           1,080\n","       BatchNorm2d-2         [-1, 40, 192, 192]              80\n","              SiLU-3         [-1, 40, 192, 192]               0\n","            Conv2d-4         [-1, 40, 192, 192]             360\n","       BatchNorm2d-5         [-1, 40, 192, 192]              80\n","              SiLU-6         [-1, 40, 192, 192]               0\n","            Conv2d-7             [-1, 10, 1, 1]             410\n","              SiLU-8             [-1, 10, 1, 1]               0\n","            Conv2d-9             [-1, 40, 1, 1]             440\n","          Sigmoid-10             [-1, 40, 1, 1]               0\n","    SqueezeExcite-11         [-1, 40, 192, 192]               0\n","           Conv2d-12         [-1, 24, 192, 192]             960\n","      BatchNorm2d-13         [-1, 24, 192, 192]              48\n","         Identity-14         [-1, 24, 192, 192]               0\n","DepthwiseSeparableConv-15         [-1, 24, 192, 192]               0\n","           Conv2d-16         [-1, 24, 192, 192]             216\n","      BatchNorm2d-17         [-1, 24, 192, 192]              48\n","             SiLU-18         [-1, 24, 192, 192]               0\n","           Conv2d-19              [-1, 6, 1, 1]             150\n","             SiLU-20              [-1, 6, 1, 1]               0\n","           Conv2d-21             [-1, 24, 1, 1]             168\n","          Sigmoid-22             [-1, 24, 1, 1]               0\n","    SqueezeExcite-23         [-1, 24, 192, 192]               0\n","           Conv2d-24         [-1, 24, 192, 192]             576\n","      BatchNorm2d-25         [-1, 24, 192, 192]              48\n","         Identity-26         [-1, 24, 192, 192]               0\n","DepthwiseSeparableConv-27         [-1, 24, 192, 192]               0\n","           Conv2d-28        [-1, 144, 192, 192]           3,456\n","      BatchNorm2d-29        [-1, 144, 192, 192]             288\n","             SiLU-30        [-1, 144, 192, 192]               0\n","           Conv2d-31          [-1, 144, 96, 96]           1,296\n","      BatchNorm2d-32          [-1, 144, 96, 96]             288\n","             SiLU-33          [-1, 144, 96, 96]               0\n","           Conv2d-34              [-1, 6, 1, 1]             870\n","             SiLU-35              [-1, 6, 1, 1]               0\n","           Conv2d-36            [-1, 144, 1, 1]           1,008\n","          Sigmoid-37            [-1, 144, 1, 1]               0\n","    SqueezeExcite-38          [-1, 144, 96, 96]               0\n","           Conv2d-39           [-1, 32, 96, 96]           4,608\n","      BatchNorm2d-40           [-1, 32, 96, 96]              64\n"," InvertedResidual-41           [-1, 32, 96, 96]               0\n","           Conv2d-42          [-1, 192, 96, 96]           6,144\n","      BatchNorm2d-43          [-1, 192, 96, 96]             384\n","             SiLU-44          [-1, 192, 96, 96]               0\n","           Conv2d-45          [-1, 192, 96, 96]           1,728\n","      BatchNorm2d-46          [-1, 192, 96, 96]             384\n","             SiLU-47          [-1, 192, 96, 96]               0\n","           Conv2d-48              [-1, 8, 1, 1]           1,544\n","             SiLU-49              [-1, 8, 1, 1]               0\n","           Conv2d-50            [-1, 192, 1, 1]           1,728\n","          Sigmoid-51            [-1, 192, 1, 1]               0\n","    SqueezeExcite-52          [-1, 192, 96, 96]               0\n","           Conv2d-53           [-1, 32, 96, 96]           6,144\n","      BatchNorm2d-54           [-1, 32, 96, 96]              64\n"," InvertedResidual-55           [-1, 32, 96, 96]               0\n","           Conv2d-56          [-1, 192, 96, 96]           6,144\n","      BatchNorm2d-57          [-1, 192, 96, 96]             384\n","             SiLU-58          [-1, 192, 96, 96]               0\n","           Conv2d-59          [-1, 192, 96, 96]           1,728\n","      BatchNorm2d-60          [-1, 192, 96, 96]             384\n","             SiLU-61          [-1, 192, 96, 96]               0\n","           Conv2d-62              [-1, 8, 1, 1]           1,544\n","             SiLU-63              [-1, 8, 1, 1]               0\n","           Conv2d-64            [-1, 192, 1, 1]           1,728\n","          Sigmoid-65            [-1, 192, 1, 1]               0\n","    SqueezeExcite-66          [-1, 192, 96, 96]               0\n","           Conv2d-67           [-1, 32, 96, 96]           6,144\n","      BatchNorm2d-68           [-1, 32, 96, 96]              64\n"," InvertedResidual-69           [-1, 32, 96, 96]               0\n","           Conv2d-70          [-1, 192, 96, 96]           6,144\n","      BatchNorm2d-71          [-1, 192, 96, 96]             384\n","             SiLU-72          [-1, 192, 96, 96]               0\n","           Conv2d-73          [-1, 192, 48, 48]           4,800\n","      BatchNorm2d-74          [-1, 192, 48, 48]             384\n","             SiLU-75          [-1, 192, 48, 48]               0\n","           Conv2d-76              [-1, 8, 1, 1]           1,544\n","             SiLU-77              [-1, 8, 1, 1]               0\n","           Conv2d-78            [-1, 192, 1, 1]           1,728\n","          Sigmoid-79            [-1, 192, 1, 1]               0\n","    SqueezeExcite-80          [-1, 192, 48, 48]               0\n","           Conv2d-81           [-1, 48, 48, 48]           9,216\n","      BatchNorm2d-82           [-1, 48, 48, 48]              96\n"," InvertedResidual-83           [-1, 48, 48, 48]               0\n","           Conv2d-84          [-1, 288, 48, 48]          13,824\n","      BatchNorm2d-85          [-1, 288, 48, 48]             576\n","             SiLU-86          [-1, 288, 48, 48]               0\n","           Conv2d-87          [-1, 288, 48, 48]           7,200\n","      BatchNorm2d-88          [-1, 288, 48, 48]             576\n","             SiLU-89          [-1, 288, 48, 48]               0\n","           Conv2d-90             [-1, 12, 1, 1]           3,468\n","             SiLU-91             [-1, 12, 1, 1]               0\n","           Conv2d-92            [-1, 288, 1, 1]           3,744\n","          Sigmoid-93            [-1, 288, 1, 1]               0\n","    SqueezeExcite-94          [-1, 288, 48, 48]               0\n","           Conv2d-95           [-1, 48, 48, 48]          13,824\n","      BatchNorm2d-96           [-1, 48, 48, 48]              96\n"," InvertedResidual-97           [-1, 48, 48, 48]               0\n","           Conv2d-98          [-1, 288, 48, 48]          13,824\n","      BatchNorm2d-99          [-1, 288, 48, 48]             576\n","            SiLU-100          [-1, 288, 48, 48]               0\n","          Conv2d-101          [-1, 288, 48, 48]           7,200\n","     BatchNorm2d-102          [-1, 288, 48, 48]             576\n","            SiLU-103          [-1, 288, 48, 48]               0\n","          Conv2d-104             [-1, 12, 1, 1]           3,468\n","            SiLU-105             [-1, 12, 1, 1]               0\n","          Conv2d-106            [-1, 288, 1, 1]           3,744\n","         Sigmoid-107            [-1, 288, 1, 1]               0\n","   SqueezeExcite-108          [-1, 288, 48, 48]               0\n","          Conv2d-109           [-1, 48, 48, 48]          13,824\n","     BatchNorm2d-110           [-1, 48, 48, 48]              96\n","InvertedResidual-111           [-1, 48, 48, 48]               0\n","          Conv2d-112          [-1, 288, 48, 48]          13,824\n","     BatchNorm2d-113          [-1, 288, 48, 48]             576\n","            SiLU-114          [-1, 288, 48, 48]               0\n","          Conv2d-115          [-1, 288, 24, 24]           2,592\n","     BatchNorm2d-116          [-1, 288, 24, 24]             576\n","            SiLU-117          [-1, 288, 24, 24]               0\n","          Conv2d-118             [-1, 12, 1, 1]           3,468\n","            SiLU-119             [-1, 12, 1, 1]               0\n","          Conv2d-120            [-1, 288, 1, 1]           3,744\n","         Sigmoid-121            [-1, 288, 1, 1]               0\n","   SqueezeExcite-122          [-1, 288, 24, 24]               0\n","          Conv2d-123           [-1, 96, 24, 24]          27,648\n","     BatchNorm2d-124           [-1, 96, 24, 24]             192\n","InvertedResidual-125           [-1, 96, 24, 24]               0\n","          Conv2d-126          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-127          [-1, 576, 24, 24]           1,152\n","            SiLU-128          [-1, 576, 24, 24]               0\n","          Conv2d-129          [-1, 576, 24, 24]           5,184\n","     BatchNorm2d-130          [-1, 576, 24, 24]           1,152\n","            SiLU-131          [-1, 576, 24, 24]               0\n","          Conv2d-132             [-1, 24, 1, 1]          13,848\n","            SiLU-133             [-1, 24, 1, 1]               0\n","          Conv2d-134            [-1, 576, 1, 1]          14,400\n","         Sigmoid-135            [-1, 576, 1, 1]               0\n","   SqueezeExcite-136          [-1, 576, 24, 24]               0\n","          Conv2d-137           [-1, 96, 24, 24]          55,296\n","     BatchNorm2d-138           [-1, 96, 24, 24]             192\n","InvertedResidual-139           [-1, 96, 24, 24]               0\n","          Conv2d-140          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-141          [-1, 576, 24, 24]           1,152\n","            SiLU-142          [-1, 576, 24, 24]               0\n","          Conv2d-143          [-1, 576, 24, 24]           5,184\n","     BatchNorm2d-144          [-1, 576, 24, 24]           1,152\n","            SiLU-145          [-1, 576, 24, 24]               0\n","          Conv2d-146             [-1, 24, 1, 1]          13,848\n","            SiLU-147             [-1, 24, 1, 1]               0\n","          Conv2d-148            [-1, 576, 1, 1]          14,400\n","         Sigmoid-149            [-1, 576, 1, 1]               0\n","   SqueezeExcite-150          [-1, 576, 24, 24]               0\n","          Conv2d-151           [-1, 96, 24, 24]          55,296\n","     BatchNorm2d-152           [-1, 96, 24, 24]             192\n","InvertedResidual-153           [-1, 96, 24, 24]               0\n","          Conv2d-154          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-155          [-1, 576, 24, 24]           1,152\n","            SiLU-156          [-1, 576, 24, 24]               0\n","          Conv2d-157          [-1, 576, 24, 24]           5,184\n","     BatchNorm2d-158          [-1, 576, 24, 24]           1,152\n","            SiLU-159          [-1, 576, 24, 24]               0\n","          Conv2d-160             [-1, 24, 1, 1]          13,848\n","            SiLU-161             [-1, 24, 1, 1]               0\n","          Conv2d-162            [-1, 576, 1, 1]          14,400\n","         Sigmoid-163            [-1, 576, 1, 1]               0\n","   SqueezeExcite-164          [-1, 576, 24, 24]               0\n","          Conv2d-165           [-1, 96, 24, 24]          55,296\n","     BatchNorm2d-166           [-1, 96, 24, 24]             192\n","InvertedResidual-167           [-1, 96, 24, 24]               0\n","          Conv2d-168          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-169          [-1, 576, 24, 24]           1,152\n","            SiLU-170          [-1, 576, 24, 24]               0\n","          Conv2d-171          [-1, 576, 24, 24]           5,184\n","     BatchNorm2d-172          [-1, 576, 24, 24]           1,152\n","            SiLU-173          [-1, 576, 24, 24]               0\n","          Conv2d-174             [-1, 24, 1, 1]          13,848\n","            SiLU-175             [-1, 24, 1, 1]               0\n","          Conv2d-176            [-1, 576, 1, 1]          14,400\n","         Sigmoid-177            [-1, 576, 1, 1]               0\n","   SqueezeExcite-178          [-1, 576, 24, 24]               0\n","          Conv2d-179           [-1, 96, 24, 24]          55,296\n","     BatchNorm2d-180           [-1, 96, 24, 24]             192\n","InvertedResidual-181           [-1, 96, 24, 24]               0\n","          Conv2d-182          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-183          [-1, 576, 24, 24]           1,152\n","            SiLU-184          [-1, 576, 24, 24]               0\n","          Conv2d-185          [-1, 576, 24, 24]          14,400\n","     BatchNorm2d-186          [-1, 576, 24, 24]           1,152\n","            SiLU-187          [-1, 576, 24, 24]               0\n","          Conv2d-188             [-1, 24, 1, 1]          13,848\n","            SiLU-189             [-1, 24, 1, 1]               0\n","          Conv2d-190            [-1, 576, 1, 1]          14,400\n","         Sigmoid-191            [-1, 576, 1, 1]               0\n","   SqueezeExcite-192          [-1, 576, 24, 24]               0\n","          Conv2d-193          [-1, 136, 24, 24]          78,336\n","     BatchNorm2d-194          [-1, 136, 24, 24]             272\n","InvertedResidual-195          [-1, 136, 24, 24]               0\n","          Conv2d-196          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-197          [-1, 816, 24, 24]           1,632\n","            SiLU-198          [-1, 816, 24, 24]               0\n","          Conv2d-199          [-1, 816, 24, 24]          20,400\n","     BatchNorm2d-200          [-1, 816, 24, 24]           1,632\n","            SiLU-201          [-1, 816, 24, 24]               0\n","          Conv2d-202             [-1, 34, 1, 1]          27,778\n","            SiLU-203             [-1, 34, 1, 1]               0\n","          Conv2d-204            [-1, 816, 1, 1]          28,560\n","         Sigmoid-205            [-1, 816, 1, 1]               0\n","   SqueezeExcite-206          [-1, 816, 24, 24]               0\n","          Conv2d-207          [-1, 136, 24, 24]         110,976\n","     BatchNorm2d-208          [-1, 136, 24, 24]             272\n","InvertedResidual-209          [-1, 136, 24, 24]               0\n","          Conv2d-210          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-211          [-1, 816, 24, 24]           1,632\n","            SiLU-212          [-1, 816, 24, 24]               0\n","          Conv2d-213          [-1, 816, 24, 24]          20,400\n","     BatchNorm2d-214          [-1, 816, 24, 24]           1,632\n","            SiLU-215          [-1, 816, 24, 24]               0\n","          Conv2d-216             [-1, 34, 1, 1]          27,778\n","            SiLU-217             [-1, 34, 1, 1]               0\n","          Conv2d-218            [-1, 816, 1, 1]          28,560\n","         Sigmoid-219            [-1, 816, 1, 1]               0\n","   SqueezeExcite-220          [-1, 816, 24, 24]               0\n","          Conv2d-221          [-1, 136, 24, 24]         110,976\n","     BatchNorm2d-222          [-1, 136, 24, 24]             272\n","InvertedResidual-223          [-1, 136, 24, 24]               0\n","          Conv2d-224          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-225          [-1, 816, 24, 24]           1,632\n","            SiLU-226          [-1, 816, 24, 24]               0\n","          Conv2d-227          [-1, 816, 24, 24]          20,400\n","     BatchNorm2d-228          [-1, 816, 24, 24]           1,632\n","            SiLU-229          [-1, 816, 24, 24]               0\n","          Conv2d-230             [-1, 34, 1, 1]          27,778\n","            SiLU-231             [-1, 34, 1, 1]               0\n","          Conv2d-232            [-1, 816, 1, 1]          28,560\n","         Sigmoid-233            [-1, 816, 1, 1]               0\n","   SqueezeExcite-234          [-1, 816, 24, 24]               0\n","          Conv2d-235          [-1, 136, 24, 24]         110,976\n","     BatchNorm2d-236          [-1, 136, 24, 24]             272\n","InvertedResidual-237          [-1, 136, 24, 24]               0\n","          Conv2d-238          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-239          [-1, 816, 24, 24]           1,632\n","            SiLU-240          [-1, 816, 24, 24]               0\n","          Conv2d-241          [-1, 816, 24, 24]          20,400\n","     BatchNorm2d-242          [-1, 816, 24, 24]           1,632\n","            SiLU-243          [-1, 816, 24, 24]               0\n","          Conv2d-244             [-1, 34, 1, 1]          27,778\n","            SiLU-245             [-1, 34, 1, 1]               0\n","          Conv2d-246            [-1, 816, 1, 1]          28,560\n","         Sigmoid-247            [-1, 816, 1, 1]               0\n","   SqueezeExcite-248          [-1, 816, 24, 24]               0\n","          Conv2d-249          [-1, 136, 24, 24]         110,976\n","     BatchNorm2d-250          [-1, 136, 24, 24]             272\n","InvertedResidual-251          [-1, 136, 24, 24]               0\n","          Conv2d-252          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-253          [-1, 816, 24, 24]           1,632\n","            SiLU-254          [-1, 816, 24, 24]               0\n","          Conv2d-255          [-1, 816, 12, 12]          20,400\n","     BatchNorm2d-256          [-1, 816, 12, 12]           1,632\n","            SiLU-257          [-1, 816, 12, 12]               0\n","          Conv2d-258             [-1, 34, 1, 1]          27,778\n","            SiLU-259             [-1, 34, 1, 1]               0\n","          Conv2d-260            [-1, 816, 1, 1]          28,560\n","         Sigmoid-261            [-1, 816, 1, 1]               0\n","   SqueezeExcite-262          [-1, 816, 12, 12]               0\n","          Conv2d-263          [-1, 232, 12, 12]         189,312\n","     BatchNorm2d-264          [-1, 232, 12, 12]             464\n","InvertedResidual-265          [-1, 232, 12, 12]               0\n","          Conv2d-266         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-267         [-1, 1392, 12, 12]           2,784\n","            SiLU-268         [-1, 1392, 12, 12]               0\n","          Conv2d-269         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-270         [-1, 1392, 12, 12]           2,784\n","            SiLU-271         [-1, 1392, 12, 12]               0\n","          Conv2d-272             [-1, 58, 1, 1]          80,794\n","            SiLU-273             [-1, 58, 1, 1]               0\n","          Conv2d-274           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-275           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-276         [-1, 1392, 12, 12]               0\n","          Conv2d-277          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-278          [-1, 232, 12, 12]             464\n","InvertedResidual-279          [-1, 232, 12, 12]               0\n","          Conv2d-280         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-281         [-1, 1392, 12, 12]           2,784\n","            SiLU-282         [-1, 1392, 12, 12]               0\n","          Conv2d-283         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-284         [-1, 1392, 12, 12]           2,784\n","            SiLU-285         [-1, 1392, 12, 12]               0\n","          Conv2d-286             [-1, 58, 1, 1]          80,794\n","            SiLU-287             [-1, 58, 1, 1]               0\n","          Conv2d-288           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-289           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-290         [-1, 1392, 12, 12]               0\n","          Conv2d-291          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-292          [-1, 232, 12, 12]             464\n","InvertedResidual-293          [-1, 232, 12, 12]               0\n","          Conv2d-294         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-295         [-1, 1392, 12, 12]           2,784\n","            SiLU-296         [-1, 1392, 12, 12]               0\n","          Conv2d-297         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-298         [-1, 1392, 12, 12]           2,784\n","            SiLU-299         [-1, 1392, 12, 12]               0\n","          Conv2d-300             [-1, 58, 1, 1]          80,794\n","            SiLU-301             [-1, 58, 1, 1]               0\n","          Conv2d-302           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-303           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-304         [-1, 1392, 12, 12]               0\n","          Conv2d-305          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-306          [-1, 232, 12, 12]             464\n","InvertedResidual-307          [-1, 232, 12, 12]               0\n","          Conv2d-308         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-309         [-1, 1392, 12, 12]           2,784\n","            SiLU-310         [-1, 1392, 12, 12]               0\n","          Conv2d-311         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-312         [-1, 1392, 12, 12]           2,784\n","            SiLU-313         [-1, 1392, 12, 12]               0\n","          Conv2d-314             [-1, 58, 1, 1]          80,794\n","            SiLU-315             [-1, 58, 1, 1]               0\n","          Conv2d-316           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-317           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-318         [-1, 1392, 12, 12]               0\n","          Conv2d-319          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-320          [-1, 232, 12, 12]             464\n","InvertedResidual-321          [-1, 232, 12, 12]               0\n","          Conv2d-322         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-323         [-1, 1392, 12, 12]           2,784\n","            SiLU-324         [-1, 1392, 12, 12]               0\n","          Conv2d-325         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-326         [-1, 1392, 12, 12]           2,784\n","            SiLU-327         [-1, 1392, 12, 12]               0\n","          Conv2d-328             [-1, 58, 1, 1]          80,794\n","            SiLU-329             [-1, 58, 1, 1]               0\n","          Conv2d-330           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-331           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-332         [-1, 1392, 12, 12]               0\n","          Conv2d-333          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-334          [-1, 232, 12, 12]             464\n","InvertedResidual-335          [-1, 232, 12, 12]               0\n","          Conv2d-336         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-337         [-1, 1392, 12, 12]           2,784\n","            SiLU-338         [-1, 1392, 12, 12]               0\n","          Conv2d-339         [-1, 1392, 12, 12]          12,528\n","     BatchNorm2d-340         [-1, 1392, 12, 12]           2,784\n","            SiLU-341         [-1, 1392, 12, 12]               0\n","          Conv2d-342             [-1, 58, 1, 1]          80,794\n","            SiLU-343             [-1, 58, 1, 1]               0\n","          Conv2d-344           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-345           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-346         [-1, 1392, 12, 12]               0\n","          Conv2d-347          [-1, 384, 12, 12]         534,528\n","     BatchNorm2d-348          [-1, 384, 12, 12]             768\n","InvertedResidual-349          [-1, 384, 12, 12]               0\n","          Conv2d-350         [-1, 2304, 12, 12]         884,736\n","     BatchNorm2d-351         [-1, 2304, 12, 12]           4,608\n","            SiLU-352         [-1, 2304, 12, 12]               0\n","          Conv2d-353         [-1, 2304, 12, 12]          20,736\n","     BatchNorm2d-354         [-1, 2304, 12, 12]           4,608\n","            SiLU-355         [-1, 2304, 12, 12]               0\n","          Conv2d-356             [-1, 96, 1, 1]         221,280\n","            SiLU-357             [-1, 96, 1, 1]               0\n","          Conv2d-358           [-1, 2304, 1, 1]         223,488\n","         Sigmoid-359           [-1, 2304, 1, 1]               0\n","   SqueezeExcite-360         [-1, 2304, 12, 12]               0\n","          Conv2d-361          [-1, 384, 12, 12]         884,736\n","     BatchNorm2d-362          [-1, 384, 12, 12]             768\n","InvertedResidual-363          [-1, 384, 12, 12]               0\n","          Conv2d-364         [-1, 1536, 12, 12]         589,824\n","     BatchNorm2d-365         [-1, 1536, 12, 12]           3,072\n","            SiLU-366         [-1, 1536, 12, 12]               0\n","AdaptiveAvgPool2d-367           [-1, 1536, 1, 1]               0\n","         Flatten-368                 [-1, 1536]               0\n","SelectAdaptivePool2d-369                 [-1, 1536]               0\n","          Linear-370                    [-1, 2]           3,074\n","================================================================\n","Total params: 10,699,306\n","Trainable params: 10,699,306\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 996.83\n","Params size (MB): 40.81\n","Estimated Total Size (MB): 1039.33\n","----------------------------------------------------------------\n","model : efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 1.1022930145263672\n","minibatch AVG loss: 1.9570249795913697\n","Epoch: 1     train index of 5 minibatch: 2      time used: 0.768693208694458\n","minibatch AVG loss: 1.877181100845337\n","Epoch: 1     train index of 5 minibatch: 3      time used: 0.7771756649017334\n","minibatch AVG loss: 1.7647089958190918\n","\n","Epoch: 1  train \n","Loss: 1.7111  Acc: 46.3768\n","benign precision: 41.1765  recall: 46.6667\n","benign sensitivity: 46.6667  specificity: 47.3684\n","benign FPR: 52.6316  NPV: 52.9412\n","benign TP: 14.0\n","benign TN: 18.0\n","benign FP: 20.0\n","benign FN: 16.0\n","malignant precision: 52.9412  recall: 47.3684\n","malignant sensitivity: 47.3684  specificity: 46.6667\n","malignant FPR: 53.3333  NPV: 41.1765\n","malignant TP: 18.0\n","malignant TN: 14.0\n","malignant FP: 16.0\n","malignant FN: 20.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 1.7805  Acc: 43.7500\n","benign precision: 40.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 33.3333\n","benign FPR: 66.6667  NPV: 50.0000\n","benign TP: 4.0\n","benign TN: 3.0\n","benign FP: 6.0\n","benign FN: 3.0\n","malignant precision: 50.0000  recall: 33.3333\n","malignant sensitivity: 33.3333  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 40.0000\n","malignant TP: 3.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 0.9921169281005859\n","minibatch AVG loss: 1.3367918372154235\n","Epoch: 2     train index of 5 minibatch: 2      time used: 0.7763822078704834\n","minibatch AVG loss: 2.1897631645202638\n","Epoch: 2     train index of 5 minibatch: 3      time used: 0.7708675861358643\n","minibatch AVG loss: 1.6775668919086457\n","\n","Epoch: 2  train \n","Loss: 1.6553  Acc: 50.7246\n","benign precision: 45.4545  recall: 50.0000\n","benign sensitivity: 50.0000  specificity: 52.6316\n","benign FPR: 47.3684  NPV: 57.1429\n","benign TP: 15.0\n","benign TN: 20.0\n","benign FP: 18.0\n","benign FN: 15.0\n","malignant precision: 57.1429  recall: 52.6316\n","malignant sensitivity: 52.6316  specificity: 50.0000\n","malignant FPR: 50.0000  NPV: 45.4545\n","malignant TP: 20.0\n","malignant TN: 15.0\n","malignant FP: 15.0\n","malignant FN: 18.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 209.3332  Acc: 31.2500\n","benign precision: 16.6667  recall: 14.2857\n","benign sensitivity: 14.2857  specificity: 44.4444\n","benign FPR: 55.5556  NPV: 40.0000\n","benign TP: 1.0\n","benign TN: 4.0\n","benign FP: 5.0\n","benign FN: 6.0\n","malignant precision: 40.0000  recall: 44.4444\n","malignant sensitivity: 44.4444  specificity: 14.2857\n","malignant FPR: 85.7143  NPV: 16.6667\n","malignant TP: 4.0\n","malignant TN: 1.0\n","malignant FP: 6.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 0.9968528747558594\n","minibatch AVG loss: 0.8780026733875275\n","Epoch: 3     train index of 5 minibatch: 2      time used: 0.7761120796203613\n","minibatch AVG loss: 1.3051899433135987\n","Epoch: 3     train index of 5 minibatch: 3      time used: 0.7710771560668945\n","minibatch AVG loss: 1.026234209537506\n","\n","Epoch: 3  train \n","Loss: 1.0131  Acc: 68.1159\n","benign precision: 62.1622  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 63.1579\n","benign FPR: 36.8421  NPV: 77.4194\n","benign TP: 23.0\n","benign TN: 24.0\n","benign FP: 14.0\n","benign FN: 7.0\n","malignant precision: 77.4194  recall: 63.1579\n","malignant sensitivity: 63.1579  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 62.1622\n","malignant TP: 24.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 14.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 1283.1323  Acc: 62.5000\n","benign precision: 57.1429  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 66.6667\n","benign FPR: 33.3333  NPV: 66.6667\n","benign TP: 4.0\n","benign TN: 6.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 66.6667  recall: 66.6667\n","malignant sensitivity: 66.6667  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 57.1429\n","malignant TP: 6.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 0.9992570877075195\n","minibatch AVG loss: 0.3773676723241806\n","Epoch: 4     train index of 5 minibatch: 2      time used: 0.7745363712310791\n","minibatch AVG loss: 0.8568615518510342\n","Epoch: 4     train index of 5 minibatch: 3      time used: 0.7720401287078857\n","minibatch AVG loss: 1.2265627266839148\n","\n","Epoch: 4  train \n","Loss: 0.7673  Acc: 68.1159\n","benign precision: 63.3333  recall: 65.5172\n","benign sensitivity: 65.5172  specificity: 71.7949\n","benign FPR: 28.2051  NPV: 73.6842\n","benign TP: 19.0\n","benign TN: 28.0\n","benign FP: 11.0\n","benign FN: 10.0\n","malignant precision: 73.6842  recall: 71.7949\n","malignant sensitivity: 71.7949  specificity: 65.5172\n","malignant FPR: 34.4828  NPV: 63.3333\n","malignant TP: 28.0\n","malignant TN: 19.0\n","malignant FP: 10.0\n","malignant FN: 11.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.7911  Acc: 68.7500\n","benign precision: 62.5000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 66.6667\n","benign FPR: 33.3333  NPV: 75.0000\n","benign TP: 5.0\n","benign TN: 6.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 75.0000  recall: 66.6667\n","malignant sensitivity: 66.6667  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 62.5000\n","malignant TP: 6.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 0.9980885982513428\n","minibatch AVG loss: 0.31909703016281127\n","Epoch: 5     train index of 5 minibatch: 2      time used: 0.7781574726104736\n","minibatch AVG loss: 0.9570839330554008\n","Epoch: 5     train index of 5 minibatch: 3      time used: 0.7776637077331543\n","minibatch AVG loss: 1.007977584004402\n","\n","Epoch: 5  train \n","Loss: 0.6811  Acc: 73.9130\n","benign precision: 71.4286  recall: 68.9655\n","benign sensitivity: 68.9655  specificity: 79.4872\n","benign FPR: 20.5128  NPV: 77.5000\n","benign TP: 20.0\n","benign TN: 31.0\n","benign FP: 8.0\n","benign FN: 9.0\n","malignant precision: 77.5000  recall: 79.4872\n","malignant sensitivity: 79.4872  specificity: 68.9655\n","malignant FPR: 31.0345  NPV: 71.4286\n","malignant TP: 31.0\n","malignant TN: 20.0\n","malignant FP: 9.0\n","malignant FN: 8.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 1658.9247  Acc: 56.2500\n","benign precision: 50.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 55.5556\n","benign FPR: 44.4444  NPV: 62.5000\n","benign TP: 4.0\n","benign TN: 5.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 62.5000  recall: 55.5556\n","malignant sensitivity: 55.5556  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 50.0000\n","malignant TP: 5.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 1.0190775394439697\n","minibatch AVG loss: 1.1383545756340028\n","Epoch: 6     train index of 5 minibatch: 2      time used: 0.7815444469451904\n","minibatch AVG loss: 0.7062064826488494\n","Epoch: 6     train index of 5 minibatch: 3      time used: 0.7848315238952637\n","minibatch AVG loss: 0.8373259603977203\n","\n","Epoch: 6  train \n","Loss: 0.8352  Acc: 68.1159\n","benign precision: 62.5000  recall: 68.9655\n","benign sensitivity: 68.9655  specificity: 69.2308\n","benign FPR: 30.7692  NPV: 75.0000\n","benign TP: 20.0\n","benign TN: 27.0\n","benign FP: 12.0\n","benign FN: 9.0\n","malignant precision: 75.0000  recall: 69.2308\n","malignant sensitivity: 69.2308  specificity: 68.9655\n","malignant FPR: 31.0345  NPV: 62.5000\n","malignant TP: 27.0\n","malignant TN: 20.0\n","malignant FP: 9.0\n","malignant FN: 12.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.5464  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 1.0173368453979492\n","minibatch AVG loss: 0.5561710208654403\n","Epoch: 7     train index of 5 minibatch: 2      time used: 0.7822577953338623\n","minibatch AVG loss: 0.5366761356592178\n","Epoch: 7     train index of 5 minibatch: 3      time used: 0.7876496315002441\n","minibatch AVG loss: 0.5640563428401947\n","\n","Epoch: 7  train \n","Loss: 0.5278  Acc: 76.8116\n","benign precision: 75.0000  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 80.0000\n","benign TP: 21.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 8.0\n","malignant precision: 80.0000  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 75.0000\n","malignant TP: 32.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 461.4512  Acc: 62.5000\n","benign precision: 60.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 63.6364\n","benign TP: 3.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 4.0\n","malignant precision: 63.6364  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 60.0000\n","malignant TP: 7.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 1.0104575157165527\n","minibatch AVG loss: 0.39451711624860764\n","Epoch: 8     train index of 5 minibatch: 2      time used: 0.7892241477966309\n","minibatch AVG loss: 0.8643442034721375\n","Epoch: 8     train index of 5 minibatch: 3      time used: 0.7895288467407227\n","minibatch AVG loss: 0.8273880302906036\n","\n","Epoch: 8  train \n","Loss: 0.6674  Acc: 71.0145\n","benign precision: 68.9655  recall: 66.6667\n","benign sensitivity: 66.6667  specificity: 76.3158\n","benign FPR: 23.6842  NPV: 74.3590\n","benign TP: 20.0\n","benign TN: 29.0\n","benign FP: 9.0\n","benign FN: 10.0\n","malignant precision: 74.3590  recall: 76.3158\n","malignant sensitivity: 76.3158  specificity: 66.6667\n","malignant FPR: 33.3333  NPV: 68.9655\n","malignant TP: 29.0\n","malignant TN: 20.0\n","malignant FP: 10.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 1088.0547  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 1.0232276916503906\n","minibatch AVG loss: 1.0639311447739601\n","Epoch: 9     train index of 5 minibatch: 2      time used: 0.7931275367736816\n","minibatch AVG loss: 0.3344750788062811\n","Epoch: 9     train index of 5 minibatch: 3      time used: 0.7893168926239014\n","minibatch AVG loss: 0.6179397702217102\n","\n","Epoch: 9  train \n","Loss: 0.6028  Acc: 72.4638\n","benign precision: 70.0000  recall: 70.0000\n","benign sensitivity: 70.0000  specificity: 76.3158\n","benign FPR: 23.6842  NPV: 76.3158\n","benign TP: 21.0\n","benign TN: 29.0\n","benign FP: 9.0\n","benign FN: 9.0\n","malignant precision: 76.3158  recall: 76.3158\n","malignant sensitivity: 76.3158  specificity: 70.0000\n","malignant FPR: 30.0000  NPV: 70.0000\n","malignant TP: 29.0\n","malignant TN: 21.0\n","malignant FP: 9.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 379.6534  Acc: 62.5000\n","benign precision: 57.1429  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 66.6667\n","benign FPR: 33.3333  NPV: 66.6667\n","benign TP: 4.0\n","benign TN: 6.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 66.6667  recall: 66.6667\n","malignant sensitivity: 66.6667  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 57.1429\n","malignant TP: 6.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 1.0082602500915527\n","minibatch AVG loss: 0.5530046820640564\n","Epoch: 10     train index of 5 minibatch: 2      time used: 0.7967770099639893\n","minibatch AVG loss: 0.7334378525614739\n","Epoch: 10     train index of 5 minibatch: 3      time used: 0.7920718193054199\n","minibatch AVG loss: 0.7320470623672009\n","\n","Epoch: 10  train \n","Loss: 0.6790  Acc: 71.0145\n","benign precision: 70.3704  recall: 63.3333\n","benign sensitivity: 63.3333  specificity: 78.9474\n","benign FPR: 21.0526  NPV: 73.1707\n","benign TP: 19.0\n","benign TN: 30.0\n","benign FP: 8.0\n","benign FN: 11.0\n","malignant precision: 73.1707  recall: 78.9474\n","malignant sensitivity: 78.9474  specificity: 63.3333\n","malignant FPR: 36.6667  NPV: 70.3704\n","malignant TP: 30.0\n","malignant TN: 19.0\n","malignant FP: 11.0\n","malignant FN: 8.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.6842  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 1.010758876800537\n","minibatch AVG loss: 0.8879028618335724\n","Epoch: 11     train index of 5 minibatch: 2      time used: 0.7892642021179199\n","minibatch AVG loss: 0.6425500303506851\n","Epoch: 11     train index of 5 minibatch: 3      time used: 0.7899665832519531\n","minibatch AVG loss: 1.2217056348919868\n","\n","Epoch: 11  train \n","Loss: 0.8088  Acc: 66.6667\n","benign precision: 64.2857  recall: 60.0000\n","benign sensitivity: 60.0000  specificity: 73.6842\n","benign FPR: 26.3158  NPV: 70.0000\n","benign TP: 18.0\n","benign TN: 28.0\n","benign FP: 10.0\n","benign FN: 12.0\n","malignant precision: 70.0000  recall: 73.6842\n","malignant sensitivity: 73.6842  specificity: 60.0000\n","malignant FPR: 40.0000  NPV: 64.2857\n","malignant TP: 28.0\n","malignant TN: 18.0\n","malignant FP: 12.0\n","malignant FN: 10.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 30.2910  Acc: 62.5000\n","benign precision: 60.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 63.6364\n","benign TP: 3.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 4.0\n","malignant precision: 63.6364  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 60.0000\n","malignant TP: 7.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 1.0142335891723633\n","minibatch AVG loss: 0.5982166364789009\n","Epoch: 12     train index of 5 minibatch: 2      time used: 0.7870454788208008\n","minibatch AVG loss: 0.8558812022209168\n","Epoch: 12     train index of 5 minibatch: 3      time used: 0.7863082885742188\n","minibatch AVG loss: 0.762281707674265\n","\n","Epoch: 12  train \n","Loss: 0.6628  Acc: 75.3623\n","benign precision: 70.9677  recall: 75.8621\n","benign sensitivity: 75.8621  specificity: 76.9231\n","benign FPR: 23.0769  NPV: 81.0811\n","benign TP: 22.0\n","benign TN: 30.0\n","benign FP: 9.0\n","benign FN: 7.0\n","malignant precision: 81.0811  recall: 76.9231\n","malignant sensitivity: 76.9231  specificity: 75.8621\n","malignant FPR: 24.1379  NPV: 70.9677\n","malignant TP: 30.0\n","malignant TN: 22.0\n","malignant FP: 7.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 63.7918  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 1.0169332027435303\n","minibatch AVG loss: 0.42891616821289064\n","Epoch: 13     train index of 5 minibatch: 2      time used: 0.7830753326416016\n","minibatch AVG loss: 0.8662535637617111\n","Epoch: 13     train index of 5 minibatch: 3      time used: 0.7853152751922607\n","minibatch AVG loss: 0.35663223676383493\n","\n","Epoch: 13  train \n","Loss: 0.5397  Acc: 78.2609\n","benign precision: 70.2703  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 71.7949\n","benign FPR: 28.2051  NPV: 90.3226\n","benign TP: 26.0\n","benign TN: 28.0\n","benign FP: 11.0\n","benign FN: 3.0\n","malignant precision: 90.3226  recall: 71.7949\n","malignant sensitivity: 71.7949  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 70.2703\n","malignant TP: 28.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 11.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 74.7008  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 1.007814884185791\n","minibatch AVG loss: 0.7950965490192174\n","Epoch: 14     train index of 5 minibatch: 2      time used: 0.7860119342803955\n","minibatch AVG loss: 0.6948118790984154\n","Epoch: 14     train index of 5 minibatch: 3      time used: 0.782902717590332\n","minibatch AVG loss: 0.19632456740364432\n","\n","Epoch: 14  train \n","Loss: 0.5021  Acc: 79.7101\n","benign precision: 81.4815  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 80.4878\n","benign TP: 22.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 8.0\n","malignant precision: 80.4878  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 81.4815\n","malignant TP: 33.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 8.9956  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 1.010451078414917\n","minibatch AVG loss: 0.73791763484478\n","Epoch: 15     train index of 5 minibatch: 2      time used: 0.790799617767334\n","minibatch AVG loss: 0.5113128185272217\n","Epoch: 15     train index of 5 minibatch: 3      time used: 0.782719612121582\n","minibatch AVG loss: 0.5321767352521419\n","\n","Epoch: 15  train \n","Loss: 0.5355  Acc: 79.7101\n","benign precision: 80.7692  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 80.9524\n","benign TP: 21.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 8.0\n","malignant precision: 80.9524  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 80.7692\n","malignant TP: 34.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 12.0025  Acc: 75.0000\n","benign precision: 71.4286  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 77.7778\n","benign TP: 5.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 77.7778  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 71.4286\n","malignant TP: 7.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 1.0118520259857178\n","minibatch AVG loss: 0.7728991866111755\n","Epoch: 16     train index of 5 minibatch: 2      time used: 0.7826783657073975\n","minibatch AVG loss: 0.7310189306735992\n","Epoch: 16     train index of 5 minibatch: 3      time used: 0.78023362159729\n","minibatch AVG loss: 0.5551122456789017\n","\n","Epoch: 16  train \n","Loss: 0.6106  Acc: 75.3623\n","benign precision: 71.8750  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 76.3158\n","benign FPR: 23.6842  NPV: 80.5556\n","benign TP: 23.0\n","benign TN: 29.0\n","benign FP: 9.0\n","benign FN: 7.0\n","malignant precision: 80.5556  recall: 76.3158\n","malignant sensitivity: 76.3158  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 71.8750\n","malignant TP: 29.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.8664  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 1.0161635875701904\n","minibatch AVG loss: 0.718121375143528\n","Epoch: 17     train index of 5 minibatch: 2      time used: 0.7834343910217285\n","minibatch AVG loss: 0.8190535321831703\n","Epoch: 17     train index of 5 minibatch: 3      time used: 0.7825164794921875\n","minibatch AVG loss: 0.4268814243376255\n","\n","Epoch: 17  train \n","Loss: 0.6587  Acc: 72.4638\n","benign precision: 67.7419  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 74.3590\n","benign FPR: 25.6410  NPV: 78.3784\n","benign TP: 21.0\n","benign TN: 29.0\n","benign FP: 10.0\n","benign FN: 8.0\n","malignant precision: 78.3784  recall: 74.3590\n","malignant sensitivity: 74.3590  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 67.7419\n","malignant TP: 29.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 10.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 2581.6933  Acc: 68.7500\n","benign precision: 75.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 66.6667\n","benign TP: 3.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 66.6667  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 75.0000\n","malignant TP: 8.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 0.9982824325561523\n","minibatch AVG loss: 0.310449455678463\n","Epoch: 18     train index of 5 minibatch: 2      time used: 0.7761678695678711\n","minibatch AVG loss: 0.2748803060501814\n","Epoch: 18     train index of 5 minibatch: 3      time used: 0.7754900455474854\n","minibatch AVG loss: 0.6582048781216144\n","\n","Epoch: 18  train \n","Loss: 0.4167  Acc: 81.1594\n","benign precision: 80.0000  recall: 80.0000\n","benign sensitivity: 80.0000  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 84.2105\n","benign TP: 24.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 6.0\n","malignant precision: 84.2105  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 80.0000\n","malignant FPR: 20.0000  NPV: 80.0000\n","malignant TP: 32.0\n","malignant TN: 24.0\n","malignant FP: 6.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 80.7731  Acc: 68.7500\n","benign precision: 75.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 66.6667\n","benign TP: 3.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 66.6667  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 75.0000\n","malignant TP: 8.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 1.0061345100402832\n","minibatch AVG loss: 0.21234738193452357\n","Epoch: 19     train index of 5 minibatch: 2      time used: 0.775338888168335\n","minibatch AVG loss: 0.7148546421900391\n","Epoch: 19     train index of 5 minibatch: 3      time used: 0.7736554145812988\n","minibatch AVG loss: 0.4935320973396301\n","\n","Epoch: 19  train \n","Loss: 0.4346  Acc: 81.1594\n","benign precision: 77.4194  recall: 82.7586\n","benign sensitivity: 82.7586  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 86.4865\n","benign TP: 24.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 5.0\n","malignant precision: 86.4865  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 82.7586\n","malignant FPR: 17.2414  NPV: 77.4194\n","malignant TP: 32.0\n","malignant TN: 24.0\n","malignant FP: 5.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 31.9467  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 1.0048613548278809\n","minibatch AVG loss: 0.7574868284165859\n","Epoch: 20     train index of 5 minibatch: 2      time used: 0.7742078304290771\n","minibatch AVG loss: 0.4513305380940437\n","Epoch: 20     train index of 5 minibatch: 3      time used: 0.7735445499420166\n","minibatch AVG loss: 0.736120730638504\n","\n","Epoch: 20  train \n","Loss: 0.6103  Acc: 73.9130\n","benign precision: 74.0741  recall: 66.6667\n","benign sensitivity: 66.6667  specificity: 81.5789\n","benign FPR: 18.4211  NPV: 75.6098\n","benign TP: 20.0\n","benign TN: 31.0\n","benign FP: 7.0\n","benign FN: 10.0\n","malignant precision: 75.6098  recall: 81.5789\n","malignant sensitivity: 81.5789  specificity: 66.6667\n","malignant FPR: 33.3333  NPV: 74.0741\n","malignant TP: 31.0\n","malignant TN: 20.0\n","malignant FP: 10.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 910.9110  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 1.0059595108032227\n","minibatch AVG loss: 0.9051134616136551\n","Epoch: 21     train index of 5 minibatch: 2      time used: 0.7735946178436279\n","minibatch AVG loss: 0.3652901330962777\n","Epoch: 21     train index of 5 minibatch: 3      time used: 0.7746832370758057\n","minibatch AVG loss: 0.4887133829295635\n","\n","Epoch: 21  train \n","Loss: 0.5578  Acc: 81.1594\n","benign precision: 77.4194  recall: 82.7586\n","benign sensitivity: 82.7586  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 86.4865\n","benign TP: 24.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 5.0\n","malignant precision: 86.4865  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 82.7586\n","malignant FPR: 17.2414  NPV: 77.4194\n","malignant TP: 32.0\n","malignant TN: 24.0\n","malignant FP: 5.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 566.6646  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 1.0046944618225098\n","minibatch AVG loss: 0.2774053843226284\n","Epoch: 22     train index of 5 minibatch: 2      time used: 0.7750012874603271\n","minibatch AVG loss: 0.7952829122543335\n","Epoch: 22     train index of 5 minibatch: 3      time used: 0.7773261070251465\n","minibatch AVG loss: 0.18288081958889962\n","\n","Epoch: 22  train \n","Loss: 0.3962  Acc: 84.0580\n","benign precision: 85.7143  recall: 80.0000\n","benign sensitivity: 80.0000  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 85.0000\n","benign TP: 24.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 6.0\n","malignant precision: 85.0000  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 80.0000\n","malignant FPR: 20.0000  NPV: 85.7143\n","malignant TP: 34.0\n","malignant TN: 24.0\n","malignant FP: 6.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.5892  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 1.0132062435150146\n","minibatch AVG loss: 0.3945703819394112\n","Epoch: 23     train index of 5 minibatch: 2      time used: 0.7739167213439941\n","minibatch AVG loss: 0.22857104428112507\n","Epoch: 23     train index of 5 minibatch: 3      time used: 0.7742512226104736\n","minibatch AVG loss: 0.5578062813729048\n","\n","Epoch: 23  train \n","Loss: 0.3636  Acc: 82.6087\n","benign precision: 85.1852  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 82.9268\n","benign TP: 23.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 7.0\n","malignant precision: 82.9268  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 85.1852\n","malignant TP: 34.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 10.0271  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 1.0143396854400635\n","minibatch AVG loss: 0.754798898100853\n","Epoch: 24     train index of 5 minibatch: 2      time used: 0.7730281352996826\n","minibatch AVG loss: 0.2941738348454237\n","Epoch: 24     train index of 5 minibatch: 3      time used: 0.77685546875\n","minibatch AVG loss: 0.9463278591632843\n","\n","Epoch: 24  train \n","Loss: 0.5967  Acc: 76.8116\n","benign precision: 77.7778  recall: 70.0000\n","benign sensitivity: 70.0000  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 78.0488\n","benign TP: 21.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 9.0\n","malignant precision: 78.0488  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 70.0000\n","malignant FPR: 30.0000  NPV: 77.7778\n","malignant TP: 32.0\n","malignant TN: 21.0\n","malignant FP: 9.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.6300  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 1.009516954421997\n","minibatch AVG loss: 0.24124940037727355\n","Epoch: 25     train index of 5 minibatch: 2      time used: 0.7726960182189941\n","minibatch AVG loss: 0.2797332316637039\n","Epoch: 25     train index of 5 minibatch: 3      time used: 0.7699525356292725\n","minibatch AVG loss: 0.37870469242334365\n","\n","Epoch: 25  train \n","Loss: 0.2982  Acc: 84.0580\n","benign precision: 83.3333  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 86.8421\n","benign TP: 25.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 5.0\n","malignant precision: 86.8421  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 83.3333\n","malignant TP: 33.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 46.6655  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 1.0101943016052246\n","minibatch AVG loss: 0.29699127897620203\n","Epoch: 26     train index of 5 minibatch: 2      time used: 0.7753386497497559\n","minibatch AVG loss: 0.39655220732092855\n","Epoch: 26     train index of 5 minibatch: 3      time used: 0.7740769386291504\n","minibatch AVG loss: 0.3551987797021866\n","\n","Epoch: 26  train \n","Loss: 0.3231  Acc: 88.4058\n","benign precision: 92.5926  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 87.8049\n","benign TP: 25.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 5.0\n","malignant precision: 87.8049  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 92.5926\n","malignant TP: 36.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 166.9528  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 1.0071089267730713\n","minibatch AVG loss: 1.118881344795227\n","Epoch: 27     train index of 5 minibatch: 2      time used: 0.7746455669403076\n","minibatch AVG loss: 0.5743764586746692\n","Epoch: 27     train index of 5 minibatch: 3      time used: 0.7763786315917969\n","minibatch AVG loss: 0.33781723976135253\n","\n","Epoch: 27  train \n","Loss: 0.5901  Acc: 72.4638\n","benign precision: 70.3704  recall: 65.5172\n","benign sensitivity: 65.5172  specificity: 79.4872\n","benign FPR: 20.5128  NPV: 75.6098\n","benign TP: 19.0\n","benign TN: 31.0\n","benign FP: 8.0\n","benign FN: 10.0\n","malignant precision: 75.6098  recall: 79.4872\n","malignant sensitivity: 79.4872  specificity: 65.5172\n","malignant FPR: 34.4828  NPV: 70.3704\n","malignant TP: 31.0\n","malignant TN: 19.0\n","malignant FP: 10.0\n","malignant FN: 8.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 1.2741  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 1.000748872756958\n","minibatch AVG loss: 0.9231176227331161\n","Epoch: 28     train index of 5 minibatch: 2      time used: 0.7744600772857666\n","minibatch AVG loss: 0.5070841997861862\n","Epoch: 28     train index of 5 minibatch: 3      time used: 0.7744259834289551\n","minibatch AVG loss: 0.8365982115268707\n","\n","Epoch: 28  train \n","Loss: 0.7196  Acc: 73.9130\n","benign precision: 74.0741  recall: 66.6667\n","benign sensitivity: 66.6667  specificity: 81.5789\n","benign FPR: 18.4211  NPV: 75.6098\n","benign TP: 20.0\n","benign TN: 31.0\n","benign FP: 7.0\n","benign FN: 10.0\n","malignant precision: 75.6098  recall: 81.5789\n","malignant sensitivity: 81.5789  specificity: 66.6667\n","malignant FPR: 33.3333  NPV: 74.0741\n","malignant TP: 31.0\n","malignant TN: 20.0\n","malignant FP: 10.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 517.5823  Acc: 68.7500\n","benign precision: 75.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 66.6667\n","benign TP: 3.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 66.6667  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 75.0000\n","malignant TP: 8.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 0.997011661529541\n","minibatch AVG loss: 0.2989155787974596\n","Epoch: 29     train index of 5 minibatch: 2      time used: 0.7757329940795898\n","minibatch AVG loss: 0.5082917554304004\n","Epoch: 29     train index of 5 minibatch: 3      time used: 0.7814984321594238\n","minibatch AVG loss: 0.49044696167111396\n","\n","Epoch: 29  train \n","Loss: 0.4410  Acc: 81.1594\n","benign precision: 74.2857  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 76.9231\n","benign FPR: 23.0769  NPV: 90.9091\n","benign TP: 26.0\n","benign TN: 30.0\n","benign FP: 9.0\n","benign FN: 3.0\n","malignant precision: 90.9091  recall: 76.9231\n","malignant sensitivity: 76.9231  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 74.2857\n","malignant TP: 30.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.5460  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 1.0073676109313965\n","minibatch AVG loss: 0.4131104789674282\n","Epoch: 30     train index of 5 minibatch: 2      time used: 0.7774825096130371\n","minibatch AVG loss: 0.41294232696527616\n","Epoch: 30     train index of 5 minibatch: 3      time used: 0.774437427520752\n","minibatch AVG loss: 0.6952643968164921\n","\n","Epoch: 30  train \n","Loss: 0.4736  Acc: 81.1594\n","benign precision: 82.1429  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 82.5000\n","benign TP: 23.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 7.0\n","malignant precision: 82.5000  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 82.1429\n","malignant TP: 33.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.2490  Acc: 87.5000\n","benign precision: 85.7143  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 88.8889\n","benign TP: 6.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 88.8889  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 85.7143\n","malignant TP: 8.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 1.0048675537109375\n","minibatch AVG loss: 1.0608161866664887\n","Epoch: 31     train index of 5 minibatch: 2      time used: 0.7790431976318359\n","minibatch AVG loss: 0.11325421333312988\n","Epoch: 31     train index of 5 minibatch: 3      time used: 0.7788629531860352\n","minibatch AVG loss: 0.4958044677972794\n","\n","Epoch: 31  train \n","Loss: 0.5389  Acc: 78.2609\n","benign precision: 78.5714  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 80.0000\n","benign TP: 22.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 8.0\n","malignant precision: 80.0000  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 78.5714\n","malignant TP: 32.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.7810  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 1.0073130130767822\n","minibatch AVG loss: 0.4620082229375839\n","Epoch: 32     train index of 5 minibatch: 2      time used: 0.7773258686065674\n","minibatch AVG loss: 0.17873464673757553\n","Epoch: 32     train index of 5 minibatch: 3      time used: 0.7782495021820068\n","minibatch AVG loss: 0.6238136053085327\n","\n","Epoch: 32  train \n","Loss: 0.4083  Acc: 78.2609\n","benign precision: 76.6667  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 81.5789\n","benign FPR: 18.4211  NPV: 81.5789\n","benign TP: 23.0\n","benign TN: 31.0\n","benign FP: 7.0\n","benign FN: 7.0\n","malignant precision: 81.5789  recall: 81.5789\n","malignant sensitivity: 81.5789  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 76.6667\n","malignant TP: 31.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 831.7723  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 1.003824234008789\n","minibatch AVG loss: 0.6884267754852772\n","Epoch: 33     train index of 5 minibatch: 2      time used: 0.7806723117828369\n","minibatch AVG loss: 0.12142549157142639\n","Epoch: 33     train index of 5 minibatch: 3      time used: 0.7783048152923584\n","minibatch AVG loss: 0.6556641308590769\n","\n","Epoch: 33  train \n","Loss: 0.5105  Acc: 78.2609\n","benign precision: 77.7778  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 80.4878\n","benign TP: 21.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 8.0\n","malignant precision: 80.4878  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 77.7778\n","malignant TP: 33.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 291.7508  Acc: 68.7500\n","benign precision: 75.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 66.6667\n","benign TP: 3.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 66.6667  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 75.0000\n","malignant TP: 8.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 0.9990153312683105\n","minibatch AVG loss: 0.09087957851588727\n","Epoch: 34     train index of 5 minibatch: 2      time used: 0.7787754535675049\n","minibatch AVG loss: 0.7429322302341461\n","Epoch: 34     train index of 5 minibatch: 3      time used: 0.7760741710662842\n","minibatch AVG loss: 0.4649028401821852\n","\n","Epoch: 34  train \n","Loss: 0.3933  Acc: 91.3043\n","benign precision: 90.3226  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 94.5946\n","benign TP: 28.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.5946  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 90.3226\n","malignant TP: 35.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.4790  Acc: 81.2500\n","benign precision: 75.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 87.5000\n","benign TP: 6.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 87.5000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 75.0000\n","malignant TP: 7.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 1.0081768035888672\n","minibatch AVG loss: 0.5025598280597479\n","Epoch: 35     train index of 5 minibatch: 2      time used: 0.778578519821167\n","minibatch AVG loss: 0.5675670087337494\n","Epoch: 35     train index of 5 minibatch: 3      time used: 0.7781214714050293\n","minibatch AVG loss: 0.19389493856579065\n","\n","Epoch: 35  train \n","Loss: 0.3767  Acc: 84.0580\n","benign precision: 78.7879  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 91.4286\n","benign TP: 26.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 3.0\n","malignant precision: 91.4286  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 78.7879\n","malignant TP: 32.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.4683  Acc: 87.5000\n","benign precision: 85.7143  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 88.8889\n","benign TP: 6.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 88.8889  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 85.7143\n","malignant TP: 8.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 1.0128309726715088\n","minibatch AVG loss: 0.8644182813353837\n","Epoch: 36     train index of 5 minibatch: 2      time used: 0.7818310260772705\n","minibatch AVG loss: 0.31812992356717584\n","Epoch: 36     train index of 5 minibatch: 3      time used: 0.7810416221618652\n","minibatch AVG loss: 0.9529461145401001\n","\n","Epoch: 36  train \n","Loss: 0.6525  Acc: 73.9130\n","benign precision: 68.5714  recall: 80.0000\n","benign sensitivity: 80.0000  specificity: 71.0526\n","benign FPR: 28.9474  NPV: 81.8182\n","benign TP: 24.0\n","benign TN: 27.0\n","benign FP: 11.0\n","benign FN: 6.0\n","malignant precision: 81.8182  recall: 71.0526\n","malignant sensitivity: 71.0526  specificity: 80.0000\n","malignant FPR: 20.0000  NPV: 68.5714\n","malignant TP: 27.0\n","malignant TN: 24.0\n","malignant FP: 6.0\n","malignant FN: 11.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.4426  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 1.002098560333252\n","minibatch AVG loss: 0.4959048077464104\n","Epoch: 37     train index of 5 minibatch: 2      time used: 0.777320384979248\n","minibatch AVG loss: 0.19398467615246773\n","Epoch: 37     train index of 5 minibatch: 3      time used: 0.7791321277618408\n","minibatch AVG loss: 0.15063157975673674\n","\n","Epoch: 37  train \n","Loss: 0.2798  Acc: 89.8551\n","benign precision: 90.0000  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 92.1053\n","benign TP: 27.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 92.1053  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 90.0000\n","malignant TP: 35.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.3668  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 0.997760534286499\n","minibatch AVG loss: 0.6396785661578178\n","Epoch: 38     train index of 5 minibatch: 2      time used: 0.7775371074676514\n","minibatch AVG loss: 0.3197360701858997\n","Epoch: 38     train index of 5 minibatch: 3      time used: 0.7778089046478271\n","minibatch AVG loss: 0.10930809527635574\n","\n","Epoch: 38  train \n","Loss: 0.3878  Acc: 85.5072\n","benign precision: 83.3333  recall: 86.2069\n","benign sensitivity: 86.2069  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 89.4737\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 4.0\n","malignant precision: 89.4737  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 86.2069\n","malignant FPR: 13.7931  NPV: 83.3333\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 4.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.6544  Acc: 75.0000\n","benign precision: 100.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 69.2308\n","benign TP: 3.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 69.2308  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 1.015099048614502\n","minibatch AVG loss: 0.366667889803648\n","Epoch: 39     train index of 5 minibatch: 2      time used: 0.781440258026123\n","minibatch AVG loss: 0.40095503041520714\n","Epoch: 39     train index of 5 minibatch: 3      time used: 0.7779161930084229\n","minibatch AVG loss: 0.3490343937650323\n","\n","Epoch: 39  train \n","Loss: 0.4016  Acc: 85.5072\n","benign precision: 83.8710  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 89.1892\n","benign TP: 26.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 4.0\n","malignant precision: 89.1892  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 83.8710\n","malignant TP: 33.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.3025  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 1.0085608959197998\n","minibatch AVG loss: 0.4011215683072805\n","Epoch: 40     train index of 5 minibatch: 2      time used: 0.7769002914428711\n","minibatch AVG loss: 0.17695913836359978\n","Epoch: 40     train index of 5 minibatch: 3      time used: 0.7788197994232178\n","minibatch AVG loss: 0.4185126803815365\n","\n","Epoch: 40  train \n","Loss: 0.3431  Acc: 82.6087\n","benign precision: 84.6154  recall: 75.8621\n","benign sensitivity: 75.8621  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 83.3333\n","benign TP: 22.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 7.0\n","malignant precision: 83.3333  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 75.8621\n","malignant FPR: 24.1379  NPV: 84.6154\n","malignant TP: 35.0\n","malignant TN: 22.0\n","malignant FP: 7.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.4699  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 0.9937915802001953\n","minibatch AVG loss: 0.28152433447539804\n","Epoch: 41     train index of 5 minibatch: 2      time used: 0.7796621322631836\n","minibatch AVG loss: 0.6860367625020445\n","Epoch: 41     train index of 5 minibatch: 3      time used: 0.7747793197631836\n","minibatch AVG loss: 0.10485997458454221\n","\n","Epoch: 41  train \n","Loss: 0.3144  Acc: 86.9565\n","benign precision: 89.2857  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 87.5000\n","benign TP: 25.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 5.0\n","malignant precision: 87.5000  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 89.2857\n","malignant TP: 35.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 64.5832  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 1.0031371116638184\n","minibatch AVG loss: 0.5255067013204098\n","Epoch: 42     train index of 5 minibatch: 2      time used: 0.775824785232544\n","minibatch AVG loss: 0.2742401693016291\n","Epoch: 42     train index of 5 minibatch: 3      time used: 0.775930643081665\n","minibatch AVG loss: 0.6198887504637242\n","\n","Epoch: 42  train \n","Loss: 0.4391  Acc: 82.6087\n","benign precision: 88.0000  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 81.3953\n","benign TP: 22.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 8.0\n","malignant precision: 81.3953  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 88.0000\n","malignant TP: 35.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 157.2608  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 1.0057616233825684\n","minibatch AVG loss: 0.5597825091332197\n","Epoch: 43     train index of 5 minibatch: 2      time used: 0.7772877216339111\n","minibatch AVG loss: 0.5360522889532149\n","Epoch: 43     train index of 5 minibatch: 3      time used: 0.7802915573120117\n","minibatch AVG loss: 0.8686472997069359\n","\n","Epoch: 43  train \n","Loss: 0.7305  Acc: 79.7101\n","benign precision: 79.3103  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 82.0513\n","benign TP: 23.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 7.0\n","malignant precision: 82.0513  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 79.3103\n","malignant TP: 32.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.3934  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 1.0108046531677246\n","minibatch AVG loss: 0.10479531064629555\n","Epoch: 44     train index of 5 minibatch: 2      time used: 0.7784812450408936\n","minibatch AVG loss: 0.5018340066075325\n","Epoch: 44     train index of 5 minibatch: 3      time used: 0.7789251804351807\n","minibatch AVG loss: 0.21686718557029963\n","\n","Epoch: 44  train \n","Loss: 0.2402  Acc: 86.9565\n","benign precision: 89.2857  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 87.5000\n","benign TP: 25.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 5.0\n","malignant precision: 87.5000  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 89.2857\n","malignant TP: 35.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 26.4711  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 1.0043134689331055\n","minibatch AVG loss: 0.040614502504467964\n","Epoch: 45     train index of 5 minibatch: 2      time used: 0.7770726680755615\n","minibatch AVG loss: 0.49332650899887087\n","Epoch: 45     train index of 5 minibatch: 3      time used: 0.7773158550262451\n","minibatch AVG loss: 0.5151101843453944\n","\n","Epoch: 45  train \n","Loss: 0.3367  Acc: 86.9565\n","benign precision: 92.3077  recall: 80.0000\n","benign sensitivity: 80.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 85.7143\n","benign TP: 24.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 6.0\n","malignant precision: 85.7143  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 80.0000\n","malignant FPR: 20.0000  NPV: 92.3077\n","malignant TP: 36.0\n","malignant TN: 24.0\n","malignant FP: 6.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.4288  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 1.004002571105957\n","minibatch AVG loss: 0.24877679757773877\n","Epoch: 46     train index of 5 minibatch: 2      time used: 0.7766814231872559\n","minibatch AVG loss: 0.25148398280143736\n","Epoch: 46     train index of 5 minibatch: 3      time used: 0.7762596607208252\n","minibatch AVG loss: 0.3409158200025558\n","\n","Epoch: 46  train \n","Loss: 0.2984  Acc: 86.9565\n","benign precision: 86.6667  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 89.4737\n","benign TP: 26.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 4.0\n","malignant precision: 89.4737  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 86.6667\n","malignant TP: 34.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.4202  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 1.0044090747833252\n","minibatch AVG loss: 0.4350720651447773\n","Epoch: 47     train index of 5 minibatch: 2      time used: 0.7764897346496582\n","minibatch AVG loss: 0.08453275859355927\n","Epoch: 47     train index of 5 minibatch: 3      time used: 0.7758762836456299\n","minibatch AVG loss: 0.4434047698974609\n","\n","Epoch: 47  train \n","Loss: 0.3433  Acc: 81.1594\n","benign precision: 79.3103  recall: 79.3103\n","benign sensitivity: 79.3103  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 84.6154\n","benign TP: 23.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 6.0\n","malignant precision: 84.6154  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 79.3103\n","malignant FPR: 20.6897  NPV: 79.3103\n","malignant TP: 33.0\n","malignant TN: 23.0\n","malignant FP: 6.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.5288  Acc: 75.0000\n","benign precision: 100.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 69.2308\n","benign TP: 3.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 69.2308  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 1.0034685134887695\n","minibatch AVG loss: 0.5106301851570606\n","Epoch: 48     train index of 5 minibatch: 2      time used: 0.7793331146240234\n","minibatch AVG loss: 0.25923250541090964\n","Epoch: 48     train index of 5 minibatch: 3      time used: 0.7779159545898438\n","minibatch AVG loss: 0.26003462300868707\n","\n","Epoch: 48  train \n","Loss: 0.3134  Acc: 82.6087\n","benign precision: 82.1429  recall: 79.3103\n","benign sensitivity: 79.3103  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 85.0000\n","benign TP: 23.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 6.0\n","malignant precision: 85.0000  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 79.3103\n","malignant FPR: 20.6897  NPV: 82.1429\n","malignant TP: 34.0\n","malignant TN: 23.0\n","malignant FP: 6.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.2534  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 1.0057809352874756\n","minibatch AVG loss: 0.14988429993391036\n","Epoch: 49     train index of 5 minibatch: 2      time used: 0.7774693965911865\n","minibatch AVG loss: 0.05589677281677723\n","Epoch: 49     train index of 5 minibatch: 3      time used: 0.7755105495452881\n","minibatch AVG loss: 0.5216778606176377\n","\n","Epoch: 49  train \n","Loss: 0.2120  Acc: 88.4058\n","benign precision: 82.3529  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 97.0588\n","benign TP: 28.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 1.0\n","malignant precision: 97.0588  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 82.3529\n","malignant TP: 33.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.2944  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 1.0021841526031494\n","minibatch AVG loss: 0.37494342774152756\n","Epoch: 50     train index of 5 minibatch: 2      time used: 0.7768838405609131\n","minibatch AVG loss: 0.14648336991667749\n","Epoch: 50     train index of 5 minibatch: 3      time used: 0.7820239067077637\n","minibatch AVG loss: 0.9328735113143921\n","\n","Epoch: 50  train \n","Loss: 0.4277  Acc: 85.5072\n","benign precision: 83.3333  recall: 86.2069\n","benign sensitivity: 86.2069  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 89.4737\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 4.0\n","malignant precision: 89.4737  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 86.2069\n","malignant FPR: 13.7931  NPV: 83.3333\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 4.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 140.7380  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Training complete in 2m 42s\n","Best epoch idx:  37\n","Best epoch train Acc: 89.855072\n","Best epoch val Acc: 93.750000\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2jo4ttKTTNL","executionInfo":{"status":"ok","timestamp":1651156677352,"user_tz":-480,"elapsed":583288,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"1891b681-a658-46a1-b52a-cb2678d4ef5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='swin_b_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['swin_base_patch4_window7_224',\n"," 'swin_base_patch4_window7_224_in22k',\n"," 'swin_base_patch4_window12_384',\n"," 'swin_base_patch4_window12_384_in22k',\n"," 'swin_large_patch4_window7_224',\n"," 'swin_large_patch4_window7_224_in22k',\n"," 'swin_large_patch4_window12_384',\n"," 'swin_large_patch4_window12_384_in22k',\n"," 'swin_small_patch4_window7_224',\n"," 'swin_tiny_patch4_window7_224']\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth\" to /root/.cache/torch/hub/checkpoints/swin_base_patch4_window12_384_22kto1k.pth\n","test model output： tensor([[-0.0427, -0.0044]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 128, 96, 96]           6,272\n","         LayerNorm-2            [-1, 9216, 128]             256\n","        PatchEmbed-3            [-1, 9216, 128]               0\n","           Dropout-4            [-1, 9216, 128]               0\n","         LayerNorm-5            [-1, 9216, 128]             256\n","            Linear-6             [-1, 144, 384]          49,536\n","           Softmax-7          [-1, 4, 144, 144]               0\n","           Dropout-8          [-1, 4, 144, 144]               0\n","            Linear-9             [-1, 144, 128]          16,512\n","          Dropout-10             [-1, 144, 128]               0\n","  WindowAttention-11             [-1, 144, 128]               0\n","         Identity-12            [-1, 9216, 128]               0\n","        LayerNorm-13            [-1, 9216, 128]             256\n","           Linear-14            [-1, 9216, 512]          66,048\n","             GELU-15            [-1, 9216, 512]               0\n","          Dropout-16            [-1, 9216, 512]               0\n","           Linear-17            [-1, 9216, 128]          65,664\n","          Dropout-18            [-1, 9216, 128]               0\n","              Mlp-19            [-1, 9216, 128]               0\n","         Identity-20            [-1, 9216, 128]               0\n","SwinTransformerBlock-21            [-1, 9216, 128]               0\n","        LayerNorm-22            [-1, 9216, 128]             256\n","           Linear-23             [-1, 144, 384]          49,536\n","          Softmax-24          [-1, 4, 144, 144]               0\n","          Dropout-25          [-1, 4, 144, 144]               0\n","           Linear-26             [-1, 144, 128]          16,512\n","          Dropout-27             [-1, 144, 128]               0\n","  WindowAttention-28             [-1, 144, 128]               0\n","         DropPath-29            [-1, 9216, 128]               0\n","        LayerNorm-30            [-1, 9216, 128]             256\n","           Linear-31            [-1, 9216, 512]          66,048\n","             GELU-32            [-1, 9216, 512]               0\n","          Dropout-33            [-1, 9216, 512]               0\n","           Linear-34            [-1, 9216, 128]          65,664\n","          Dropout-35            [-1, 9216, 128]               0\n","              Mlp-36            [-1, 9216, 128]               0\n","         DropPath-37            [-1, 9216, 128]               0\n","SwinTransformerBlock-38            [-1, 9216, 128]               0\n","        LayerNorm-39            [-1, 2304, 512]           1,024\n","           Linear-40            [-1, 2304, 256]         131,072\n","     PatchMerging-41            [-1, 2304, 256]               0\n","       BasicLayer-42            [-1, 2304, 256]               0\n","        LayerNorm-43            [-1, 2304, 256]             512\n","           Linear-44             [-1, 144, 768]         197,376\n","          Softmax-45          [-1, 8, 144, 144]               0\n","          Dropout-46          [-1, 8, 144, 144]               0\n","           Linear-47             [-1, 144, 256]          65,792\n","          Dropout-48             [-1, 144, 256]               0\n","  WindowAttention-49             [-1, 144, 256]               0\n","         DropPath-50            [-1, 2304, 256]               0\n","        LayerNorm-51            [-1, 2304, 256]             512\n","           Linear-52           [-1, 2304, 1024]         263,168\n","             GELU-53           [-1, 2304, 1024]               0\n","          Dropout-54           [-1, 2304, 1024]               0\n","           Linear-55            [-1, 2304, 256]         262,400\n","          Dropout-56            [-1, 2304, 256]               0\n","              Mlp-57            [-1, 2304, 256]               0\n","         DropPath-58            [-1, 2304, 256]               0\n","SwinTransformerBlock-59            [-1, 2304, 256]               0\n","        LayerNorm-60            [-1, 2304, 256]             512\n","           Linear-61             [-1, 144, 768]         197,376\n","          Softmax-62          [-1, 8, 144, 144]               0\n","          Dropout-63          [-1, 8, 144, 144]               0\n","           Linear-64             [-1, 144, 256]          65,792\n","          Dropout-65             [-1, 144, 256]               0\n","  WindowAttention-66             [-1, 144, 256]               0\n","         DropPath-67            [-1, 2304, 256]               0\n","        LayerNorm-68            [-1, 2304, 256]             512\n","           Linear-69           [-1, 2304, 1024]         263,168\n","             GELU-70           [-1, 2304, 1024]               0\n","          Dropout-71           [-1, 2304, 1024]               0\n","           Linear-72            [-1, 2304, 256]         262,400\n","          Dropout-73            [-1, 2304, 256]               0\n","              Mlp-74            [-1, 2304, 256]               0\n","         DropPath-75            [-1, 2304, 256]               0\n","SwinTransformerBlock-76            [-1, 2304, 256]               0\n","        LayerNorm-77            [-1, 576, 1024]           2,048\n","           Linear-78             [-1, 576, 512]         524,288\n","     PatchMerging-79             [-1, 576, 512]               0\n","       BasicLayer-80             [-1, 576, 512]               0\n","        LayerNorm-81             [-1, 576, 512]           1,024\n","           Linear-82            [-1, 144, 1536]         787,968\n","          Softmax-83         [-1, 16, 144, 144]               0\n","          Dropout-84         [-1, 16, 144, 144]               0\n","           Linear-85             [-1, 144, 512]         262,656\n","          Dropout-86             [-1, 144, 512]               0\n","  WindowAttention-87             [-1, 144, 512]               0\n","         DropPath-88             [-1, 576, 512]               0\n","        LayerNorm-89             [-1, 576, 512]           1,024\n","           Linear-90            [-1, 576, 2048]       1,050,624\n","             GELU-91            [-1, 576, 2048]               0\n","          Dropout-92            [-1, 576, 2048]               0\n","           Linear-93             [-1, 576, 512]       1,049,088\n","          Dropout-94             [-1, 576, 512]               0\n","              Mlp-95             [-1, 576, 512]               0\n","         DropPath-96             [-1, 576, 512]               0\n","SwinTransformerBlock-97             [-1, 576, 512]               0\n","        LayerNorm-98             [-1, 576, 512]           1,024\n","           Linear-99            [-1, 144, 1536]         787,968\n","         Softmax-100         [-1, 16, 144, 144]               0\n","         Dropout-101         [-1, 16, 144, 144]               0\n","          Linear-102             [-1, 144, 512]         262,656\n","         Dropout-103             [-1, 144, 512]               0\n"," WindowAttention-104             [-1, 144, 512]               0\n","        DropPath-105             [-1, 576, 512]               0\n","       LayerNorm-106             [-1, 576, 512]           1,024\n","          Linear-107            [-1, 576, 2048]       1,050,624\n","            GELU-108            [-1, 576, 2048]               0\n","         Dropout-109            [-1, 576, 2048]               0\n","          Linear-110             [-1, 576, 512]       1,049,088\n","         Dropout-111             [-1, 576, 512]               0\n","             Mlp-112             [-1, 576, 512]               0\n","        DropPath-113             [-1, 576, 512]               0\n","SwinTransformerBlock-114             [-1, 576, 512]               0\n","       LayerNorm-115             [-1, 576, 512]           1,024\n","          Linear-116            [-1, 144, 1536]         787,968\n","         Softmax-117         [-1, 16, 144, 144]               0\n","         Dropout-118         [-1, 16, 144, 144]               0\n","          Linear-119             [-1, 144, 512]         262,656\n","         Dropout-120             [-1, 144, 512]               0\n"," WindowAttention-121             [-1, 144, 512]               0\n","        DropPath-122             [-1, 576, 512]               0\n","       LayerNorm-123             [-1, 576, 512]           1,024\n","          Linear-124            [-1, 576, 2048]       1,050,624\n","            GELU-125            [-1, 576, 2048]               0\n","         Dropout-126            [-1, 576, 2048]               0\n","          Linear-127             [-1, 576, 512]       1,049,088\n","         Dropout-128             [-1, 576, 512]               0\n","             Mlp-129             [-1, 576, 512]               0\n","        DropPath-130             [-1, 576, 512]               0\n","SwinTransformerBlock-131             [-1, 576, 512]               0\n","       LayerNorm-132             [-1, 576, 512]           1,024\n","          Linear-133            [-1, 144, 1536]         787,968\n","         Softmax-134         [-1, 16, 144, 144]               0\n","         Dropout-135         [-1, 16, 144, 144]               0\n","          Linear-136             [-1, 144, 512]         262,656\n","         Dropout-137             [-1, 144, 512]               0\n"," WindowAttention-138             [-1, 144, 512]               0\n","        DropPath-139             [-1, 576, 512]               0\n","       LayerNorm-140             [-1, 576, 512]           1,024\n","          Linear-141            [-1, 576, 2048]       1,050,624\n","            GELU-142            [-1, 576, 2048]               0\n","         Dropout-143            [-1, 576, 2048]               0\n","          Linear-144             [-1, 576, 512]       1,049,088\n","         Dropout-145             [-1, 576, 512]               0\n","             Mlp-146             [-1, 576, 512]               0\n","        DropPath-147             [-1, 576, 512]               0\n","SwinTransformerBlock-148             [-1, 576, 512]               0\n","       LayerNorm-149             [-1, 576, 512]           1,024\n","          Linear-150            [-1, 144, 1536]         787,968\n","         Softmax-151         [-1, 16, 144, 144]               0\n","         Dropout-152         [-1, 16, 144, 144]               0\n","          Linear-153             [-1, 144, 512]         262,656\n","         Dropout-154             [-1, 144, 512]               0\n"," WindowAttention-155             [-1, 144, 512]               0\n","        DropPath-156             [-1, 576, 512]               0\n","       LayerNorm-157             [-1, 576, 512]           1,024\n","          Linear-158            [-1, 576, 2048]       1,050,624\n","            GELU-159            [-1, 576, 2048]               0\n","         Dropout-160            [-1, 576, 2048]               0\n","          Linear-161             [-1, 576, 512]       1,049,088\n","         Dropout-162             [-1, 576, 512]               0\n","             Mlp-163             [-1, 576, 512]               0\n","        DropPath-164             [-1, 576, 512]               0\n","SwinTransformerBlock-165             [-1, 576, 512]               0\n","       LayerNorm-166             [-1, 576, 512]           1,024\n","          Linear-167            [-1, 144, 1536]         787,968\n","         Softmax-168         [-1, 16, 144, 144]               0\n","         Dropout-169         [-1, 16, 144, 144]               0\n","          Linear-170             [-1, 144, 512]         262,656\n","         Dropout-171             [-1, 144, 512]               0\n"," WindowAttention-172             [-1, 144, 512]               0\n","        DropPath-173             [-1, 576, 512]               0\n","       LayerNorm-174             [-1, 576, 512]           1,024\n","          Linear-175            [-1, 576, 2048]       1,050,624\n","            GELU-176            [-1, 576, 2048]               0\n","         Dropout-177            [-1, 576, 2048]               0\n","          Linear-178             [-1, 576, 512]       1,049,088\n","         Dropout-179             [-1, 576, 512]               0\n","             Mlp-180             [-1, 576, 512]               0\n","        DropPath-181             [-1, 576, 512]               0\n","SwinTransformerBlock-182             [-1, 576, 512]               0\n","       LayerNorm-183             [-1, 576, 512]           1,024\n","          Linear-184            [-1, 144, 1536]         787,968\n","         Softmax-185         [-1, 16, 144, 144]               0\n","         Dropout-186         [-1, 16, 144, 144]               0\n","          Linear-187             [-1, 144, 512]         262,656\n","         Dropout-188             [-1, 144, 512]               0\n"," WindowAttention-189             [-1, 144, 512]               0\n","        DropPath-190             [-1, 576, 512]               0\n","       LayerNorm-191             [-1, 576, 512]           1,024\n","          Linear-192            [-1, 576, 2048]       1,050,624\n","            GELU-193            [-1, 576, 2048]               0\n","         Dropout-194            [-1, 576, 2048]               0\n","          Linear-195             [-1, 576, 512]       1,049,088\n","         Dropout-196             [-1, 576, 512]               0\n","             Mlp-197             [-1, 576, 512]               0\n","        DropPath-198             [-1, 576, 512]               0\n","SwinTransformerBlock-199             [-1, 576, 512]               0\n","       LayerNorm-200             [-1, 576, 512]           1,024\n","          Linear-201            [-1, 144, 1536]         787,968\n","         Softmax-202         [-1, 16, 144, 144]               0\n","         Dropout-203         [-1, 16, 144, 144]               0\n","          Linear-204             [-1, 144, 512]         262,656\n","         Dropout-205             [-1, 144, 512]               0\n"," WindowAttention-206             [-1, 144, 512]               0\n","        DropPath-207             [-1, 576, 512]               0\n","       LayerNorm-208             [-1, 576, 512]           1,024\n","          Linear-209            [-1, 576, 2048]       1,050,624\n","            GELU-210            [-1, 576, 2048]               0\n","         Dropout-211            [-1, 576, 2048]               0\n","          Linear-212             [-1, 576, 512]       1,049,088\n","         Dropout-213             [-1, 576, 512]               0\n","             Mlp-214             [-1, 576, 512]               0\n","        DropPath-215             [-1, 576, 512]               0\n","SwinTransformerBlock-216             [-1, 576, 512]               0\n","       LayerNorm-217             [-1, 576, 512]           1,024\n","          Linear-218            [-1, 144, 1536]         787,968\n","         Softmax-219         [-1, 16, 144, 144]               0\n","         Dropout-220         [-1, 16, 144, 144]               0\n","          Linear-221             [-1, 144, 512]         262,656\n","         Dropout-222             [-1, 144, 512]               0\n"," WindowAttention-223             [-1, 144, 512]               0\n","        DropPath-224             [-1, 576, 512]               0\n","       LayerNorm-225             [-1, 576, 512]           1,024\n","          Linear-226            [-1, 576, 2048]       1,050,624\n","            GELU-227            [-1, 576, 2048]               0\n","         Dropout-228            [-1, 576, 2048]               0\n","          Linear-229             [-1, 576, 512]       1,049,088\n","         Dropout-230             [-1, 576, 512]               0\n","             Mlp-231             [-1, 576, 512]               0\n","        DropPath-232             [-1, 576, 512]               0\n","SwinTransformerBlock-233             [-1, 576, 512]               0\n","       LayerNorm-234             [-1, 576, 512]           1,024\n","          Linear-235            [-1, 144, 1536]         787,968\n","         Softmax-236         [-1, 16, 144, 144]               0\n","         Dropout-237         [-1, 16, 144, 144]               0\n","          Linear-238             [-1, 144, 512]         262,656\n","         Dropout-239             [-1, 144, 512]               0\n"," WindowAttention-240             [-1, 144, 512]               0\n","        DropPath-241             [-1, 576, 512]               0\n","       LayerNorm-242             [-1, 576, 512]           1,024\n","          Linear-243            [-1, 576, 2048]       1,050,624\n","            GELU-244            [-1, 576, 2048]               0\n","         Dropout-245            [-1, 576, 2048]               0\n","          Linear-246             [-1, 576, 512]       1,049,088\n","         Dropout-247             [-1, 576, 512]               0\n","             Mlp-248             [-1, 576, 512]               0\n","        DropPath-249             [-1, 576, 512]               0\n","SwinTransformerBlock-250             [-1, 576, 512]               0\n","       LayerNorm-251             [-1, 576, 512]           1,024\n","          Linear-252            [-1, 144, 1536]         787,968\n","         Softmax-253         [-1, 16, 144, 144]               0\n","         Dropout-254         [-1, 16, 144, 144]               0\n","          Linear-255             [-1, 144, 512]         262,656\n","         Dropout-256             [-1, 144, 512]               0\n"," WindowAttention-257             [-1, 144, 512]               0\n","        DropPath-258             [-1, 576, 512]               0\n","       LayerNorm-259             [-1, 576, 512]           1,024\n","          Linear-260            [-1, 576, 2048]       1,050,624\n","            GELU-261            [-1, 576, 2048]               0\n","         Dropout-262            [-1, 576, 2048]               0\n","          Linear-263             [-1, 576, 512]       1,049,088\n","         Dropout-264             [-1, 576, 512]               0\n","             Mlp-265             [-1, 576, 512]               0\n","        DropPath-266             [-1, 576, 512]               0\n","SwinTransformerBlock-267             [-1, 576, 512]               0\n","       LayerNorm-268             [-1, 576, 512]           1,024\n","          Linear-269            [-1, 144, 1536]         787,968\n","         Softmax-270         [-1, 16, 144, 144]               0\n","         Dropout-271         [-1, 16, 144, 144]               0\n","          Linear-272             [-1, 144, 512]         262,656\n","         Dropout-273             [-1, 144, 512]               0\n"," WindowAttention-274             [-1, 144, 512]               0\n","        DropPath-275             [-1, 576, 512]               0\n","       LayerNorm-276             [-1, 576, 512]           1,024\n","          Linear-277            [-1, 576, 2048]       1,050,624\n","            GELU-278            [-1, 576, 2048]               0\n","         Dropout-279            [-1, 576, 2048]               0\n","          Linear-280             [-1, 576, 512]       1,049,088\n","         Dropout-281             [-1, 576, 512]               0\n","             Mlp-282             [-1, 576, 512]               0\n","        DropPath-283             [-1, 576, 512]               0\n","SwinTransformerBlock-284             [-1, 576, 512]               0\n","       LayerNorm-285             [-1, 576, 512]           1,024\n","          Linear-286            [-1, 144, 1536]         787,968\n","         Softmax-287         [-1, 16, 144, 144]               0\n","         Dropout-288         [-1, 16, 144, 144]               0\n","          Linear-289             [-1, 144, 512]         262,656\n","         Dropout-290             [-1, 144, 512]               0\n"," WindowAttention-291             [-1, 144, 512]               0\n","        DropPath-292             [-1, 576, 512]               0\n","       LayerNorm-293             [-1, 576, 512]           1,024\n","          Linear-294            [-1, 576, 2048]       1,050,624\n","            GELU-295            [-1, 576, 2048]               0\n","         Dropout-296            [-1, 576, 2048]               0\n","          Linear-297             [-1, 576, 512]       1,049,088\n","         Dropout-298             [-1, 576, 512]               0\n","             Mlp-299             [-1, 576, 512]               0\n","        DropPath-300             [-1, 576, 512]               0\n","SwinTransformerBlock-301             [-1, 576, 512]               0\n","       LayerNorm-302             [-1, 576, 512]           1,024\n","          Linear-303            [-1, 144, 1536]         787,968\n","         Softmax-304         [-1, 16, 144, 144]               0\n","         Dropout-305         [-1, 16, 144, 144]               0\n","          Linear-306             [-1, 144, 512]         262,656\n","         Dropout-307             [-1, 144, 512]               0\n"," WindowAttention-308             [-1, 144, 512]               0\n","        DropPath-309             [-1, 576, 512]               0\n","       LayerNorm-310             [-1, 576, 512]           1,024\n","          Linear-311            [-1, 576, 2048]       1,050,624\n","            GELU-312            [-1, 576, 2048]               0\n","         Dropout-313            [-1, 576, 2048]               0\n","          Linear-314             [-1, 576, 512]       1,049,088\n","         Dropout-315             [-1, 576, 512]               0\n","             Mlp-316             [-1, 576, 512]               0\n","        DropPath-317             [-1, 576, 512]               0\n","SwinTransformerBlock-318             [-1, 576, 512]               0\n","       LayerNorm-319             [-1, 576, 512]           1,024\n","          Linear-320            [-1, 144, 1536]         787,968\n","         Softmax-321         [-1, 16, 144, 144]               0\n","         Dropout-322         [-1, 16, 144, 144]               0\n","          Linear-323             [-1, 144, 512]         262,656\n","         Dropout-324             [-1, 144, 512]               0\n"," WindowAttention-325             [-1, 144, 512]               0\n","        DropPath-326             [-1, 576, 512]               0\n","       LayerNorm-327             [-1, 576, 512]           1,024\n","          Linear-328            [-1, 576, 2048]       1,050,624\n","            GELU-329            [-1, 576, 2048]               0\n","         Dropout-330            [-1, 576, 2048]               0\n","          Linear-331             [-1, 576, 512]       1,049,088\n","         Dropout-332             [-1, 576, 512]               0\n","             Mlp-333             [-1, 576, 512]               0\n","        DropPath-334             [-1, 576, 512]               0\n","SwinTransformerBlock-335             [-1, 576, 512]               0\n","       LayerNorm-336             [-1, 576, 512]           1,024\n","          Linear-337            [-1, 144, 1536]         787,968\n","         Softmax-338         [-1, 16, 144, 144]               0\n","         Dropout-339         [-1, 16, 144, 144]               0\n","          Linear-340             [-1, 144, 512]         262,656\n","         Dropout-341             [-1, 144, 512]               0\n"," WindowAttention-342             [-1, 144, 512]               0\n","        DropPath-343             [-1, 576, 512]               0\n","       LayerNorm-344             [-1, 576, 512]           1,024\n","          Linear-345            [-1, 576, 2048]       1,050,624\n","            GELU-346            [-1, 576, 2048]               0\n","         Dropout-347            [-1, 576, 2048]               0\n","          Linear-348             [-1, 576, 512]       1,049,088\n","         Dropout-349             [-1, 576, 512]               0\n","             Mlp-350             [-1, 576, 512]               0\n","        DropPath-351             [-1, 576, 512]               0\n","SwinTransformerBlock-352             [-1, 576, 512]               0\n","       LayerNorm-353             [-1, 576, 512]           1,024\n","          Linear-354            [-1, 144, 1536]         787,968\n","         Softmax-355         [-1, 16, 144, 144]               0\n","         Dropout-356         [-1, 16, 144, 144]               0\n","          Linear-357             [-1, 144, 512]         262,656\n","         Dropout-358             [-1, 144, 512]               0\n"," WindowAttention-359             [-1, 144, 512]               0\n","        DropPath-360             [-1, 576, 512]               0\n","       LayerNorm-361             [-1, 576, 512]           1,024\n","          Linear-362            [-1, 576, 2048]       1,050,624\n","            GELU-363            [-1, 576, 2048]               0\n","         Dropout-364            [-1, 576, 2048]               0\n","          Linear-365             [-1, 576, 512]       1,049,088\n","         Dropout-366             [-1, 576, 512]               0\n","             Mlp-367             [-1, 576, 512]               0\n","        DropPath-368             [-1, 576, 512]               0\n","SwinTransformerBlock-369             [-1, 576, 512]               0\n","       LayerNorm-370             [-1, 576, 512]           1,024\n","          Linear-371            [-1, 144, 1536]         787,968\n","         Softmax-372         [-1, 16, 144, 144]               0\n","         Dropout-373         [-1, 16, 144, 144]               0\n","          Linear-374             [-1, 144, 512]         262,656\n","         Dropout-375             [-1, 144, 512]               0\n"," WindowAttention-376             [-1, 144, 512]               0\n","        DropPath-377             [-1, 576, 512]               0\n","       LayerNorm-378             [-1, 576, 512]           1,024\n","          Linear-379            [-1, 576, 2048]       1,050,624\n","            GELU-380            [-1, 576, 2048]               0\n","         Dropout-381            [-1, 576, 2048]               0\n","          Linear-382             [-1, 576, 512]       1,049,088\n","         Dropout-383             [-1, 576, 512]               0\n","             Mlp-384             [-1, 576, 512]               0\n","        DropPath-385             [-1, 576, 512]               0\n","SwinTransformerBlock-386             [-1, 576, 512]               0\n","       LayerNorm-387            [-1, 144, 2048]           4,096\n","          Linear-388            [-1, 144, 1024]       2,097,152\n","    PatchMerging-389            [-1, 144, 1024]               0\n","      BasicLayer-390            [-1, 144, 1024]               0\n","       LayerNorm-391            [-1, 144, 1024]           2,048\n","          Linear-392            [-1, 144, 3072]       3,148,800\n","         Softmax-393         [-1, 32, 144, 144]               0\n","         Dropout-394         [-1, 32, 144, 144]               0\n","          Linear-395            [-1, 144, 1024]       1,049,600\n","         Dropout-396            [-1, 144, 1024]               0\n"," WindowAttention-397            [-1, 144, 1024]               0\n","        DropPath-398            [-1, 144, 1024]               0\n","       LayerNorm-399            [-1, 144, 1024]           2,048\n","          Linear-400            [-1, 144, 4096]       4,198,400\n","            GELU-401            [-1, 144, 4096]               0\n","         Dropout-402            [-1, 144, 4096]               0\n","          Linear-403            [-1, 144, 1024]       4,195,328\n","         Dropout-404            [-1, 144, 1024]               0\n","             Mlp-405            [-1, 144, 1024]               0\n","        DropPath-406            [-1, 144, 1024]               0\n","SwinTransformerBlock-407            [-1, 144, 1024]               0\n","       LayerNorm-408            [-1, 144, 1024]           2,048\n","          Linear-409            [-1, 144, 3072]       3,148,800\n","         Softmax-410         [-1, 32, 144, 144]               0\n","         Dropout-411         [-1, 32, 144, 144]               0\n","          Linear-412            [-1, 144, 1024]       1,049,600\n","         Dropout-413            [-1, 144, 1024]               0\n"," WindowAttention-414            [-1, 144, 1024]               0\n","        DropPath-415            [-1, 144, 1024]               0\n","       LayerNorm-416            [-1, 144, 1024]           2,048\n","          Linear-417            [-1, 144, 4096]       4,198,400\n","            GELU-418            [-1, 144, 4096]               0\n","         Dropout-419            [-1, 144, 4096]               0\n","          Linear-420            [-1, 144, 1024]       4,195,328\n","         Dropout-421            [-1, 144, 1024]               0\n","             Mlp-422            [-1, 144, 1024]               0\n","        DropPath-423            [-1, 144, 1024]               0\n","SwinTransformerBlock-424            [-1, 144, 1024]               0\n","      BasicLayer-425            [-1, 144, 1024]               0\n","       LayerNorm-426            [-1, 144, 1024]           2,048\n","AdaptiveAvgPool1d-427              [-1, 1024, 1]               0\n","          Linear-428                    [-1, 2]           2,050\n","================================================================\n","Total params: 86,681,730\n","Trainable params: 86,681,730\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1670.91\n","Params size (MB): 330.66\n","Estimated Total Size (MB): 2003.27\n","----------------------------------------------------------------\n","model : swin_b_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 2.7096199989318848\n","minibatch AVG loss: 0.7352073311805725\n","Epoch: 1     train index of 5 minibatch: 2      time used: 2.5164551734924316\n","minibatch AVG loss: 0.7399341702461243\n","Epoch: 1     train index of 5 minibatch: 3      time used: 2.530055284500122\n","minibatch AVG loss: 0.5622180223464965\n","\n","Epoch: 1  train \n","Loss: 0.6619  Acc: 56.5217\n","benign precision: 50.0000  recall: 20.6897\n","benign sensitivity: 20.6897  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 58.9286\n","benign TP: 6.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 23.0\n","malignant precision: 58.9286  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 20.6897\n","malignant FPR: 79.3103  NPV: 50.0000\n","malignant TP: 33.0\n","malignant TN: 6.0\n","malignant FP: 23.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.5732  Acc: 68.7500\n","benign precision: 100.0000  recall: 28.5714\n","benign sensitivity: 28.5714  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 64.2857\n","benign TP: 2.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 5.0\n","malignant precision: 64.2857  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 28.5714\n","malignant FPR: 71.4286  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 2.0\n","malignant FP: 5.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 2.7620317935943604\n","minibatch AVG loss: 0.5345485985279084\n","Epoch: 2     train index of 5 minibatch: 2      time used: 2.5819249153137207\n","minibatch AVG loss: 0.3888377070426941\n","Epoch: 2     train index of 5 minibatch: 3      time used: 2.5960350036621094\n","minibatch AVG loss: 0.48160170316696166\n","\n","Epoch: 2  train \n","Loss: 0.4497  Acc: 81.1594\n","benign precision: 100.0000  recall: 60.0000\n","benign sensitivity: 60.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 76.0000\n","benign TP: 18.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 12.0\n","malignant precision: 76.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 60.0000\n","malignant FPR: 40.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 18.0\n","malignant FP: 12.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.4283  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 2.856555700302124\n","minibatch AVG loss: 0.3094523370265961\n","Epoch: 3     train index of 5 minibatch: 2      time used: 2.6856868267059326\n","minibatch AVG loss: 0.3278944373130798\n","Epoch: 3     train index of 5 minibatch: 3      time used: 2.6871066093444824\n","minibatch AVG loss: 0.1897150918841362\n","\n","Epoch: 3  train \n","Loss: 0.2592  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.2143  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 2.9547719955444336\n","minibatch AVG loss: 0.18693777024745942\n","Epoch: 4     train index of 5 minibatch: 2      time used: 2.7961177825927734\n","minibatch AVG loss: 0.09592105448246002\n","Epoch: 4     train index of 5 minibatch: 3      time used: 2.7876267433166504\n","minibatch AVG loss: 0.10379887372255325\n","\n","Epoch: 4  train \n","Loss: 0.1298  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.4359\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.1028  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 3.039301633834839\n","minibatch AVG loss: 0.06508282497525215\n","Epoch: 5     train index of 5 minibatch: 2      time used: 2.8385491371154785\n","minibatch AVG loss: 0.04487128891050816\n","Epoch: 5     train index of 5 minibatch: 3      time used: 2.803614616394043\n","minibatch AVG loss: 0.06535793524235486\n","\n","Epoch: 5  train \n","Loss: 0.0539  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.0635  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 2.983466386795044\n","minibatch AVG loss: 0.05171684119850397\n","Epoch: 6     train index of 5 minibatch: 2      time used: 2.759767532348633\n","minibatch AVG loss: 0.026398024708032607\n","Epoch: 6     train index of 5 minibatch: 3      time used: 2.738381862640381\n","minibatch AVG loss: 0.02699548862874508\n","\n","Epoch: 6  train \n","Loss: 0.0333  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.0432  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 2.911590099334717\n","minibatch AVG loss: 0.016607085801661016\n","Epoch: 7     train index of 5 minibatch: 2      time used: 2.703538179397583\n","minibatch AVG loss: 0.016635891422629355\n","Epoch: 7     train index of 5 minibatch: 3      time used: 2.6666183471679688\n","minibatch AVG loss: 0.023491998203098774\n","\n","Epoch: 7  train \n","Loss: 0.0186  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.0362  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 2.862732172012329\n","minibatch AVG loss: 0.01889198091812432\n","Epoch: 8     train index of 5 minibatch: 2      time used: 2.656601667404175\n","minibatch AVG loss: 0.025403132289648057\n","Epoch: 8     train index of 5 minibatch: 3      time used: 2.6301050186157227\n","minibatch AVG loss: 0.041549814073368906\n","\n","Epoch: 8  train \n","Loss: 0.0262  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.0348  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 2.842799186706543\n","minibatch AVG loss: 0.07925992235541343\n","Epoch: 9     train index of 5 minibatch: 2      time used: 2.6421751976013184\n","minibatch AVG loss: 0.006383732007816434\n","Epoch: 9     train index of 5 minibatch: 3      time used: 2.6229169368743896\n","minibatch AVG loss: 0.01846170462667942\n","\n","Epoch: 9  train \n","Loss: 0.0309  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.0273  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 2.8298356533050537\n","minibatch AVG loss: 0.008427143841981889\n","Epoch: 10     train index of 5 minibatch: 2      time used: 2.63680362701416\n","minibatch AVG loss: 0.02837879229336977\n","Epoch: 10     train index of 5 minibatch: 3      time used: 2.6423280239105225\n","minibatch AVG loss: 0.009697286458685995\n","\n","Epoch: 10  train \n","Loss: 0.0145  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.0255  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 2.85178804397583\n","minibatch AVG loss: 0.008015903364866971\n","Epoch: 11     train index of 5 minibatch: 2      time used: 2.672774314880371\n","minibatch AVG loss: 0.005701917409896851\n","Epoch: 11     train index of 5 minibatch: 3      time used: 2.6576497554779053\n","minibatch AVG loss: 0.006328235799446702\n","\n","Epoch: 11  train \n","Loss: 0.0071  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.0236  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 2.8984568119049072\n","minibatch AVG loss: 0.016868506092578173\n","Epoch: 12     train index of 5 minibatch: 2      time used: 2.6855247020721436\n","minibatch AVG loss: 0.007266719196923077\n","Epoch: 12     train index of 5 minibatch: 3      time used: 2.6894092559814453\n","minibatch AVG loss: 0.008912668051198124\n","\n","Epoch: 12  train \n","Loss: 0.0168  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.0210  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 2.914947271347046\n","minibatch AVG loss: 0.024012930365279316\n","Epoch: 13     train index of 5 minibatch: 2      time used: 2.717315196990967\n","minibatch AVG loss: 0.00800320403650403\n","Epoch: 13     train index of 5 minibatch: 3      time used: 2.701916456222534\n","minibatch AVG loss: 0.009448547940701246\n","\n","Epoch: 13  train \n","Loss: 0.0131  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.0207  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 2.9096059799194336\n","minibatch AVG loss: 0.006467982567846775\n","Epoch: 14     train index of 5 minibatch: 2      time used: 2.7164549827575684\n","minibatch AVG loss: 0.01925270778592676\n","Epoch: 14     train index of 5 minibatch: 3      time used: 2.693286657333374\n","minibatch AVG loss: 0.015178688243031502\n","\n","Epoch: 14  train \n","Loss: 0.0161  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.0184  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 2.9085497856140137\n","minibatch AVG loss: 0.002918010973371565\n","Epoch: 15     train index of 5 minibatch: 2      time used: 2.699538469314575\n","minibatch AVG loss: 0.01280780613888055\n","Epoch: 15     train index of 5 minibatch: 3      time used: 2.6904797554016113\n","minibatch AVG loss: 0.0447930061025545\n","\n","Epoch: 15  train \n","Loss: 0.0185  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.0244  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 2.9006338119506836\n","minibatch AVG loss: 0.016294137062504887\n","Epoch: 16     train index of 5 minibatch: 2      time used: 2.696781873703003\n","minibatch AVG loss: 0.12466414319351315\n","Epoch: 16     train index of 5 minibatch: 3      time used: 2.6835110187530518\n","minibatch AVG loss: 0.012495490768924356\n","\n","Epoch: 16  train \n","Loss: 0.0482  Acc: 97.1014\n","benign precision: 96.6667  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.6667\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.0280  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 2.8923158645629883\n","minibatch AVG loss: 0.11817280312534421\n","Epoch: 17     train index of 5 minibatch: 2      time used: 2.695192813873291\n","minibatch AVG loss: 0.0969238007441163\n","Epoch: 17     train index of 5 minibatch: 3      time used: 2.673271656036377\n","minibatch AVG loss: 0.07351713804528118\n","\n","Epoch: 17  train \n","Loss: 0.0844  Acc: 94.2029\n","benign precision: 93.5484  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 97.2973\n","benign TP: 29.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 93.5484\n","malignant TP: 36.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.0320  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 2.8924946784973145\n","minibatch AVG loss: 0.019481833651661874\n","Epoch: 18     train index of 5 minibatch: 2      time used: 2.695814847946167\n","minibatch AVG loss: 0.005102025659289211\n","Epoch: 18     train index of 5 minibatch: 3      time used: 2.6744823455810547\n","minibatch AVG loss: 0.025637586368247867\n","\n","Epoch: 18  train \n","Loss: 0.0150  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.0252  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 2.884774684906006\n","minibatch AVG loss: 0.03629101801197976\n","Epoch: 19     train index of 5 minibatch: 2      time used: 2.6957828998565674\n","minibatch AVG loss: 0.006816860707476735\n","Epoch: 19     train index of 5 minibatch: 3      time used: 2.675992488861084\n","minibatch AVG loss: 0.003300899360328913\n","\n","Epoch: 19  train \n","Loss: 0.0138  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.0238  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 2.89228892326355\n","minibatch AVG loss: 0.0028566831606440246\n","Epoch: 20     train index of 5 minibatch: 2      time used: 2.6943697929382324\n","minibatch AVG loss: 0.002663516695611179\n","Epoch: 20     train index of 5 minibatch: 3      time used: 2.689016819000244\n","minibatch AVG loss: 0.006960856867954135\n","\n","Epoch: 20  train \n","Loss: 0.0051  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.0231  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 2.8952221870422363\n","minibatch AVG loss: 0.0027410083683207633\n","Epoch: 21     train index of 5 minibatch: 2      time used: 2.69075608253479\n","minibatch AVG loss: 0.0035359990084543823\n","Epoch: 21     train index of 5 minibatch: 3      time used: 2.680461883544922\n","minibatch AVG loss: 0.009740862622857093\n","\n","Epoch: 21  train \n","Loss: 0.0050  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.0236  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 2.885836362838745\n","minibatch AVG loss: 0.0052923493552953\n","Epoch: 22     train index of 5 minibatch: 2      time used: 2.6887149810791016\n","minibatch AVG loss: 0.005517419055104255\n","Epoch: 22     train index of 5 minibatch: 3      time used: 2.6786603927612305\n","minibatch AVG loss: 0.0017795070307329297\n","\n","Epoch: 22  train \n","Loss: 0.0050  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.0230  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 2.8889105319976807\n","minibatch AVG loss: 0.0055595062440261245\n","Epoch: 23     train index of 5 minibatch: 2      time used: 2.7009904384613037\n","minibatch AVG loss: 0.001969376183114946\n","Epoch: 23     train index of 5 minibatch: 3      time used: 2.682626962661743\n","minibatch AVG loss: 0.0029011727077886463\n","\n","Epoch: 23  train \n","Loss: 0.0033  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.0232  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 2.90044903755188\n","minibatch AVG loss: 0.002496536250691861\n","Epoch: 24     train index of 5 minibatch: 2      time used: 2.697050094604492\n","minibatch AVG loss: 0.01797484268900007\n","Epoch: 24     train index of 5 minibatch: 3      time used: 2.6783688068389893\n","minibatch AVG loss: 0.0032098519150167704\n","\n","Epoch: 24  train \n","Loss: 0.0079  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.0290  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 2.89517879486084\n","minibatch AVG loss: 0.012387207802385091\n","Epoch: 25     train index of 5 minibatch: 2      time used: 2.6968657970428467\n","minibatch AVG loss: 0.004557134071364999\n","Epoch: 25     train index of 5 minibatch: 3      time used: 2.6779050827026367\n","minibatch AVG loss: 0.0044244193937629465\n","\n","Epoch: 25  train \n","Loss: 0.0066  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.0243  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 2.899923086166382\n","minibatch AVG loss: 0.004215731401927769\n","Epoch: 26     train index of 5 minibatch: 2      time used: 2.688091278076172\n","minibatch AVG loss: 0.0022502606734633446\n","Epoch: 26     train index of 5 minibatch: 3      time used: 2.6845364570617676\n","minibatch AVG loss: 0.0026486972579732536\n","\n","Epoch: 26  train \n","Loss: 0.0029  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.0183  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 2.887852668762207\n","minibatch AVG loss: 0.002274022693745792\n","Epoch: 27     train index of 5 minibatch: 2      time used: 2.695207118988037\n","minibatch AVG loss: 0.00305303493514657\n","Epoch: 27     train index of 5 minibatch: 3      time used: 2.6787829399108887\n","minibatch AVG loss: 0.0022411082638427613\n","\n","Epoch: 27  train \n","Loss: 0.0023  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.0175  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 2.9039528369903564\n","minibatch AVG loss: 0.00874912035651505\n","Epoch: 28     train index of 5 minibatch: 2      time used: 2.704005479812622\n","minibatch AVG loss: 0.0018819811695721\n","Epoch: 28     train index of 5 minibatch: 3      time used: 2.6749258041381836\n","minibatch AVG loss: 0.0023342132568359374\n","\n","Epoch: 28  train \n","Loss: 0.0088  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.0149  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 2.894549608230591\n","minibatch AVG loss: 0.0032572813099250196\n","Epoch: 29     train index of 5 minibatch: 2      time used: 2.695747137069702\n","minibatch AVG loss: 0.008966274128761142\n","Epoch: 29     train index of 5 minibatch: 3      time used: 2.6803009510040283\n","minibatch AVG loss: 0.011276749358512462\n","\n","Epoch: 29  train \n","Loss: 0.0071  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.0137  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 2.902799367904663\n","minibatch AVG loss: 0.0023476692964322864\n","Epoch: 30     train index of 5 minibatch: 2      time used: 2.699781894683838\n","minibatch AVG loss: 0.0020979651482775806\n","Epoch: 30     train index of 5 minibatch: 3      time used: 2.6811206340789795\n","minibatch AVG loss: 0.0013856878271326424\n","\n","Epoch: 30  train \n","Loss: 0.0024  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.0128  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 2.892488956451416\n","minibatch AVG loss: 0.0020209627225995066\n","Epoch: 31     train index of 5 minibatch: 2      time used: 2.6913208961486816\n","minibatch AVG loss: 0.012026262888684868\n","Epoch: 31     train index of 5 minibatch: 3      time used: 2.689378261566162\n","minibatch AVG loss: 0.003075358527712524\n","\n","Epoch: 31  train \n","Loss: 0.0051  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.0123  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 2.8961002826690674\n","minibatch AVG loss: 0.011186720803380013\n","Epoch: 32     train index of 5 minibatch: 2      time used: 2.6888136863708496\n","minibatch AVG loss: 0.0022274722810834645\n","Epoch: 32     train index of 5 minibatch: 3      time used: 2.6849899291992188\n","minibatch AVG loss: 0.004988796729594469\n","\n","Epoch: 32  train \n","Loss: 0.0056  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.0119  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 2.91208553314209\n","minibatch AVG loss: 0.005133449810091406\n","Epoch: 33     train index of 5 minibatch: 2      time used: 2.699780225753784\n","minibatch AVG loss: 0.002991995820775628\n","Epoch: 33     train index of 5 minibatch: 3      time used: 2.6879525184631348\n","minibatch AVG loss: 0.0016587189398705958\n","\n","Epoch: 33  train \n","Loss: 0.0029  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.0116  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 2.8940136432647705\n","minibatch AVG loss: 0.0052826571045443416\n","Epoch: 34     train index of 5 minibatch: 2      time used: 2.703395128250122\n","minibatch AVG loss: 0.009633858315646648\n","Epoch: 34     train index of 5 minibatch: 3      time used: 2.6916756629943848\n","minibatch AVG loss: 0.0018411466444376856\n","\n","Epoch: 34  train \n","Loss: 0.0057  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.0114  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 2.9005446434020996\n","minibatch AVG loss: 0.001234388502780348\n","Epoch: 35     train index of 5 minibatch: 2      time used: 2.6940770149230957\n","minibatch AVG loss: 0.0037755253026261924\n","Epoch: 35     train index of 5 minibatch: 3      time used: 2.6964826583862305\n","minibatch AVG loss: 0.0008116081240586937\n","\n","Epoch: 35  train \n","Loss: 0.0022  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.0116  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 2.8880858421325684\n","minibatch AVG loss: 0.004132045537699014\n","Epoch: 36     train index of 5 minibatch: 2      time used: 2.698326826095581\n","minibatch AVG loss: 0.0029174740659072993\n","Epoch: 36     train index of 5 minibatch: 3      time used: 2.6839845180511475\n","minibatch AVG loss: 0.0012027728953398764\n","\n","Epoch: 36  train \n","Loss: 0.0026  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.0117  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 2.8915395736694336\n","minibatch AVG loss: 0.0038105751853436233\n","Epoch: 37     train index of 5 minibatch: 2      time used: 2.6848385334014893\n","minibatch AVG loss: 0.002652019541710615\n","Epoch: 37     train index of 5 minibatch: 3      time used: 2.6786606311798096\n","minibatch AVG loss: 0.003988866438157856\n","\n","Epoch: 37  train \n","Loss: 0.0031  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.0116  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 2.9070327281951904\n","minibatch AVG loss: 0.0016590616025496275\n","Epoch: 38     train index of 5 minibatch: 2      time used: 2.6919944286346436\n","minibatch AVG loss: 0.0015983163844794034\n","Epoch: 38     train index of 5 minibatch: 3      time used: 2.661114454269409\n","minibatch AVG loss: 0.007572015002369881\n","\n","Epoch: 38  train \n","Loss: 0.0032  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.0116  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 2.8892266750335693\n","minibatch AVG loss: 0.00300816532690078\n","Epoch: 39     train index of 5 minibatch: 2      time used: 2.681368112564087\n","minibatch AVG loss: 0.0028275471762754024\n","Epoch: 39     train index of 5 minibatch: 3      time used: 2.6758339405059814\n","minibatch AVG loss: 0.0015128474682569505\n","\n","Epoch: 39  train \n","Loss: 0.0022  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.0113  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 2.8905131816864014\n","minibatch AVG loss: 0.0020126077928580345\n","Epoch: 40     train index of 5 minibatch: 2      time used: 2.6902387142181396\n","minibatch AVG loss: 0.0012460546335205437\n","Epoch: 40     train index of 5 minibatch: 3      time used: 2.6699748039245605\n","minibatch AVG loss: 0.0018320139730349182\n","\n","Epoch: 40  train \n","Loss: 0.0021  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.0114  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 2.8601794242858887\n","minibatch AVG loss: 0.00339548175688833\n","Epoch: 41     train index of 5 minibatch: 2      time used: 2.671347141265869\n","minibatch AVG loss: 0.0024685759097337725\n","Epoch: 41     train index of 5 minibatch: 3      time used: 2.675647735595703\n","minibatch AVG loss: 0.002326429740060121\n","\n","Epoch: 41  train \n","Loss: 0.0028  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.0111  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 2.875925064086914\n","minibatch AVG loss: 0.0020213077310472726\n","Epoch: 42     train index of 5 minibatch: 2      time used: 2.680851697921753\n","minibatch AVG loss: 0.003695205319672823\n","Epoch: 42     train index of 5 minibatch: 3      time used: 2.6714649200439453\n","minibatch AVG loss: 0.0027027493109926582\n","\n","Epoch: 42  train \n","Loss: 0.0029  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.0110  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 2.8745062351226807\n","minibatch AVG loss: 0.0028215103840921072\n","Epoch: 43     train index of 5 minibatch: 2      time used: 2.6844303607940674\n","minibatch AVG loss: 0.002146763540804386\n","Epoch: 43     train index of 5 minibatch: 3      time used: 2.6764369010925293\n","minibatch AVG loss: 0.003749334497842938\n","\n","Epoch: 43  train \n","Loss: 0.0028  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.0109  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 2.896385669708252\n","minibatch AVG loss: 0.0009972449392080308\n","Epoch: 44     train index of 5 minibatch: 2      time used: 2.684004545211792\n","minibatch AVG loss: 0.005314807849936187\n","Epoch: 44     train index of 5 minibatch: 3      time used: 2.666703939437866\n","minibatch AVG loss: 0.005284581356681884\n","\n","Epoch: 44  train \n","Loss: 0.0035  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.0108  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 2.8899807929992676\n","minibatch AVG loss: 0.0018277398659847677\n","Epoch: 45     train index of 5 minibatch: 2      time used: 2.68399715423584\n","minibatch AVG loss: 0.0018577839538920671\n","Epoch: 45     train index of 5 minibatch: 3      time used: 2.692208766937256\n","minibatch AVG loss: 0.007266317144967616\n","\n","Epoch: 45  train \n","Loss: 0.0035  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.0109  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 2.9053590297698975\n","minibatch AVG loss: 0.001997015392407775\n","Epoch: 46     train index of 5 minibatch: 2      time used: 2.6898674964904785\n","minibatch AVG loss: 0.0019915690179914238\n","Epoch: 46     train index of 5 minibatch: 3      time used: 2.6708250045776367\n","minibatch AVG loss: 0.0025983060244470836\n","\n","Epoch: 46  train \n","Loss: 0.0021  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.0109  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 2.89140248298645\n","minibatch AVG loss: 0.001910259376745671\n","Epoch: 47     train index of 5 minibatch: 2      time used: 2.7049357891082764\n","minibatch AVG loss: 0.012344941520132125\n","Epoch: 47     train index of 5 minibatch: 3      time used: 2.6857173442840576\n","minibatch AVG loss: 0.0038060309598222375\n","\n","Epoch: 47  train \n","Loss: 0.0054  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.0108  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 2.9075918197631836\n","minibatch AVG loss: 0.0019307301205117255\n","Epoch: 48     train index of 5 minibatch: 2      time used: 2.709174633026123\n","minibatch AVG loss: 0.0027924665948376058\n","Epoch: 48     train index of 5 minibatch: 3      time used: 2.6837258338928223\n","minibatch AVG loss: 0.01013199946610257\n","\n","Epoch: 48  train \n","Loss: 0.0045  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.0107  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 2.8971028327941895\n","minibatch AVG loss: 0.0024925598641857504\n","Epoch: 49     train index of 5 minibatch: 2      time used: 2.698695659637451\n","minibatch AVG loss: 0.0019679056480526925\n","Epoch: 49     train index of 5 minibatch: 3      time used: 2.6845614910125732\n","minibatch AVG loss: 0.002443891204893589\n","\n","Epoch: 49  train \n","Loss: 0.0023  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.0106  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 2.908477544784546\n","minibatch AVG loss: 0.0024198892409913243\n","Epoch: 50     train index of 5 minibatch: 2      time used: 2.6993298530578613\n","minibatch AVG loss: 0.003781838924624026\n","Epoch: 50     train index of 5 minibatch: 3      time used: 2.68823504447937\n","minibatch AVG loss: 0.006013953511137515\n","\n","Epoch: 50  train \n","Loss: 0.0039  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.0106  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 8m 35s\n","Best epoch idx:  50\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_swin_b_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx swin_b_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"markdown","metadata":{"id":"9A2NqlIySHZo"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9a_q06xzYk-0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651156716906,"user_tz":-480,"elapsed":39561,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"1f895981-e7fb-4960-bd84-66d016f3b335"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[-0.0660, -0.3694]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","model loaded\n","model : ViT_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ViT_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.3913605213165283\n","minibatch AVG loss: 0.09088364329363685\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.21163129806518555\n","minibatch AVG loss: 0.0009090366191230714\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.21138691902160645\n","minibatch AVG loss: 0.0018524654428802023\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.21084809303283691\n","minibatch AVG loss: 0.02139533595982357\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.21231460571289062\n","minibatch AVG loss: 0.0016460985429148423\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.2115480899810791\n","minibatch AVG loss: 0.0001718972113849304\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.211409330368042\n","minibatch AVG loss: 0.3723823448557596\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.2144606113433838\n","minibatch AVG loss: 0.02895282608733396\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.21376943588256836\n","minibatch AVG loss: 0.0073993839265313\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.21608257293701172\n","minibatch AVG loss: 0.055877193046035244\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.21402335166931152\n","minibatch AVG loss: 0.0073031947016716\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.2148888111114502\n","minibatch AVG loss: 0.018990924385434482\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.21698546409606934\n","minibatch AVG loss: 0.04222289582030499\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.21642017364501953\n","minibatch AVG loss: 0.16099161887832453\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.21930289268493652\n","minibatch AVG loss: 0.00041292459209216756\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.21828508377075195\n","minibatch AVG loss: 0.001071294306893833\n","\n","Epoch:  test \n","Loss: 0.0508  Acc: 97.5000\n","benign precision: 97.2973  recall: 97.2973\n","benign sensitivity: 97.2973  specificity: 97.6744\n","benign FPR: 2.3256  NPV: 97.6744\n","benign TP: 36.0\n","benign TN: 42.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.6744  recall: 97.6744\n","malignant sensitivity: 97.6744  specificity: 97.2973\n","malignant FPR: 2.7027  NPV: 97.2973\n","malignant TP: 42.0\n","malignant TN: 36.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","Testing complete in 0m 31s\n"]}],"source":["!python Test.py --model_idx ViT_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xz7xxOutaaQ0","executionInfo":{"status":"ok","timestamp":1651156756186,"user_tz":-480,"elapsed":39285,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"a0768e68-56c1-40a3-a04e-d7adb402e3a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[-0.9102,  0.1864]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","model loaded\n","model : ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.3778986930847168\n","minibatch AVG loss: 0.007268519850913435\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.21764039993286133\n","minibatch AVG loss: 8.408306462115434e-05\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.21848416328430176\n","minibatch AVG loss: 0.04389513360055162\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.223649263381958\n","minibatch AVG loss: 0.000742451669248112\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.2226545810699463\n","minibatch AVG loss: 0.33843972586507787\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.22117829322814941\n","minibatch AVG loss: 0.0003802479756359389\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.22557592391967773\n","minibatch AVG loss: 0.008427053375089599\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.2265329360961914\n","minibatch AVG loss: 0.035342492513507295\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.225341796875\n","minibatch AVG loss: 8.897130101104267e-05\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.2234506607055664\n","minibatch AVG loss: 0.006721081444959509\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.22700738906860352\n","minibatch AVG loss: 0.018450951543127304\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.2273414134979248\n","minibatch AVG loss: 0.05090675650244521\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.22302937507629395\n","minibatch AVG loss: 0.0014605646029394847\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.225142240524292\n","minibatch AVG loss: 0.07413288341267048\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.22334051132202148\n","minibatch AVG loss: 5.388197534728078e-06\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.223891019821167\n","minibatch AVG loss: 0.0028386493387074553\n","\n","Epoch:  test \n","Loss: 0.0368  Acc: 98.7500\n","benign precision: 100.0000  recall: 97.2973\n","benign sensitivity: 97.2973  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.7273\n","benign TP: 36.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.7273  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 97.2973\n","malignant FPR: 2.7027  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 36.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 31s\n"]}],"source":["!python Test.py --model_idx ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Qe7P80UatEv","executionInfo":{"status":"ok","timestamp":1651156797046,"user_tz":-480,"elapsed":40865,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"b39c4aab-30d3-48b1-e6d9-4204e7329069"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 0.1734, -0.0014]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","model loaded\n","model : ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.4049055576324463\n","minibatch AVG loss: 0.06898847967386246\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.21649718284606934\n","minibatch AVG loss: 0.021632977155968548\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.21540117263793945\n","minibatch AVG loss: 0.05562780029140413\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.2144947052001953\n","minibatch AVG loss: 0.07549212584272028\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.2147533893585205\n","minibatch AVG loss: 0.08422242507804185\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.21269798278808594\n","minibatch AVG loss: 0.013265389041043818\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.2115936279296875\n","minibatch AVG loss: 0.1837251389399171\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.21753144264221191\n","minibatch AVG loss: 0.02454190105199814\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.2165970802307129\n","minibatch AVG loss: 0.05688560288399458\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.2141706943511963\n","minibatch AVG loss: 0.04015160566195845\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.21789956092834473\n","minibatch AVG loss: 0.03423870950937271\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.21660733222961426\n","minibatch AVG loss: 0.029124563932418822\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.2190408706665039\n","minibatch AVG loss: 0.03948083943687379\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.21920394897460938\n","minibatch AVG loss: 0.08009353429079055\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.22074365615844727\n","minibatch AVG loss: 0.008219569455832243\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.21773338317871094\n","minibatch AVG loss: 0.014990325877442956\n","\n","Epoch:  test \n","Loss: 0.0519  Acc: 98.7500\n","benign precision: 100.0000  recall: 97.2973\n","benign sensitivity: 97.2973  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.7273\n","benign TP: 36.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.7273  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 97.2973\n","malignant FPR: 2.7027  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 36.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 31s\n"]}],"source":["!python Test.py --model_idx ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Hx39eVgatzK","executionInfo":{"status":"ok","timestamp":1651156835008,"user_tz":-480,"elapsed":37966,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"67a5cbe9-b712-48e0-ab96-c22537b5fa08"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[0.0708, 0.1962]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","model loaded\n","model : ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.38332223892211914\n","minibatch AVG loss: 0.03965641297399998\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.21826410293579102\n","minibatch AVG loss: 0.018216618802398443\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.22004914283752441\n","minibatch AVG loss: 0.10124108060263097\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.2188258171081543\n","minibatch AVG loss: 0.08198282904922963\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.21725010871887207\n","minibatch AVG loss: 0.030169670889154076\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.22119140625\n","minibatch AVG loss: 0.03463305083569139\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.220017671585083\n","minibatch AVG loss: 0.1377294928766787\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.22148394584655762\n","minibatch AVG loss: 0.07353107463568449\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.22581219673156738\n","minibatch AVG loss: 0.07631861791014671\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.22240710258483887\n","minibatch AVG loss: 0.07474507809383794\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.2001938819885254\n","minibatch AVG loss: 0.056209254264831546\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.22158217430114746\n","minibatch AVG loss: 0.04956618156284094\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.22327685356140137\n","minibatch AVG loss: 0.030446830205619335\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.22424602508544922\n","minibatch AVG loss: 0.05372587479650974\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.22568845748901367\n","minibatch AVG loss: 0.017883060243912043\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.22257471084594727\n","minibatch AVG loss: 0.0035070294979959726\n","\n","Epoch:  test \n","Loss: 0.0550  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 37.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 37.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 31s\n"]}],"source":["!python Test.py --model_idx ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtBaK8ZdL1X7","executionInfo":{"status":"ok","timestamp":1651156868318,"user_tz":-480,"elapsed":33318,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"8730ee4c-0f6d-4025-9b99-f9fbf1374749"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","test model output： tensor([[-0.9905, -1.2161]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","model loaded\n","model : ResNet50_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ResNet50_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.22195219993591309\n","minibatch AVG loss: 0.2323053312022239\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.0838472843170166\n","minibatch AVG loss: 0.016993183363229036\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.07171416282653809\n","minibatch AVG loss: 0.1273073631396983\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.09260797500610352\n","minibatch AVG loss: 0.10455432562157511\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.07474160194396973\n","minibatch AVG loss: 0.018649355277739234\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.06895995140075684\n","minibatch AVG loss: 0.014132234643329866\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.06925511360168457\n","minibatch AVG loss: 0.07772843023994938\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.06815624237060547\n","minibatch AVG loss: 0.011631050158757716\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.08311033248901367\n","minibatch AVG loss: 0.033888889383524655\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.07191824913024902\n","minibatch AVG loss: 0.0941688597202301\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.07132196426391602\n","minibatch AVG loss: 0.033860289677977565\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.06709909439086914\n","minibatch AVG loss: 0.01447802186012268\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.08513283729553223\n","minibatch AVG loss: 0.015170091763138771\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.07471942901611328\n","minibatch AVG loss: 0.02151481220498681\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.07039427757263184\n","minibatch AVG loss: 0.025665557105094196\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.09824013710021973\n","minibatch AVG loss: 0.012380525842308998\n","\n","Epoch:  test \n","Loss: 0.0534  Acc: 98.7500\n","benign precision: 100.0000  recall: 97.2973\n","benign sensitivity: 97.2973  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.7273\n","benign TP: 36.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.7273  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 97.2973\n","malignant FPR: 2.7027  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 36.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 27s\n"]}],"source":["!python Test.py --model_idx ResNet50_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpO0kpfrSLNH","executionInfo":{"status":"ok","timestamp":1651156911985,"user_tz":-480,"elapsed":43672,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"1465d1ab-23dc-45ba-86c1-4eaf7e5e421f"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","['vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224']\n","test model output： tensor([[0.8741, 0.9087]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","model loaded\n","model : ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.49413514137268066\n","minibatch AVG loss: 0.00742482467321679\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.2865760326385498\n","minibatch AVG loss: 0.002951249445322901\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.2907702922821045\n","minibatch AVG loss: 0.0018473068543244154\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.28809595108032227\n","minibatch AVG loss: 0.006233821075875312\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.29156041145324707\n","minibatch AVG loss: 0.007037096051499247\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.2862997055053711\n","minibatch AVG loss: 0.0012872561695985497\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.29506373405456543\n","minibatch AVG loss: 0.01030626312131062\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.29245686531066895\n","minibatch AVG loss: 0.03560496747959405\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.2942681312561035\n","minibatch AVG loss: 0.006210455193649977\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.29232287406921387\n","minibatch AVG loss: 0.05390129570150748\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.29535961151123047\n","minibatch AVG loss: 0.00414046723744832\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.29597926139831543\n","minibatch AVG loss: 0.0033032891340553762\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.2912313938140869\n","minibatch AVG loss: 0.009589462832082063\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.29274821281433105\n","minibatch AVG loss: 0.029609713330864908\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.29225993156433105\n","minibatch AVG loss: 0.001620503282174468\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.2941169738769531\n","minibatch AVG loss: 0.0025430566631257532\n","\n","Epoch:  test \n","Loss: 0.0115  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 37.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 37.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 35s\n"]}],"source":["!python Test.py --model_idx ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIrMFuz5SLd9","executionInfo":{"status":"ok","timestamp":1651156946952,"user_tz":-480,"elapsed":34972,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"953a4cf5-e4a2-4188-e8cc-1d9f51ed001f"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","['efficientnet_b0',\n"," 'efficientnet_b1',\n"," 'efficientnet_b1_pruned',\n"," 'efficientnet_b2',\n"," 'efficientnet_b2_pruned',\n"," 'efficientnet_b2a',\n"," 'efficientnet_b3',\n"," 'efficientnet_b3_pruned',\n"," 'efficientnet_b3a',\n"," 'efficientnet_b4',\n"," 'efficientnet_b5',\n"," 'efficientnet_b6',\n"," 'efficientnet_b7',\n"," 'efficientnet_b8',\n"," 'efficientnet_cc_b0_4e',\n"," 'efficientnet_cc_b0_8e',\n"," 'efficientnet_cc_b1_8e',\n"," 'efficientnet_el',\n"," 'efficientnet_el_pruned',\n"," 'efficientnet_em',\n"," 'efficientnet_es',\n"," 'efficientnet_es_pruned',\n"," 'efficientnet_l2',\n"," 'efficientnet_lite0',\n"," 'efficientnet_lite1',\n"," 'efficientnet_lite2',\n"," 'efficientnet_lite3',\n"," 'efficientnet_lite4',\n"," 'efficientnetv2_l',\n"," 'efficientnetv2_m',\n"," 'efficientnetv2_rw_m',\n"," 'efficientnetv2_rw_s',\n"," 'efficientnetv2_rw_t',\n"," 'efficientnetv2_s',\n"," 'efficientnetv2_xl',\n"," 'gc_efficientnetv2_rw_t',\n"," 'tf_efficientnet_b0',\n"," 'tf_efficientnet_b0_ap',\n"," 'tf_efficientnet_b0_ns',\n"," 'tf_efficientnet_b1',\n"," 'tf_efficientnet_b1_ap',\n"," 'tf_efficientnet_b1_ns',\n"," 'tf_efficientnet_b2',\n"," 'tf_efficientnet_b2_ap',\n"," 'tf_efficientnet_b2_ns',\n"," 'tf_efficientnet_b3',\n"," 'tf_efficientnet_b3_ap',\n"," 'tf_efficientnet_b3_ns',\n"," 'tf_efficientnet_b4',\n"," 'tf_efficientnet_b4_ap',\n"," 'tf_efficientnet_b4_ns',\n"," 'tf_efficientnet_b5',\n"," 'tf_efficientnet_b5_ap',\n"," 'tf_efficientnet_b5_ns',\n"," 'tf_efficientnet_b6',\n"," 'tf_efficientnet_b6_ap',\n"," 'tf_efficientnet_b6_ns',\n"," 'tf_efficientnet_b7',\n"," 'tf_efficientnet_b7_ap',\n"," 'tf_efficientnet_b7_ns',\n"," 'tf_efficientnet_b8',\n"," 'tf_efficientnet_b8_ap',\n"," 'tf_efficientnet_cc_b0_4e',\n"," 'tf_efficientnet_cc_b0_8e',\n"," 'tf_efficientnet_cc_b1_8e',\n"," 'tf_efficientnet_el',\n"," 'tf_efficientnet_em',\n"," 'tf_efficientnet_es',\n"," 'tf_efficientnet_l2_ns',\n"," 'tf_efficientnet_l2_ns_475',\n"," 'tf_efficientnet_lite0',\n"," 'tf_efficientnet_lite1',\n"," 'tf_efficientnet_lite2',\n"," 'tf_efficientnet_lite3',\n"," 'tf_efficientnet_lite4',\n"," 'tf_efficientnetv2_b0',\n"," 'tf_efficientnetv2_b1',\n"," 'tf_efficientnetv2_b2',\n"," 'tf_efficientnetv2_b3',\n"," 'tf_efficientnetv2_l',\n"," 'tf_efficientnetv2_l_in21ft1k',\n"," 'tf_efficientnetv2_l_in21k',\n"," 'tf_efficientnetv2_m',\n"," 'tf_efficientnetv2_m_in21ft1k',\n"," 'tf_efficientnetv2_m_in21k',\n"," 'tf_efficientnetv2_s',\n"," 'tf_efficientnetv2_s_in21ft1k',\n"," 'tf_efficientnetv2_s_in21k',\n"," 'tf_efficientnetv2_xl_in21ft1k',\n"," 'tf_efficientnetv2_xl_in21k']\n","test model output： tensor([[ 1.3080, -4.4135]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","model loaded\n","model : efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.2792050838470459\n","minibatch AVG loss: 0.5223921787983272\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.12444329261779785\n","minibatch AVG loss: 0.13006911414559\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.12735319137573242\n","minibatch AVG loss: 0.06535111355406116\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.12260651588439941\n","minibatch AVG loss: 0.11441411434789188\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.12287116050720215\n","minibatch AVG loss: 1.5243259994778782\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.12138748168945312\n","minibatch AVG loss: 0.068743218597956\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.11879229545593262\n","minibatch AVG loss: 0.30519902277737854\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.1255795955657959\n","minibatch AVG loss: 0.8355811167508364\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.12511396408081055\n","minibatch AVG loss: 0.003368392988340929\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.11915278434753418\n","minibatch AVG loss: 0.5515683469362557\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.1253352165222168\n","minibatch AVG loss: 0.11788123556580103\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.12018966674804688\n","minibatch AVG loss: 0.002707213484791282\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.13148832321166992\n","minibatch AVG loss: 0.6429514683783054\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.12038683891296387\n","minibatch AVG loss: 1.2375610298942774\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.12226295471191406\n","minibatch AVG loss: 0.03555286275222898\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.12166023254394531\n","minibatch AVG loss: 0.12990448437085433\n","\n","Epoch:  test \n","Loss: 0.3930  Acc: 88.7500\n","benign precision: 88.8889  recall: 86.4865\n","benign sensitivity: 86.4865  specificity: 90.6977\n","benign FPR: 9.3023  NPV: 88.6364\n","benign TP: 32.0\n","benign TN: 39.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 88.6364  recall: 90.6977\n","malignant sensitivity: 90.6977  specificity: 86.4865\n","malignant FPR: 13.5135  NPV: 88.8889\n","malignant TP: 39.0\n","malignant TN: 32.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","Testing complete in 0m 28s\n"]}],"source":["!python Test.py --model_idx efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oEQ8PIN4TUxi","executionInfo":{"status":"ok","timestamp":1651156986401,"user_tz":-480,"elapsed":39454,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"}},"outputId":"c119611d-01f9-416a-de1f-6df086a39814"},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: ['benign', 'malignant']\n","['swin_base_patch4_window7_224',\n"," 'swin_base_patch4_window7_224_in22k',\n"," 'swin_base_patch4_window12_384',\n"," 'swin_base_patch4_window12_384_in22k',\n"," 'swin_large_patch4_window7_224',\n"," 'swin_large_patch4_window7_224_in22k',\n"," 'swin_large_patch4_window12_384',\n"," 'swin_large_patch4_window12_384_in22k',\n"," 'swin_small_patch4_window7_224',\n"," 'swin_tiny_patch4_window7_224']\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","test model output： tensor([[ 0.2302, -0.0063]], grad_fn=<AddmmBackward0>)\n","model is ready now!\n","model loaded\n","model : swin_b_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='swin_b_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.4097747802734375\n","minibatch AVG loss: 0.019800987746566533\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.22193336486816406\n","minibatch AVG loss: 0.005874749948270619\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.22192597389221191\n","minibatch AVG loss: 0.0313045569229871\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.22263622283935547\n","minibatch AVG loss: 0.014982829615473748\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.2233593463897705\n","minibatch AVG loss: 0.007927273260429501\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.22272753715515137\n","minibatch AVG loss: 0.004435597639530897\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.22185039520263672\n","minibatch AVG loss: 0.019707429688423872\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.2252352237701416\n","minibatch AVG loss: 0.012495383433997632\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.22503256797790527\n","minibatch AVG loss: 0.0289368633646518\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.22343802452087402\n","minibatch AVG loss: 0.035310793505050245\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.22410273551940918\n","minibatch AVG loss: 0.009600149933248758\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.2252810001373291\n","minibatch AVG loss: 0.00882851891219616\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.22875714302062988\n","minibatch AVG loss: 0.0833288858179003\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.22528839111328125\n","minibatch AVG loss: 0.6312025662278756\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.227691650390625\n","minibatch AVG loss: 0.002623951714485884\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.22759699821472168\n","minibatch AVG loss: 0.05882742665708065\n","\n","Epoch:  test \n","Loss: 0.0609  Acc: 98.7500\n","benign precision: 97.3684  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.6744\n","benign FPR: 2.3256  NPV: 100.0000\n","benign TP: 37.0\n","benign TN: 42.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.6744\n","malignant sensitivity: 97.6744  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 97.3684\n","malignant TP: 42.0\n","malignant TN: 37.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","Testing complete in 0m 32s\n"]}],"source":["!python Test.py --model_idx swin_b_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"markdown","metadata":{"id":"32lV43PnKVJx"},"source":["# Synchronize files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCZDUffdQep4"},"outputs":[],"source":["# change working dir\n","import os\n","os.chdir(\"/home/MIL_Experiment/code/utils\")\n","!python check_log_json.py --draw_root /home/MIL_Experiment/runs --record_dir /home/MIL_Experiment/CSV_logs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"_Wx0ymiiEuyS","outputId":"c41cdee4-f4ac-4c85-a48b-37444b83c446"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘/content/drive/MyDrive/MIL_SI_sample’: File exists\n"]}],"source":["# create path on google drive\n","!mkdir /content/drive/MyDrive/MIL_SI_sample\n","# copy the results\n","!/bin/cp -rf /home/MIL_Experiment/* /content/drive/MyDrive/MIL_SI_sample/\n","print('results copy completed!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9lzAtLIhnGe5"},"outputs":[],"source":["!date --date='+8 hour'  # CST time zone"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Sample CLS warwick_Experiment 384 401 lf05 Counterparts Train.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}